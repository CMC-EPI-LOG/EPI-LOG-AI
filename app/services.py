import os
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
import voyageai
from openai import OpenAI
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGODB_URI")
VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_NAME = "epilog_db"
GUIDELINES_COLLECTION = "medical_guidelines"
AIR_QUALITY_COLLECTION = "daily_air_quality"

if not MONGO_URI:
    # Fallback to a dummy URI if not set to prevent startup crash, but it will fail on request
    print("WARNING: MONGODB_URI is not set.")
    MONGO_URI = "mongodb://localhost:27017"

# Initialize Clients
try:
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client[DB_NAME]
except Exception as e:
    print(f"Error initializing MongoDB client: {e}")
    mongo_client = None
    db = None

try:
    vo_client = voyageai.Client(api_key=VOYAGE_API_KEY)
except Exception as e:
    print(f"Error initializing Voyage AI client: {e}")
    vo_client = None

try:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    openai_client = None

async def get_air_quality(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch air quality data for the given station and today's date.
    """
    if db is None:
        raise Exception("Database connection not initialized")
        
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    # Try to find today's data for the station
    # Note: In a real scenario, you might need to query an external API if DB doesn't have it.
    # For this task, we assume it's in the DB or we simulate it if not found (for dev purposes).
    
    try:
        result = await db[AIR_QUALITY_COLLECTION].find_one({
            "stationName": station_name,
            "date": today_str
        })
        
        if result:
            return result
            
        # Mock data if not found (for demonstration purposes as requested structure implies data exists)
        # In production, this should return None or raise specific error
        print(f"No air quality data found for {station_name} on {today_str}. Using mock data.")
        return {
            "stationName": station_name,
            "date": today_str,
            "pm10_grade": "ë‚˜ì¨",
            "pm25_grade": "ë‚˜ì¨",
            "co_grade": "ë³´í†µ",
            "o3_grade": "ë³´í†µ",
            "no2_grade": "ì¢‹ìŒ",
            "so2_grade": "ì¢‹ìŒ",
            "integrated_grade": "ë‚˜ì¨"
        }
        
    except Exception as e:
        print(f"Error fetching air quality: {e}")
        raise e

CACHE_COLLECTION = "rag_cache"

def _generate_cache_key(air_data: Dict[str, Any], user_profile: Dict[str, Any]) -> str:
    grade_map = {"ì¢‹ìŒ": 1, "ë³´í†µ": 2, "ë‚˜ì¨": 3, "ë§¤ìš°ë‚˜ì¨": 4}
    
    pm25 = grade_map.get(air_data.get("pm25_grade", ""), 0)
    pm10 = grade_map.get(air_data.get("pm10_grade", ""), 0)
    o3 = grade_map.get(air_data.get("o3_grade", ""), 0) # Added o3 as per user example
    
    age_group = user_profile.get("ageGroup", "unknown")
    condition = user_profile.get("condition", "unknown")
    
    # Key format: pm25:3_pm10:2_o3:1_age:adult_cond:asthma
    return f"pm25:{pm25}_pm10:{pm10}_o3:{o3}_age:{age_group}_cond:{condition}"

async def get_medical_advice(station_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main orchestration function:
    1. Get Air Quality
    2. Check Cache
    3. Construct Query
    4. Vector Search
    5. Generate Advice with LLM
    6. Save to Cache & Return
    """
    # Step A: Get Air Quality
    air_data = await get_air_quality(station_name)
    if not air_data:
        raise ValueError(f"No air quality data found for station: {station_name}")

    cache_key = ""
    # [Step A.1] Check Cache
    if db is not None:
        try:
            cache_key = _generate_cache_key(air_data, user_profile)
            cached_entry = await db[CACHE_COLLECTION].find_one({"_id": cache_key})
            
            if cached_entry:
                print(f"âœ… Cache Hit! Key: {cache_key}")
                return cached_entry["data"]
        except Exception as e:
            print(f"âš ï¸ Cache check failed: {e}")

    # Determine main issue (simplified logic)
    main_condition = "ë³´í†µ"
    if air_data.get("pm25_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ì´ˆë¯¸ì„¸ë¨¼ì§€ {air_data['pm25_grade']}"
    elif air_data.get("pm10_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ë¯¸ì„¸ë¨¼ì§€ {air_data['pm10_grade']}"
    elif air_data.get("so2_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"í™©ì‚¬/ì´ì‚°í™”í™© {air_data['so2_grade']}" # Simplified
        
    # Step B: Query Construction
    user_condition = user_profile.get("condition", "ê±´ê°•í•¨")
    age_group = user_profile.get("ageGroup", "ì„±ì¸")
    
    # Primary Query: Specific
    search_query = f"{main_condition} ìƒí™©ì—ì„œ {user_condition} {age_group} í–‰ë™ ìš”ë ¹ ì£¼ì˜ì‚¬í•­"
    print(f"Generated Search Query (Primary): {search_query}")

    # Step C: Vector Search
    relevant_docs = []
    if vo_client and db is not None:
        try:
            # 1. Primary Search
            embed_result = vo_client.embed([search_query], model="voyage-3-large", input_type="query")
            query_vector = embed_result.embeddings[0]
            
            pipeline = [
                {
                    "$vectorSearch": {
                        "index": "default",
                        "path": "embedding",
                        "queryVector": query_vector,
                        "numCandidates": 100,
                        "limit": 3
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "text": 1,
                        "category": 1,
                        "risk_level": 1,
                        "source": 1,
                        "score": {"$meta": "vectorSearchScore"}
                    }
                }
            ]
            
            cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
            relevant_docs = await cursor.to_list(length=3)
            
            # 2. Fallback Search (If no docs found)
            if not relevant_docs:
                print("âš ï¸ Primary search returned no results. Attempting fallback (General) search.")
                fallback_query = f"{main_condition} í–‰ë™ ìš”ë ¹"
                embed_result_fb = vo_client.embed([fallback_query], model="voyage-3-large", input_type="query")
                query_vector_fb = embed_result_fb.embeddings[0]
                
                pipeline[0]["$vectorSearch"]["queryVector"] = query_vector_fb
                
                cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
                relevant_docs = await cursor.to_list(length=3)
                
        except Exception as e:
            print(f"Error during vector search: {e}")
            pass

    # Step D: LLM Generation
    if not openai_client:
         return {
            "decision": "Error",
            "reason": "OpenAI Client not initialized",
            "actionItems": [],
            "references": []
        }
        
    # Prepare Context
    context_text = "\n".join([f"- [ì¶œì²˜: {doc.get('source', 'ê°€ì´ë“œë¼ì¸')}] {doc.get('text', '')}" for doc in relevant_docs]) if relevant_docs else "ê´€ë ¨ ì˜í•™ì  ê°€ì´ë“œë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    system_prompt = """
    ë‹¹ì‹ ì€ í™˜ê²½ë³´ê±´ ì˜ì‚¬ìž…ë‹ˆë‹¤. ëŒ€ê¸°ì§ˆ ë°ì´í„°ì™€ í™˜ìžì˜ ê¸°ì €ì§ˆí™˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜¤ëŠ˜ì˜ í–‰ë™ ì§€ì¹¨ì„ ë‚´ë ¤ì£¼ì„¸ìš”.
    
    [ì¤‘ìš”]
    1. ì œê³µëœ [ì˜í•™ì  ê°€ì´ë“œë¼ì¸] ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì¡°ì–¸ì„ ìž‘ì„±í•˜ì„¸ìš”.
    2. ê°€ì´ë“œë¼ì¸ì— ê·¼ê±°ê°€ ìžˆë‹¤ë©´, ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.
    3. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    
    ì‘ë‹µ í¬ë§·:
    {
        "decision": "O" | "X" | "â–³",
        "reason": "íŒë‹¨ ê·¼ê±° (ê°€ì´ë“œë¼ì¸ ë‚´ìš© ì¸ìš© í¬í•¨)",
        "actionItems": ["í–‰ë™ìš”ë ¹1", "í–‰ë™ìš”ë ¹2", "í–‰ë™ìš”ë ¹3"]
    }
    """
    
    user_prompt = f"""
    [ìƒí™© ì •ë³´]
    - ëŒ€ê¸°ì§ˆ ìƒíƒœ: {json.dumps(air_data, ensure_ascii=False)}
    - ì‚¬ìš©ìž ì •ë³´: {json.dumps(user_profile, ensure_ascii=False)}
    
    [ì˜í•™ì  ê°€ì´ë“œë¼ì¸ (ì°¸ê³  ë¬¸í—Œ)]
    {context_text}
    
    ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì´ ì‚¬ìš©ìžì—ê²Œ ë§žëŠ” ì˜¤ëŠ˜ì˜ í–‰ë™ ì§€ì¹¨ì„ ìž‘ì„±í•´ì£¼ì„¸ìš”.
    ê°€ì´ë“œë¼ì¸ì˜ ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
    """
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.7 # Slight reduction for more adherence to context
        )
        
        content = response.choices[0].message.content
        result_json = json.loads(content)
        
        # [Step E] Add References
        references = list(set([doc.get("source", "Unknown Source") for doc in relevant_docs]))
        result_json["references"] = references
        
        # [Step F] Save to Cache
        if db is not None and cache_key:
            try:
                await db[CACHE_COLLECTION].update_one(
                    {"_id": cache_key},
                    {"$set": {"data": result_json, "created_at": datetime.now()}},
                    upsert=True
                )
                print(f"ðŸ’¾ Saved to cache: {cache_key}")
            except Exception as e:
                print(f"Error saving to cache: {e}")
                
        return result_json
        
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        return {
            "decision": "Error",
            "reason": f"Failed to generate advice: {str(e)}",
            "actionItems": [],
            "references": []
        }

async def ingest_pdf(file_content: bytes, filename: str) -> Dict[str, Any]:
    """
    Ingest PDF content: Extract text -> Embed -> Store in DB.
    """
    import io
    from PyPDF2 import PdfReader

    if not vo_client:
        return {"status": "error", "message": "Voyage AI Client not initialized"}
    
    if db is None:
        return {"status": "error", "message": "Database not initialized"}

    try:
        # 1. Read PDF
        pdf_file = io.BytesIO(file_content)
        reader = PdfReader(pdf_file)
        
        extracted_text = ""
        documents_to_insert = []
        texts_to_embed = []
        
        # 2. Extract Text per Page (Chunking Strategy: 1 Page = 1 Doc for simplicity)
        print(f"ðŸ“„ Processing PDF: {filename} ({len(reader.pages)} pages)")
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 50: # Ignore empty/too short pages
                extracted_text += text + "\n"
                
                texts_to_embed.append(text)
                documents_to_insert.append({
                    "text": text,
                    "category": "pdf_upload",
                    "source": filename,
                    "page": i + 1,
                    "risk_level": "unknown", # Needs manual classification or LLM analysis
                    "created_at": datetime.now()
                })
        
        if not documents_to_insert:
            return {"status": "error", "message": "No extractable text found in PDF."}

        # 3. Embed Data
        print(f"ðŸ§  Embedding {len(texts_to_embed)} pages with Voyage AI...")
        result = vo_client.embed(texts_to_embed, model="voyage-3-large", input_type="document")
        embeddings = result.embeddings
        
        for i, doc in enumerate(documents_to_insert):
            doc["embedding"] = embeddings[i]
            
        # 4. Insert into DB
        insert_result = await db[GUIDELINES_COLLECTION].insert_many(documents_to_insert)
        
        return {
            "status": "success",
            "message": f"Successfully ingested {len(insert_result.inserted_ids)} pages from {filename}",
            "inserted_ids": [str(id) for id in insert_result.inserted_ids]
        }
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        return {"status": "error", "message": str(e)}
