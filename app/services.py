import os
import json
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional, Any
import voyageai
from openai import OpenAI
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGODB_URI")
VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_NAME = "epilog_db"
GUIDELINES_COLLECTION = "medical_guidelines"
AIR_QUALITY_COLLECTION = "daily_air_quality"
VECTOR_INDEX = "vector_index"
KST_TZ = ZoneInfo("Asia/Seoul")

if not MONGO_URI:
    # Fallback to a dummy URI if not set to prevent startup crash, but it will fail on request
    print("WARNING: MONGODB_URI is not set.")
    MONGO_URI = "mongodb://localhost:27017"

# Initialize Clients
try:
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client[DB_NAME]
except Exception as e:
    print(f"Error initializing MongoDB client: {e}")
    mongo_client = None
    db = None

# --- Logic Constants ---
GRADE_MAP = {
    "Ï¢ãÏùå": 1,
    "Î≥¥ÌÜµ": 2,
    "ÎÇòÏÅ®": 3,
    "Îß§Ïö∞ÎÇòÏÅ®": 4
}

REVERSE_GRADE_MAP = {v: k for k, v in GRADE_MAP.items()}

# Correction Weights
HUMIDITY_WEIGHTS = {
    "high": 1.2,  # > 70%
    "low": 1.1,   # < 30%
    "normal": 1.0
}

def _get_corrected_grade(
    base_grade: str, 
    temp: Optional[float], 
    humidity: Optional[float], 
    condition: str,
    pollutant_type: str # "pm25" or "o3"
) -> str:
    """
    Apply correction logic based on temperature, humidity and disease condition.
    Returns the corrected grade string.
    """
    score = GRADE_MAP.get(base_grade, 2)
    
    # 1. Humidity Correction (W_h)
    w_h = 1.0
    if humidity is not None:
        if humidity > 70:
            w_h = HUMIDITY_WEIGHTS["high"]
        elif humidity < 30:
            w_h = HUMIDITY_WEIGHTS["low"]
    
    # 2. Temperature & Disease Trigger Logic
    # Asthma + Cold + PM2.5
    if condition == "asthma" and temp is not None and temp < 5 and pollutant_type == "pm25":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # Rhinitis + Dry + PM2.5
    if condition == "rhinitis" and humidity is not None and humidity < 30 and pollutant_type == "pm25":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # Atopy + Heat + O3
    if condition == "atopy" and temp is not None and temp > 30 and pollutant_type == "o3":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # General + High Humidity + PM2.5 Bad
    if humidity is not None and humidity > 80 and pollutant_type == "pm25" and base_grade == "ÎÇòÏÅ®":
        return "Îß§Ïö∞ÎÇòÏÅ®"

    # Apply multiplicative weight if no specific trigger fired
    # (Simplified: if score * w_h rounds up to next grade)
    final_score = min(4, max(1, round(score * w_h)))
    return REVERSE_GRADE_MAP.get(final_score, base_grade)

# Decision Texts based on 80-segment dataset
DECISION_TEXTS = {
    "infant": {
        "general": {
            "ok": "Ïú†Î™®Ï∞® ÏÇ∞Ï±Ö Í∞ÄÏöî!",
            "caution": "ÏßßÏùÄ ÏÇ∞Ï±ÖÎßå Ï∂îÏ≤úÌï¥Ïöî",
            "warning": "Ïã§ÎÇ¥Í∞Ä Îçî ÏïàÏ†ÑÌï¥Ïöî"
        },
        "rhinitis": {
            "ok": "ÏΩîÍ∞Ä Ìé∏ÏïàÌïú ÎÇ†Ïù¥ÏóêÏöî",
            "caution": "ÏΩîÏ†êÎßâ Î≥¥ÏäµÏóê ÏßëÏ§ë",
            "warning": "ÏΩßÎ¨º Ïú†Î∞ú Ï£ºÏùòÎ≥¥"
        },
        "asthma": {
            "ok": "ÏÉÅÏæåÌïòÍ≤å Ïà® Ïâ¨Ïñ¥Ïöî",
            "caution": "Ï∞¨ Î∞îÎûå ÎÖ∏Ï∂ú Ï£ºÏùò",
            "warning": "ÏåïÏåïÍ±∞Î¶º Î™®ÎãàÌÑ∞ÎßÅ"
        },
        "atopy": {
            "ok": "ÌîºÎ∂Ä Í∞ÄÎ†§ÏõÄ Í±±Ï†ï Îöù",
            "caution": "ÎïÄÎÇòÎ©¥ Î∞îÎ°ú Îã¶ÏïÑÏ£ºÏÑ∏Ïöî",
            "warning": "Ïô∏Î∂Ä Î®ºÏßÄ Ï†ëÏ¥â Ï∞®Îã®"
        }
    },
    "toddler": {
        "general": {
            "ok": "ÎÜÄÏù¥ÌÑ∞ÏóêÏÑú Îõ∞ÎÜÄÏïÑÏöî",
            "caution": "Î¨º Ìïú Ïªµ ÎßàÏãúÍ≥† ÎÇòÍ∞ÄÍ∏∞",
            "warning": "Ïã§Ïô∏ ÎÜÄÏù¥Îäî ÏßßÍ≤å"
        },
        "rhinitis": {
            "ok": "ÏΩî Î©¥Ïó≠Î†• ÌÇ§Ïö∞Îäî ÎÇ†",
            "caution": "Ïû¨Ï±ÑÍ∏∞ Ïú†ÎèÑ Î®ºÏßÄ Ï°∞Ïã¨",
            "warning": "ÏûÖ ÎåÄÏã† ÏΩîÎ°ú Ïà® Ïâ¨Í∏∞"
        },
        "asthma": {
            "ok": "Í∏∞ÎèÑÍ∞Ä Ïó¥Î¶¨Îäî ÎÇ†Ïî®",
            "caution": "Í∞ëÏûëÏä§Îü∞ Í∏∞Ïπ® Ï£ºÏùò",
            "warning": "Í≤©Î†¨Ìïú Ïö¥Îèô Í∏àÏßÄ"
        },
        "atopy": {
            "ok": "ÌîºÎ∂ÄÍ∞Ä Ïà® Ïâ¨Îäî ÎÇ†",
            "caution": "ÎïÄÍ≥º Î®ºÏßÄÎ•º Î©ÄÎ¶¨Ìï¥Ïöî",
            "warning": "ÎïÄ Îã¶Í≥† Î∞îÎ°ú Î≥¥Ïäµ"
        }
    },
    "elementary_low": {
        "general": {
            "ok": "Ïö¥ÎèôÏû•ÏóêÏÑú ÎßàÏùåÍªè!",
            "caution": "Ï≤¥Ïú° Ï†Ñ ÏÉÅÌÉú ÌôïÏù∏",
            "warning": "Ïã§Ïô∏ Ï≤¥Ïú°ÏùÄ Ïâ¨Ïñ¥Í∞ÄÏöî"
        },
        "rhinitis": {
            "ok": "Ïà≤ Ï≤¥Ìóò Í∞ÄÍ∏∞ Ï¢ãÏùÄ ÎÇ†",
            "caution": "ÎßàÏä§ÌÅ¨ Ïì∞Í≥† Îì±ÍµêÌïòÍ∏∞",
            "warning": "Ïû¨Ï±ÑÍ∏∞/ÏïàÍµ¨ Ï¶ùÏÉÅ Ï£ºÏùò"
        },
        "asthma": {
            "ok": "Ïª®ÎîîÏÖò ÏµúÏÉÅÏù∏ ÎÇ†",
            "caution": "Ïö¥Îèô Í∞ïÎèÑÎ•º Ï°∞Ï†àÌï¥Ïöî",
            "warning": "Ïö¥Îèô Í∞ïÎèÑ Ï°∞Ï†à ÌïÑÏàò"
        },
        "atopy": {
            "ok": "ÏûêÏô∏ÏÑ† Ï∞®Îã®Ï†ú ÌïÑÏàò",
            "caution": "Í∏¥ÏÜåÎß§Î°ú ÌîºÎ∂Ä Î≥¥Ìò∏",
            "warning": "Í∏ÅÏßÄ ÏïäÍ≤å ÏãúÏõêÌïòÍ≤å"
        }
    },
    "elementary_high": {
        "general": {
            "ok": "ÏïºÏô∏ÌôúÎèô Í¥úÏ∞ÆÏïÑÏöî",
            "caution": "Îì±ÌïòÍµê Ïãú ÏÉÅÌÉú ÌôïÏù∏",
            "warning": "KF80 ÎßàÏä§ÌÅ¨ ÌïÑÏàò (Í≥†ÎÜçÎèÑÎäî Ìèê ÏÑ±Ïû•Ïóê ÏòÅÌñ•ÏùÑ Ï§òÏöî)"
        },
        "rhinitis": {
            "ok": "ÎßëÏùÄ Í≥µÍ∏∞Î°ú ÏΩî Ï†ïÌôî",
            "caution": "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ Ï∂îÏ≤ú",
            "warning": "ÏûÖ ÎåÄÏã† ÏΩîÎ°ú Ïà® Ïâ¨Í∏∞"
        },
        "asthma": {
            "ok": "ÏïºÏô∏ÌôúÎèô Î¨¥Î¶¨ ÏóÜÏñ¥Ïöî",
            "caution": "Ïö¥Îèô Í∞ïÎèÑ 50% ÌïòÌñ•",
            "warning": "Í∏∞ÎèÑ ÏóºÏ¶ù ÏòàÎ∞© Ï£ºÏùò (Ïã§Ïô∏ Ïù¥Îèô Ï†ÑÎ©¥ Ï†úÌïú)"
        },
        "atopy": {
            "ok": "ÌîºÎ∂Ä Ïû•Î≤Ω ÏïàÏã¨ ÎÇ†",
            "caution": "ÎïÄÍ≥º Î®ºÏßÄ Ï†ëÏ¥â Ï£ºÏùò",
            "warning": "Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌîºÎ∂Ä ÏÑ∏Ï†ï ÌïÑÏöî"
        }
    },
    "teen_adult": {
        "general": {
            "ok": "ÏïºÏô∏ ÌôúÎèô Î¨¥Î¶¨ ÏóÜÏñ¥Ïöî",
            "caution": "Ïö¥Îèô Í∞ïÎèÑÎäî ÎÇÆÏ∂îÍ≥† ÏãúÍ∞ÑÏùÄ ÏßßÍ≤å",
            "warning": "Ïã§ÎÇ¥ ÌôúÎèôÏù¥ Îçî ÏïàÏ†ÑÌï©ÎãàÎã§"
        },
        "rhinitis": {
            "ok": "ÏΩîÍ∞Ä Ìé∏ÏïàÌïú ÎÇ†ÏûÖÎãàÎã§",
            "caution": "ÏÉùÎ¶¨ÏãùÏóºÏàò ÏΩî ÏÑ∏Ï≤ô Í∂åÏû•",
            "warning": "Ïô∏Ï∂ú ÌõÑ ÏΩßÏÜç ÎØ∏ÏÑ∏Î®ºÏßÄ ÏÑ∏Ï†ï (Í≥µÍ∏∞Ï≤≠Ï†ïÍ∏∞ Í∞ÄÎèô)"
        },
        "asthma": {
            "ok": "ÏÉÅÏæåÌïú Ìò∏Ìù° Í∞ÄÎä•",
            "caution": "ÏïºÏô∏ Îü¨Îãù Í∞ïÎèÑ Ï°∞Ï†à",
            "warning": "ÏïºÏô∏ ÌôúÎèô Ï†ÑÎ©¥ Í∏àÏßÄ"
        },
        "atopy": {
            "ok": "ÌîºÎ∂Ä Í∞ÄÎ†§ÏõÄ ÏïàÏã¨",
            "caution": "ÏûêÍ∑π ÏÑ±Î∂Ñ Ï†ëÏ¥â Ï£ºÏùò",
            "warning": "Î®ºÏßÄ Ï†ëÏ¥â ÌîºÌïòÍ∏∞ (Ï¶âÍ∞ÅÏ†ÅÏù∏ ÏÑ∏Ï†ïÍ≥º Î≥¥Ïäµ)"
        }
    }
}

# Action Items templates
ACTION_ITEMS = {
    "infant": {
        "general": {
            "ok": ["Ïú†Î™®Ï∞® ÏÇ∞Ï±Ö", "15Î∂Ñ ÌôòÍ∏∞", "Î≥µÍ∑Ä ÌõÑ ÏÜêÎ∞ú ÏîªÍ∏∞"],
            "caution": ["Ïú†Î™®Ï∞® Ïª§Î≤Ñ ÏÇ¨Ïö©", "Í∑∏Îäò ÏÇ∞Ï±Ö", "Î≥µÍ∑Ä ÌõÑ Î≥¥Ïäµ"],
            "warning": ["Ï∞ΩÎ¨∏ Îã´Í∏∞/Î∞ÄÌèê", "Í≥µÍ∏∞Ï≤≠Ï†ïÍ∏∞ Í∞ÄÎèô", "ÏäµÎèÑ 50% Ïú†ÏßÄ/Î¨ºÍ±∏Î†à Ï≤≠ÏÜå"]
        },
        "rhinitis": {
            "ok": ["ÏæåÏ†ÅÌïú ÌôòÍ∏∞", "Î®ºÏßÄ ÌÑ∏Í∏∞", "Í∞ÄÎ≤ºÏö¥ Ïô∏Ï∂ú"],
            "caution": ["Í∞ÄÏäµÍ∏∞ Í∞ÄÎèô", "ÎØ∏ÏßÄÍ∑ºÌïú Î¨º ÎßàÏãúÍ∏∞", "Ïô∏Ï∂ú Í∞ÄÎ¶ºÎßâ"],
            "warning": ["Ïã§Ïô∏ ÌôúÎèô ÏûêÏ†ú/Ïã§ÎÇ¥ ÎåÄÍ∏∞", "ÏãùÏóºÏàò ÏΩî ÏÑ∏Ï†ï", "ÏÉÅÎπÑÏïΩ ÌôïÏù∏"]
        },
        "asthma": {
            "ok": ["Ïã†ÏÑ†Ìïú Í≥µÍ∏∞ Ïú†ÏßÄ", "Î≥¥Ìò∏Ïûê ÏÇ∞Ï±Ö", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù"],
            "caution": ["Î™© Í∞ÄÏã∏Í∞ú ÏÇ¨Ïö©", "Ïò®ÎèÑ Î≥ÄÌôî Ï£ºÏùò", "ÏÉÅÌÉú Í¥ÄÏ∞∞"],
            "warning": ["Í≤©Î†¨Ìïú ÎÜÄÏù¥ Í∏àÏßÄ", "ÏäµÎèÑ 55% Ïú†ÏßÄ", "ÎπÑÏÉÅÏïΩ ÌôïÏù∏/ÎåÄÏùë Ï§ÄÎπÑ"]
        },
        "atopy": {
            "ok": ["Ïô∏Ï∂ú Ï†Ñ ÏÑ†ÌÅ¨Î¶º", "ÌôúÎèô ÌõÑ ÏÑ∏Ïïà", "Î©¥ ÏÜåÏû¨ Ïò∑"],
            "caution": ["Ìú¥ÎåÄ ÏÜêÏàòÍ±¥ ÏßÄÏ∞∏", "Î≥¥ÏäµÏ†ú Î∞îÎ•¥Í∏∞", "ÏñáÏùÄ Í∏¥ÏÜåÎß§"],
            "warning": ["Ïô∏Ï∂ú ÏµúÏÜåÌôî/Ïã§ÎÇ¥ Ï≤¥Î•ò", "Í∑ÄÍ∞Ä Ï¶âÏãú ÏÉ§Ïõå", "Í≥†Î≥¥Ïäµ ÌÅ¨Î¶º/ÏãúÏõêÌïú Ïò®ÎèÑ Ïú†ÏßÄ"]
        }
    },
    "toddler": {
        "general": {
            "ok": ["ÏïºÏô∏ ÎÜÄÏù¥ Í∂åÏû•", "Ï†ÑÎ©¥ ÌôòÍ∏∞", "ÌôúÎèô ÌõÑ ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "caution": ["Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞", "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "Ïû•ÏãúÍ∞Ñ Ï≤¥Î•ò ÏûêÏ†ú"],
            "warning": ["ÏÜåÌòï ÎßàÏä§ÌÅ¨ Î∞ÄÏ∞©", "Ïã§ÎÇ¥ ÎÜÄÏù¥ ÏúÑÏ£º", "ÏïºÏô∏ ÌôúÎèô Í∏àÏßÄ/Í≥µÏ≤≠Í∏∞ ÏÇ¨Ïö©"]
        },
        "rhinitis": {
            "ok": ["Ïà≤ Ï≤¥Ìóò Ï∂îÏ≤ú", "ÌôòÍ∏∞ ÌõÑ Ï≤≠ÏÜå", "Ïô∏Ï∂ú ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÎßàÏä§ÌÅ¨ ÌïÑÏàò", "ÏΩî Ï£ºÎ≥Ä Î≥¥Ïäµ", "ÏãùÏóºÏàò ÏÑ∏Ï≤ô"],
            "warning": ["Ïô∏Ï∂ú ÌõÑ ÏΩî ÏÑ∏Ï≤ô", "Ïã§ÎÇ¥ Î®ºÏßÄ Ï†úÍ±∞", "Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞/Ï¶ùÏÉÅ Ïãú ÏïΩ Î≥µÏö©"]
        },
        "asthma": {
            "ok": ["Ïú†ÏÇ∞ÏÜå ÎÜÄÏù¥", "Í∑úÏπôÏ†Å ÏïΩ Î≥µÏö©", "Í∏∞Í¥Ä ÌôòÍ∏∞"],
            "caution": ["Ïö¥Îèô Í∞ïÎèÑ ÎÇÆÏ∂îÍ∏∞", "Ï§ëÍ∞Ñ Ìú¥Ïãù", "Ìò∏Ìù° ÏÉÅÌÉú ÌôïÏù∏"],
            "warning": ["Ï†ÄÍ∞ïÎèÑ ÎÜÄÏù¥ Ï†ÑÌôò", "Ìù°ÏûÖÍ∏∞ ÏßÄÏ∞∏", "Ïô∏Ï∂ú Í∏àÏßÄ/ÎπÑÏÉÅ Ïãú Î≥ëÏõê Î∞©Î¨∏"]
        },
        "atopy": {
            "ok": ["ÏÑ†ÌÅ¨Î¶º ÎèÑÌè¨", "ÏÇ∞Ï±Ö ÌõÑ Í∞ÄÎ≤ºÏö¥ ÏÉ§Ïõå", "Î©¥ ÏÜåÏû¨ Ïò∑"],
            "caution": ["ÏàòÏãúÎ°ú ÎïÄ Îã¶Í∏∞", "Ïô∏Ï∂ú ÌõÑ Î≥¥Ïäµ", "Í∏¥ÏÜåÎß§ Í≤âÏò∑"],
            "warning": ["Ìú¥ÎåÄÏö© Î≥¥ÏäµÏ†ú", "Ïô∏Ï∂ú ÌõÑ Ï¶âÏãú ÏÉ§Ïõå", "ÎÉâÏ∞úÏßà/ÏûêÍ∑π ÏóÜÎäî Î°úÏÖò"]
        }
    },
    "elementary_low": {
        "general": {
            "ok": ["Ïö¥ÎèôÏû•ÏóêÏÑú ÎßàÏùåÍªè", "ÍµêÏã§ Ï†ÑÎ©¥ ÌôòÍ∏∞", "ÏïºÏô∏ ÌïôÏäµ"],
            "caution": ["Ï§ëÍ∞Ñ ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÌôúÎèô ÌõÑ ÏñëÏπò", "ÎåÄÍ∏∞Ïßà Ï≤¥ÌÅ¨"],
            "warning": ["KF80 ÎßàÏä§ÌÅ¨ ÌïÑÏàò", "Ïã§ÎÇ¥ Ï≤¥Ïú° ÎåÄÏ≤¥", "Î∞©Í≥º ÌõÑ Ï¶âÏãú Í∑ÄÍ∞Ä"]
        },
        "rhinitis": {
            "ok": ["ÏïºÏô∏ ÏÇ∞Ï±Ö", "ÌôòÍ∏∞ ÌõÑ ÎåÄÏ≤≠ÏÜå", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù"],
            "caution": ["ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "ÏÜê ÏîªÍ∏∞ ÍµêÏú°", "Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞"],
            "warning": ["ÏïàÍµ¨ ÏÑ∏Ï†ï", "ÎßàÏä§ÌÅ¨ ÌïÑÏ∞©", "Ïô∏Ï∂ú ÌõÑ Î®∏Î¶¨Ïπ¥ÎùΩ ÌÑ∏Í∏∞"]
        },
        "asthma": {
            "ok": ["ÌïôÍµê Ï≤¥Ïú° Ï∞∏Ïó¨", "ÍπäÏùÄ Ìò∏Ìù° Ïö¥Îèô", "Ïª®ÎîîÏÖò Ïú†ÏßÄ"],
            "caution": ["Î¨¥Î¶¨Ìïú Îã¨Î¶¨Í∏∞ ÏûêÏ†ú", "Ï§ëÍ∞Ñ Ìú¥Ïãù", "Ìò∏Ìù° Î™®ÎãàÌÑ∞ÎßÅ"],
            "warning": ["ÏÉÅÎπÑÏïΩ Ìú¥ÎåÄ ÌôïÏù∏", "ÍµêÏÇ¨ÏóêÍ≤å ÎØ∏Î¶¨ ÏïåÎ¶¨Í∏∞", "ÎÖ∏Ï∂ú Ï†ÑÎ©¥ Ï∞®Îã®"]
        },
        "atopy": {
            "ok": ["ÏÑ†ÌÅ¨Î¶º Î∞îÎ•¥Í∏∞", "ÏïºÏô∏ ÌôúÎèô Ï¶êÍ∏∞Í∏∞", "ÌôúÎèô ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÏÜêÏàòÍ±¥ ÏßÄÏ∞∏", "Ïô∏Ï∂ú ÌõÑ Î≥¥ÏäµÏ†ú", "Î©¥ ÏÜçÏò∑ ÏûÖÌûàÍ∏∞"],
            "warning": ["ÎØ∏Ïä§Ìä∏ ÏÇ¨Ïö©", "ÌÜµÌíçÎêòÎäî Ïò∑", "ÎÉâÏ∞úÏßà ÏßÑÏ†ï/Ïã§ÎÇ¥ ÏäµÎèÑ Ï°∞Ï†à"]
        }
    },
    "elementary_high": {
        "general": {
            "ok": ["ÎπÑÌÉÄÎØº C ÏÑ≠Ï∑®", "ÏïºÏô∏ ÌôúÎèô Í∂åÏû•", "Í∑ÄÍ∞Ä ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÌïôÏõê Ïù¥Îèô Ïãú Ï≤úÏ≤úÌûà", "ÎßàÏä§ÌÅ¨ Ï∞©Ïö©", "ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "warning": ["KF80 ÎßàÏä§ÌÅ¨ ÌïÑÏàò", "Í≤©Î†¨Ìïú Ïö¥Îèô ÏûêÏ†ú", "Ïã§Ïô∏ ÎÖ∏Ï∂ú ÏµúÏÜåÌôî"]
        },
        "asthma": {
            "ok": ["Ï†ïÏÉÅÏ†ÅÏù∏ Ï≤¥Ïú° ÌôúÎèô", "Ï∂©Î∂ÑÌïú ÏàòÎ∂Ñ", "Í∑úÏπôÏ†Å ÏïΩ Î≥µÏö©"],
            "caution": ["Ïö¥Îèô Í∞ïÎèÑ Ï°∞Ï†à", "Ï§ëÍ∞ÑÏ§ëÍ∞Ñ Ìú¥Ïãù", "Ï¶ùÏÉÅ Î∞úÏÉù Ïãú Ï§ëÎã®"],
            "warning": ["Í≤©Î†¨Ìïú Ïö¥Îèô Í∏àÏßÄ", "Ìù°ÏûÖÍ∏∞ ÌïÑÏàò ÏßÄÏ∞∏", "Ïã§Ïô∏ Ïù¥Îèô Ï†ÑÎ©¥ Ï†úÌïú"]
        },
        "rhinitis": {
            "ok": ["ÏΩîÎ°ú Ïà®Ïâ¨Í∏∞ ÏßëÏ§ë", "ÌôòÍ∏∞ Î∞è Ï≤≠ÏÜå", "ÏΩî ÏÑ∏Ï≤ô"],
            "caution": ["ÌÖÄÎ∏îÎü¨ ÏßÄÏ∞∏", "Î®ºÏßÄ ÎßéÏùÄ Í≥≥ ÌîºÌïòÍ∏∞", "ÏÜê ÏîªÍ∏∞"],
            "warning": ["ÏãùÏóºÏàò ÏΩî ÏÑ∏Ï≤ô", "Îàà ÎπÑÎπÑÏßÄ ÏïäÍ∏∞", "Ïù∏Í≥µÎààÎ¨º ÏÇ¨Ïö©"]
        },
        "atopy": {
            "ok": ["Î≥¥ÏäµÏ†ú ÎèÑÌè¨", "Î©¥ ÏÜåÏû¨ ÎÇ¥Ïùò", "Ï∂©Î∂ÑÌïú ÏàòÎ©¥"],
            "caution": ["ÎïÄ Îã¶Îäî ÏàòÍ±¥ ÌïÑÏàò", "Î≥¥ÏäµÏ†ú ÎçßÎ∞îÎ•¥Í∏∞", "ÌÜµÌíçÎêòÎäî Ïò∑"],
            "warning": ["ÎØ∏ÏßÄÍ∑ºÌïú Î¨º ÏÉ§Ïõå", "Í∏âÍ≤©Ìïú Ïò®ÎèÑÎ≥ÄÌôî Ï£ºÏùò", "ÏïΩÏÇ∞ÏÑ± ÌÅ¥Î†åÏ†Ä ÏÑ∏Ïïà"]
        }
    },
    "teen_adult": {
        "general": {
            "ok": ["Ï†ïÏÉÅÏ†ÅÏù∏ ÌôúÎèô Í∞ÄÎä•", "Ï∂©Î∂ÑÌïú ÏàòÎ©¥", "ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "caution": ["ÏïºÏô∏ Ïö¥Îèô Í∞ïÎèÑ ÌïòÌñ•", "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "Ïû•ÏãúÍ∞Ñ ÎÖ∏Ï∂ú ÏûêÏ†ú"],
            "warning": ["ÏïºÏô∏ ÌôúÎèô ÎåÄÏã† Ïã§ÎÇ¥", "Ï∞ΩÎ¨∏ Î∞ÄÌèê", "Ìò∏Ìù°Í∏∞ Ï¶ùÏÉÅ Í¥ÄÏ∞∞"]
        },
        "rhinitis": {
            "ok": ["ÌôòÍ∏∞ Î∞è Ïã§ÎÇ¥ Ï†ïÌôî", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù", "ÏΩî ÏÑ∏Ï≤ô"],
            "caution": ["Ïô∏Ï∂ú Ïãú ÎßàÏä§ÌÅ¨ ÌïÑÏàò", "ÏΩî Ï£ºÎ≥Ä Î≥¥Ïäµ", "Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞"],
            "warning": ["ÏãùÏóºÏàò ÏΩî ÏÑ∏Ï≤ô", "Í∑ÄÍ∞Ä Ï¶âÏãú ÏÑ∏Ïïà", "Í≥µÍ∏∞Ï≤≠Ï†ïÍ∏∞ ÌíÄÍ∞ÄÎèô"]
        },
        "asthma": {
            "ok": ["Ïú†ÏÇ∞ÏÜå Ïö¥Îèô Í∂åÏû•", "Ïª®ÎîîÏÖò Í¥ÄÎ¶¨", "Ï†ïÍ∏∞ Í≤ÄÏßÑ"],
            "caution": ["ÏïºÏô∏ ÌôúÎèô ÏãúÍ∞Ñ Îã®Ï∂ï", "Ìù°ÏûÖÍ∏∞ ÏÜåÏßÄ", "Î¨¥Î¶¨Ìïú Ïö¥Îèô Í∏àÏßÄ"],
            "warning": ["Ïô∏Ï∂ú Ï†ÑÎ©¥ Í∏àÏßÄ", "Ïã§ÎÇ¥ ÏäµÎèÑ Ï°∞Ï†à", "Ï¶ùÏÉÅ ÏïÖÌôî Ïãú ÎÇ¥Ïõê"]
        },
        "atopy": {
            "ok": ["Í≥†Î≥¥Ïäµ ÌÅ¨Î¶º ÏÇ¨Ïö©", "ÏûêÍ∑π ÏóÜÎäî ÏÑ∏Ïïà", "Ï∂©Î∂ÑÌïú ÏàòÎ∂Ñ"],
            "caution": ["ÎïÄ Î∂ÑÎπÑ Ïãú Ï¶âÏãú Îã¶Í∏∞", "Î≥¥ÏäµÏ†ú ÏàòÏãú ÎèÑÌè¨", "Î©¥ ÏÜåÏû¨ ÏùòÎ•ò"],
            "warning": ["ÏïΩÏÇ∞ÏÑ± ÌÅ¥Î†åÏ†Ä ÏÇ¨Ïö©", "Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌîºÎ∂Ä ÏßÑÏ†ï", "ÎÖ∏Ï∂ú Î∂ÄÏúÑ ÏµúÏÜåÌôî"]
        }
    }
}

def _calculate_decision(pm25_grade: str, o3_grade: str) -> str:
    """
    Calculate decision level: 'ok', 'caution', 'warning'
    
    Logic (1:Ï¢ãÏùå, 2:Î≥¥ÌÜµ, 3:ÎÇòÏÅ®, 4:Îß§Ïö∞ÎÇòÏÅ®):
    ‚Ä¢ OK: PM2.5 <= 2 AND O3 <= 2
    ‚Ä¢ Caution: Either one is 3
    ‚Ä¢ Warning: Either one is 4 OR Both are 3
    """
    p_score = GRADE_MAP.get(pm25_grade, 2)
    o_score = GRADE_MAP.get(o3_grade, 2)
    
    # Check Warning Conditions
    if p_score >= 4 or o_score >= 4:
        return "warning"
    if p_score == 3 and o_score == 3:
        return "warning"
        
    # Check Caution Conditions
    if p_score == 3 or o_score == 3:
        return "caution"
        
    # Default OK
    return "ok"

def _normalize_age_group(age_group: Any) -> str:
    if age_group is None:
        return "elementary_high"
    raw = str(age_group).strip().lower()
    
    # Updated 5 groups based on planning document
    if raw in {"infant", "ÏòÅÏïÑ", "0-2", "0~2"}:
        return "infant"
    if raw in {"toddler", "Ïú†ÏïÑ", "3-6", "3~6"}:
        return "toddler"
    if raw in {"elementary_low", "Ï¥àÎì±Ï†ÄÌïôÎÖÑ", "Ï¥àÎì± Ï†ÄÌïôÎÖÑ", "7-9", "7~9", "1-3", "1~3"}:
        return "elementary_low"
    if raw in {"elementary_high", "Ï¥àÎì±Í≥†ÌïôÎÖÑ", "Ï¥àÎì± Í≥†ÌïôÎÖÑ", "10-12", "10~12", "4-6", "4~6"}:
        return "elementary_high"
    if raw in {"teen", "teen_adult", "Ï≤≠ÏÜåÎÖÑ", "ÏÑ±Ïù∏", "adult", "13-18", "13~18", "13+"}:
        return "teen_adult"
    
    # Fallbacks
    if "ÏòÅÏïÑ" in raw: return "infant"
    if "Ïú†ÏïÑ" in raw: return "toddler"
    if "Ï†ÄÌïôÎÖÑ" in raw: return "elementary_low"
    if "Í≥†ÌïôÎÖÑ" in raw: return "elementary_high"
    if "Ï≤≠ÏÜåÎÖÑ" in raw or "ÏÑ±Ïù∏" in raw: return "teen_adult"
    
    return "elementary_high"

def _get_display_content(age_group: str, condition: str, decision_key: str):
    """
    Returns (decision_text, action_items)
    """
    # Normalize condition
    cond_key = condition if condition in ["general", "rhinitis", "asthma", "atopy"] else "general"
    
    # Get Text
    group_data = DECISION_TEXTS.get(age_group, DECISION_TEXTS["elementary_high"])
    cond_data = group_data.get(cond_key, group_data.get("general", {}))
    d_text = cond_data.get(decision_key, "ÏÉÅÌÉú ÌôïÏù∏ ÌïÑÏöî")
    
    # Get Actions
    group_actions = ACTION_ITEMS.get(age_group, ACTION_ITEMS.get("toddler", {}))
    cond_actions = group_actions.get(cond_key, group_actions.get("general", {}))
    actions = cond_actions.get(decision_key, ["ÏÉÅÌÉúÏóê Îî∞Î•∏ Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§."])
    
    return d_text, actions[:] # Return a copy

try:
    vo_client = voyageai.Client(api_key=VOYAGE_API_KEY)
except Exception as e:
    print(f"Error initializing Voyage AI client: {e}")
    vo_client = None

try:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    openai_client = None

async def get_air_quality(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch air quality data from EPI-LOG-AIRKOREA API.
    This API aggregates real-time data from AirKorea and stores it in MongoDB.
    """
    import httpx
    
    AIRKOREA_API_URL = "https://epi-log-airkorea.vercel.app/api/stations"
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                AIRKOREA_API_URL,
                params={"stationName": station_name}
            )
            
            if response.status_code == 200:
                data = response.json()
                
                # API returns array, take first item
                if data and len(data) > 0:
                    station = data[0]
                    realtime = station.get("realtime", {})
                    
                    # Convert grade numbers to Korean text
                    grade_map = {1: "Ï¢ãÏùå", 2: "Î≥¥ÌÜµ", 3: "ÎÇòÏÅ®", 4: "Îß§Ïö∞ÎÇòÏÅ®"}
                    
                    # Extract and normalize data
                    result = {
                        "stationName": station.get("stationName", station_name),
                        "pm25_grade": grade_map.get(realtime.get("pm25", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "pm25_value": realtime.get("pm25", {}).get("value") or 50,
                        "pm10_grade": grade_map.get(realtime.get("pm10", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "pm10_value": realtime.get("pm10", {}).get("value") or 70,
                        "o3_grade": grade_map.get(realtime.get("o3", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "o3_value": realtime.get("o3", {}).get("value") or 0.05,
                        "no2_grade": grade_map.get(realtime.get("no2", {}).get("grade"), "Ï¢ãÏùå"),
                        "no2_value": realtime.get("no2", {}).get("value") or 0.02,
                        "co_grade": grade_map.get(realtime.get("co", {}).get("grade"), "Ï¢ãÏùå"),
                        "co_value": realtime.get("co", {}).get("value") or 0.5,
                        "so2_grade": grade_map.get(realtime.get("so2", {}).get("grade"), "Ï¢ãÏùå"),
                        "so2_value": realtime.get("so2", {}).get("value") or 0.003,
                        # Inject mock weather data (until weather API integration)
                        "temp": 22.0,
                        "humidity": 45.0
                    }
                    
                    print(f"‚úÖ Fetched air quality for {station_name} from EPI-LOG-AIRKOREA API")
                    return result
        
        # Fallback to mock data if API call fails
        print(f"‚ö†Ô∏è  No data from AIRKOREA API for {station_name}. Using mock data.")
    except Exception as e:
        print(f"‚ùå Error fetching air quality from AIRKOREA API: {e}")
    
    # Return mock data
    return {
        "stationName": station_name,
        "pm10_grade": "ÎÇòÏÅ®",
        "pm10_value": 85,
        "pm25_grade": "ÎÇòÏÅ®",
        "pm25_value": 65,
        "co_grade": "Î≥¥ÌÜµ",
        "co_value": 0.7,
        "o3_grade": "Î≥¥ÌÜµ",
        "o3_value": 0.065,
        "no2_grade": "Ï¢ãÏùå",
        "no2_value": 0.025,
        "so2_grade": "Ï¢ãÏùå",
        "so2_value": 0.004,
        "temp": 22.0,
        "humidity": 45.0
    }

CACHE_COLLECTION = "rag_cache"
CACHE_TTL_SECONDS = 60 * 60 * 30  # 30 hours
_cache_ttl_index_ready = False

def _generate_cache_key(air_data: Dict[str, Any], user_profile: Dict[str, Any]) -> str:
    grade_map = {"Ï¢ãÏùå": 1, "Î≥¥ÌÜµ": 2, "ÎÇòÏÅ®": 3, "Îß§Ïö∞ÎÇòÏÅ®": 4}
    
    pm25 = grade_map.get(air_data.get("pm25_grade", ""), 0)
    pm10 = grade_map.get(air_data.get("pm10_grade", ""), 0)
    o3 = grade_map.get(air_data.get("o3_grade", ""), 0) # Added o3 as per user example
    
    age_group = _normalize_age_group(user_profile.get("ageGroup"))
    condition = user_profile.get("condition", "unknown")
    date_key = air_data.get("date") or datetime.now(KST_TZ).strftime("%Y-%m-%d")
    
    # Key format: pm25:3_pm10:2_o3:1_age:adult_cond:asthma_date:2026-01-28
    return f"pm25:{pm25}_pm10:{pm10}_o3:{o3}_age:{age_group}_cond:{condition}_date:{date_key}"

async def _ensure_cache_ttl_index():
    global _cache_ttl_index_ready
    if _cache_ttl_index_ready or db is None:
        return
    try:
        await db[CACHE_COLLECTION].create_index(
            "created_at",
            expireAfterSeconds=CACHE_TTL_SECONDS,
            name="rag_cache_ttl"
        )
        _cache_ttl_index_ready = True
    except Exception as e:
        print(f"‚ö†Ô∏è Cache TTL index creation failed: {e}")

async def get_medical_advice(station_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main orchestration function with correction logic.
    """
    # Step A: Get Air Quality
    air_data = await get_air_quality(station_name)
    if not air_data:
        raise ValueError(f"No air quality data found for station: {station_name}")

    # Extract Weather Info for Correction
    temp = air_data.get("temp")
    humidity = air_data.get("humidity")
    user_condition = user_profile.get("condition", "Í±¥Í∞ïÌï®")
    age_group_raw = user_profile.get("ageGroup")
    age_group = _normalize_age_group(age_group_raw)

    # Apply Correction Logic to get "Sensed" grades
    pm25_raw = air_data.get("pm25_grade", "Î≥¥ÌÜµ")
    o3_raw = air_data.get("o3_grade", "Î≥¥ÌÜµ")
    
    pm25_corrected = _get_corrected_grade(pm25_raw, temp, humidity, user_condition, "pm25")
    o3_corrected = _get_corrected_grade(o3_raw, temp, humidity, user_condition, "o3")

    cache_key = ""
    # [Step A.1] Check Cache
    if db is not None:
        try:
            await _ensure_cache_ttl_index()
            # Simple key extension: add T/H to capture environmental context
            cache_key = _generate_cache_key(air_data, user_profile) + f"_T:{temp}_H:{humidity}"
            cached_entry = await db[CACHE_COLLECTION].find_one({"_id": cache_key})
            
            if cached_entry:
                print(f"‚úÖ Cache Hit! Key: {cache_key}")
                return cached_entry["data"]
        except Exception as e:
            print(f"‚ö†Ô∏è Cache check failed: {e}")

    # Determine main issue for search (using corrected grades)
    main_condition = "Î≥¥ÌÜµ"
    if pm25_corrected in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"Ï¥àÎØ∏ÏÑ∏Î®ºÏßÄ {pm25_corrected}"
    elif air_data.get("pm10_grade") in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"ÎØ∏ÏÑ∏Î®ºÏßÄ {air_data['pm10_grade']}"
    elif o3_corrected in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"Ïò§Ï°¥ {o3_corrected}"
        
    # Step B: Query Construction
    search_query = f"{main_condition} ÏÉÅÌô©ÏóêÏÑú {user_condition} {age_group} ÌñâÎèô ÏöîÎ†π Ï£ºÏùòÏÇ¨Ìï≠"
    print(f"Generated Search Query (Primary): {search_query}")

    # Step C: Vector Search
    relevant_docs = []
    if vo_client and db is not None:
        try:
            # 1. Primary Search
            embed_result = vo_client.embed([search_query], model="voyage-3-large", input_type="query")
            query_vector = embed_result.embeddings[0]
            
            pipeline = [
                {
                    "$vectorSearch": {
                        "index": VECTOR_INDEX,
                        "path": "embedding",
                        "queryVector": query_vector,
                        "numCandidates": 100,
                        "limit": 3
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "text": 1,
                        "category": 1,
                        "risk_level": 1,
                        "source": 1,
                        "score": {"$meta": "vectorSearchScore"}
                    }
                }
            ]
            
            cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
            relevant_docs = await cursor.to_list(length=3)
            
            # 2. Fallback Search (If no docs found)
            if not relevant_docs:
                print("‚ö†Ô∏è Primary search returned no results. Attempting fallback (General) search.")
                fallback_query = f"{main_condition} ÌñâÎèô ÏöîÎ†π"
                embed_result_fb = vo_client.embed([fallback_query], model="voyage-3-large", input_type="query")
                query_vector_fb = embed_result_fb.embeddings[0]
                
                pipeline[0]["$vectorSearch"]["queryVector"] = query_vector_fb
                
                cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
                relevant_docs = await cursor.to_list(length=3)
                
        except Exception as e:
            print(f"Error during vector search: {e}")
            pass

    # Step D: LLM Generation
    if not openai_client:
         return {
            "decision": "Error",
            "reason": "OpenAI Client not initialized",
            "actionItems": [],
            "references": []
        }

    # [Logic Update] Calculate Deterministic Decision & Action Items using CORRECTED grades
    decision_key = _calculate_decision(pm25_corrected, o3_corrected)
    decision_text, action_items = _get_display_content(age_group, user_condition, decision_key)
    
    # O3 Special Handling: Force-Append and Warnings
    is_o3_dominant = GRADE_MAP.get(o3_corrected, 1) >= GRADE_MAP.get(pm25_corrected, 1)
    if is_o3_dominant and GRADE_MAP.get(o3_corrected, 1) >= 3: # 'ÎÇòÏÅ®' Ïù¥ÏÉÅ
        decision_text += " (Ïò§Ï°¥ÏùÄ ÎßàÏä§ÌÅ¨Î°ú Í±∏Îü¨ÏßÄÏßÄ ÏïäÏïÑÏöî!)"
        # Force-Append Action Item
        o3_force_action = "Ïò§ÌõÑ 2~5Ïãú ÏÇ¨Ïù¥ÏóêÎäî Ïã§Ïô∏ ÌôúÎèôÏùÑ Ï†ÑÎ©¥ Í∏àÏßÄÌïòÍ≥† Ïã§ÎÇ¥Ïóê Î®∏Î¨¥Î•¥ÏÑ∏Ïöî."
        if o3_force_action not in action_items:
            action_items.append(o3_force_action)

    # Infant Special Warning
    if age_group == "infant":
        infant_warning = "‚Äª Ï£ºÏùò: ÎßàÏä§ÌÅ¨ Ï∞©Ïö© Í∏àÏßÄ(ÏßàÏãù ÏúÑÌóò)"
        if infant_warning not in action_items:
            action_items.insert(0, infant_warning) # Put at top

    # Logic for dual bad condition text append
    if GRADE_MAP.get(pm25_corrected, 1) >= 3 and GRADE_MAP.get(o3_corrected, 1) >= 3:
        decision_text += " (ÎØ∏ÏÑ∏Î®ºÏßÄÏôÄ Ïò§Ï°¥ Îëò Îã§ ÎÜíÏïÑÏöî!)"

    # Prepare Context
    context_text = "\n".join([f"- [Ï∂úÏ≤ò: {doc.get('source', 'Í∞ÄÏù¥ÎìúÎùºÏù∏')}] {doc.get('text', '')}" for doc in relevant_docs]) if relevant_docs else "Í¥ÄÎ†® ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    
    system_prompt = """
    ÎãπÏã†ÏùÄ ÌôòÍ≤ΩÎ≥¥Í±¥ ÏùòÏÇ¨ÏûÖÎãàÎã§. ÎåÄÍ∏∞Ïßà Îç∞Ïù¥ÌÑ∞(Ïò®ÎèÑ, ÏäµÎèÑ Ìè¨Ìï®)ÏôÄ ÌôòÏûêÏùò Í∏∞Ï†ÄÏßàÌôò Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÌåêÎã® Í∑ºÍ±∞(Reason)Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
    
    [Ï§ëÏöî]
    1. 'decision'Í≥º 'actionItems'Îäî Ïù¥ÎØ∏ ÏãúÏä§ÌÖúÏóêÏÑú Í≥ÑÏÇ∞ÎêòÏóàÏäµÎãàÎã§. ÎãπÏã†ÏùÄ Ïù¥ Í≤∞Ï†ïÏù¥ ÎÇ¥Î†§ÏßÑ 'ÏùòÌïôÏ†Å/ÌôòÍ≤ΩÏ†Å Ïù¥Ïú†(reason)'Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
    2. Î≥¥Ï†ï Î°úÏßÅÏù¥ Ï†ÅÏö©Îêú Í≤ΩÏö∞(Ïòà: ÏäµÎèÑÍ∞Ä ÎÑàÎ¨¥ ÎÜíÍ±∞ÎÇò ÎÇÆÏïÑÏÑú, ÌòπÏùÄ ÌäπÏ†ï ÏßàÌôò Ìä∏Î¶¨Í±∞Î°ú Ïù∏Ìï¥ Îì±Í∏âÏù¥ Í≤©ÏÉÅÎê®) Í∑∏ Ïù¥Ïú†Î•º ÏÑ§Î™ÖÏóê Ìè¨Ìï®ÌïòÏÑ∏Ïöî.
    3. Ï†úÍ≥µÎêú [ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏] ÎÇ¥Ïö©ÏùÑ ÏµúÏö∞ÏÑ†ÏúºÎ°ú Î∞òÏòÅÌïòÏó¨ ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
    4. Î∞òÎìúÏãú JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ïïº Ìï©ÎãàÎã§.
    """
    
    user_prompt = f"""
    [ÏÉÅÌô© Ï†ïÎ≥¥]
    - ÎåÄÍ∏∞Ïßà: Ï¥àÎØ∏ÏÑ∏Î®ºÏßÄ={pm25_raw}(Î≥¥Ï†ïÌõÑ:{pm25_corrected}), Ïò§Ï°¥={o3_raw}(Î≥¥Ï†ïÌõÑ:{o3_corrected})
    - ÌôòÍ≤Ω: Ïò®ÎèÑ={temp}¬∞C, ÏäµÎèÑ={humidity}%
    - ÏÇ¨Ïö©Ïûê: Ïó∞Î†πÎåÄ={age_group}, Í∏∞Ï†ÄÏßàÌôò={user_condition}
    - ÏãúÏä§ÌÖú Í≤∞Ï†ï: {decision_text}
    - ÏãúÏä§ÌÖú ÌñâÎèôÏàòÏπô: {action_items}
    
    [ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏ (Ï∞∏Í≥† Î¨∏Ìóå)]
    {context_text}
    
    ÏúÑ Í≤∞Ï†ïÏù¥ ÎÇ¥Î†§ÏßÑ Î∞∞Í≤ΩÍ≥º Ïù¥Ïú†Î•º Ïò®ÎèÑ, ÏäµÎèÑ, ÏßàÌôò ÌäπÏÑ±ÏùÑ Í≥†Î†§ÌïòÏó¨ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.
    """
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=1 
        )
        
        content = response.choices[0].message.content
        llm_result = json.loads(content)
        
        # Merge Results
        final_result = {
            "decision": decision_text,
            "reason": llm_result.get("reason", "Ï†ïÎ≥¥Î•º Î∂àÎü¨Ïò§Îäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."),
            "actionItems": action_items,
            "references": list(set([doc.get("source", "Unknown Source") for doc in relevant_docs])),
            # Add real-time air quality values for frontend display
            "pm25_value": air_data.get("pm25_value"),
            "o3_value": air_data.get("o3_value"),
            "pm10_value": air_data.get("pm10_value"),
            "no2_value": air_data.get("no2_value")
        }
        
        # [Step F] Save to Cache
        if db is not None and cache_key:
            try:
                await db[CACHE_COLLECTION].update_one(
                    {"_id": cache_key},
                    {"$set": {"data": final_result, "created_at": datetime.now(KST_TZ)}},
                    upsert=True
                )
                print(f"üíæ Saved to cache: {cache_key}")
            except Exception as e:
                print(f"Error saving to cache: {e}")
                
        return final_result
        
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        # Fallback even if LLM fails, we satisfy the deterministic requirement
        return {
            "decision": decision_text,
            "reason": "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÎ°ú ÏÉÅÏÑ∏ ÏÑ§Î™ÖÏùÑ Î∂àÎü¨Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÌñâÎèô ÏßÄÏπ®ÏùÄ ÏúÑÏôÄ Í∞ôÏù¥ Ï§ÄÏàòÌï¥Ï£ºÏÑ∏Ïöî.",
            "actionItems": action_items,
            "references": [],
            # Add real-time air quality values for frontend display
            "pm25_value": air_data.get("pm25_value"),
            "o3_value": air_data.get("o3_value"),
            "pm10_value": air_data.get("pm10_value"),
            "no2_value": air_data.get("no2_value")
        }

async def ingest_pdf(file_content: bytes, filename: str) -> Dict[str, Any]:
    """
    Ingest PDF content: Extract text -> Embed -> Store in DB.
    """
    import io
    from PyPDF2 import PdfReader

    if not vo_client:
        return {"status": "error", "message": "Voyage AI Client not initialized"}
    
    if db is None:
        return {"status": "error", "message": "Database not initialized"}

    try:
        # 1. Read PDF
        pdf_file = io.BytesIO(file_content)
        reader = PdfReader(pdf_file)
        
        extracted_text = ""
        documents_to_insert = []
        texts_to_embed = []
        
        # 2. Extract Text per Page (Chunking Strategy: 1 Page = 1 Doc for simplicity)
        print(f"üìÑ Processing PDF: {filename} ({len(reader.pages)} pages)")
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 50: # Ignore empty/too short pages
                extracted_text += text + "\n"
                
                texts_to_embed.append(text)
                documents_to_insert.append({
                    "text": text,
                    "category": "pdf_upload",
                    "source": filename,
                    "page": i + 1,
                    "risk_level": "unknown", # Needs manual classification or LLM analysis
                    "created_at": datetime.now()
                })
        
        if not documents_to_insert:
            return {"status": "error", "message": "No extractable text found in PDF."}

        # 3. Embed Data
        print(f"üß† Embedding {len(texts_to_embed)} pages with Voyage AI...")
        result = vo_client.embed(texts_to_embed, model="voyage-3-large", input_type="document")
        embeddings = result.embeddings
        
        for i, doc in enumerate(documents_to_insert):
            doc["embedding"] = embeddings[i]
            
        # 4. Insert into DB
        insert_result = await db[GUIDELINES_COLLECTION].insert_many(documents_to_insert)
        
        return {
            "status": "success",
            "message": f"Successfully ingested {len(insert_result.inserted_ids)} pages from {filename}",
            "inserted_ids": [str(id) for id in insert_result.inserted_ids]
        }
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        return {"status": "error", "message": str(e)}
