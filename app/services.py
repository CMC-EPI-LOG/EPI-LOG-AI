import os
import json
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional, Any
import voyageai
from openai import OpenAI
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGODB_URI")
VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_NAME = "epilog_db"
GUIDELINES_COLLECTION = "medical_guidelines"
AIR_QUALITY_COLLECTION = "daily_air_quality"
AIR_QUALITY_DATA_COLLECTION = "air_quality_data"  # Lambda cron job collection
VECTOR_INDEX = "vector_index"
KST_TZ = ZoneInfo("Asia/Seoul")

if not MONGO_URI:
    # Fallback to a dummy URI if not set to prevent startup crash, but it will fail on request
    print("WARNING: MONGODB_URI is not set.")
    MONGO_URI = "mongodb://localhost:27017"

# Initialize Clients
try:
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client[DB_NAME]
except Exception as e:
    print(f"Error initializing MongoDB client: {e}")
    mongo_client = None
    db = None

# --- Logic Constants ---
GRADE_MAP = {
    "Ï¢ãÏùå": 1,
    "Î≥¥ÌÜµ": 2,
    "ÎÇòÏÅ®": 3,
    "Îß§Ïö∞ÎÇòÏÅ®": 4
}

REVERSE_GRADE_MAP = {v: k for k, v in GRADE_MAP.items()}

# Correction Weights
HUMIDITY_WEIGHTS = {
    "high": 1.2,  # > 70%
    "low": 1.1,   # < 30%
    "normal": 1.0
}

def _get_corrected_grade(
    base_grade: str, 
    temp: Optional[float], 
    humidity: Optional[float], 
    condition: str,
    pollutant_type: str # "pm25" or "o3"
) -> str:
    """
    Apply correction logic based on temperature, humidity and disease condition.
    Returns the corrected grade string.
    """
    score = GRADE_MAP.get(base_grade, 2)
    
    # 1. Humidity Correction (W_h)
    w_h = 1.0
    if humidity is not None:
        if humidity > 70:
            w_h = HUMIDITY_WEIGHTS["high"]
        elif humidity < 30:
            w_h = HUMIDITY_WEIGHTS["low"]
    
    # 2. Temperature & Disease Trigger Logic
    # Asthma + Cold + PM2.5
    if condition == "asthma" and temp is not None and temp < 5 and pollutant_type == "pm25":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # Rhinitis + Dry + PM2.5
    if condition == "rhinitis" and humidity is not None and humidity < 30 and pollutant_type == "pm25":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # Atopy + Heat + O3
    if condition == "atopy" and temp is not None and temp > 30 and pollutant_type == "o3":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # General + High Humidity + PM2.5 Bad
    if humidity is not None and humidity > 80 and pollutant_type == "pm25" and base_grade == "ÎÇòÏÅ®":
        return "Îß§Ïö∞ÎÇòÏÅ®"

    # Apply multiplicative weight if no specific trigger fired
    # (Simplified: if score * w_h rounds up to next grade)
    final_score = min(4, max(1, round(score * w_h)))
    return REVERSE_GRADE_MAP.get(final_score, base_grade)

# Decision Texts based on logic.csv (80-segment dataset)
DECISION_TEXTS = {
    "infant": {
        "general": {
            "ok": "Ïú†Î™®Ï∞® ÏÇ∞Ï±Ö Í∞ÄÏöî",
            "caution": "ÏßßÏùÄ ÏÇ∞Ï±ÖÎßå Ï∂îÏ≤ú",
            "warning": "Ïô∏Î∂Ä Í≥µÍ∏∞ Ï†ÑÎ©¥ Ï∞®Îã®"
        },
        "rhinitis": {
            "ok": "ÏΩîÍ∞Ä Ìé∏ÏïàÌïú ÎÇ†",
            "caution": "ÏΩîÏ†êÎßâ Î≥¥Ïäµ ÏßëÏ§ë",
            "warning": "Ìò∏Ìù° Í≥§ÎûÄ Ï£ºÏùò"
        },
        "asthma": {
            "ok": "ÏÉÅÏæåÌïòÍ≤å Ïà® Ïâ¨Ïñ¥Ïöî",
            "caution": "Ï∞¨ Î∞îÎûå ÎÖ∏Ï∂ú Ï£ºÏùò",
            "warning": "Ï†àÎåÄ ÏïàÏ†ï Ïã§ÎÇ¥ ÎåÄÍ∏∞"
        },
        "atopy": {
            "ok": "Í∞ÄÎ†§ÏõÄ Í±±Ï†ï Îöù",
            "caution": "ÎïÄÎÇòÎ©¥ Î∞îÎ°ú Îã¶ÏïÑÏöî",
            "warning": "Î≥¥ÏäµÏ†ú 2Î∞∞ ÎèÑÌè¨"
        }
    },
    "toddler": {
        "general": {
            "ok": "ÎÜÄÏù¥ÌÑ∞ÏóêÏÑú Îõ∞ÎÜÄÏïÑÏöî",
            "caution": "Î¨º Ìïú Ïªµ ÎßàÏãúÍ≥† Ïô∏Ï∂ú",
            "warning": "Ïò§ÎäòÏùÄ ÏßëÏóêÏÑú ÎÜÄÏïÑÏöî"
        },
        "rhinitis": {
            "ok": "ÏΩî Î©¥Ïó≠Î†• ÌÇ§Ïö∞Îäî ÎÇ†",
            "caution": "Ïû¨Ï±ÑÍ∏∞ Î®ºÏßÄ Ï°∞Ïã¨",
            "warning": "ÌôòÍ∏∞ Í∏àÏßÄ Î¨ºÍ±∏Î†àÏßà"
        },
        "asthma": {
            "ok": "Í∏∞ÎèÑÍ∞Ä Ïó¥Î¶¨Îäî ÎÇ†Ïî®",
            "caution": "Í∞ëÏûëÏä§Îü∞ Í∏∞Ïπ® Ï£ºÏùò",
            "warning": "Ïã§Ïô∏ Ïù¥Îèô Ï†ÑÎ©¥ Ï†úÌïú"
        },
        "atopy": {
            "ok": "ÌîºÎ∂ÄÍ∞Ä Ïà® Ïâ¨Îäî ÎÇ†",
            "caution": "ÎïÄÍ≥º Î®ºÏßÄÎ•º Î©ÄÎ¶¨Ìï¥Ïöî",
            "warning": "ÌîºÎ∂Ä ÏßÑÏ†ï Ìå© Ï∂îÏ≤ú"
        }
    },
    "elementary_low": {
        "general": {
            "ok": "Ïö¥ÎèôÏû•ÏóêÏÑú ÎßàÏùåÍªè!",
            "caution": "Ï≤¥Ïú° Ï†Ñ ÏÉÅÌÉú ÌôïÏù∏",
            "warning": "Ïã§Ïô∏ ÏàòÏóÖ Ï∞∏Ïó¨ Ï†úÏô∏"
        },
        "rhinitis": {
            "ok": "Ïà≤ Ï≤¥Ìóò Í∞ÄÍ∏∞ Ï¢ãÏùÄ ÎÇ†",
            "caution": "ÎßàÏä§ÌÅ¨ Ïì∞Í≥† Îì±Íµê",
            "warning": "ÏΩîÎßâÌûò Ï¶ùÏÉÅ Í¥ÄÎ¶¨"
        },
        "asthma": {
            "ok": "Ïª®ÎîîÏÖò ÏµúÏÉÅÏù∏ ÎÇ†",
            "caution": "Ïö¥Îèô Í∞ïÎèÑÎ•º Ï°∞Ï†àÌï¥Ïöî",
            "warning": "Î∞úÏûë ÏúÑÌóò Ïã§Ïô∏ Í∏àÏßÄ"
        },
        "atopy": {
            "ok": "ÏûêÏô∏ÏÑ† Ï∞®Îã®Ï†ú ÌïÑÏàò",
            "caution": "Í∏¥ÏÜåÎß§Î°ú ÌîºÎ∂Ä Î≥¥Ìò∏",
            "warning": "Í∞ÄÎ†§ÏõÄÏ¶ù Ïã¨Ìôî Ï£ºÏùò"
        }
    },
    "elementary_high": {
        "general": {
            "ok": "ÌôòÍ∏∞ÌïòÎ©∞ Í≥µÎ∂ÄÌï¥Ïöî",
            "caution": "ÌïòÍµê ÌõÑ ÏÜêÎ∞ú ÏîªÍ∏∞",
            "warning": "Ïã§Ïô∏ ÎÖ∏Ï∂ú ÏµúÏÜåÌôî"
        },
        "rhinitis": {
            "ok": "ÏΩîÍ∞Ä ÏãúÏõêÌïú Îì±Íµ£Í∏∏",
            "caution": "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄÌïòÍ∏∞",
            "warning": "ÌôòÍ∏∞ ÎåÄÏã† Í≥µÏ≤≠Í∏∞"
        },
        "asthma": {
            "ok": "ÌôúÍ∏∞Ï∞®Í≤å Ïö¥ÎèôÌï¥Ïöî",
            "caution": "Ïö¥Îèô Ï†ÑÌõÑ ÏàòÎ∂Ñ Î≥¥Ï∂©",
            "warning": "Ïã§Ïô∏ Ïù¥Îèô Ï†ÑÎ©¥ Ï†úÌïú"
        },
        "atopy": {
            "ok": "ÏÉÅÏæåÌïú ÏïºÏô∏ ÌôúÎèô",
            "caution": "Î≥¥ÏäµÎßâ Ïú†ÏßÄÌïòÍ∏∞",
            "warning": "ÌîºÎ∂Ä ÏßÑÏ†ï Î∞è Ï∞®Îã®"
        }
    },
    "teen_adult": {
        "general": {
            "ok": "ÏïºÏô∏ Ïö¥Îèô Ï∂îÏ≤ú",
            "caution": "ÏùºÏÉÅ ÌôúÎèô Î¨¥Í¥Ä",
            "warning": "Ïã§Ïô∏ ÌôúÎèô ÏµúÏÜåÌôî"
        },
        "rhinitis": {
            "ok": "ÏÉÅÏæåÌïú Ìò∏Ìù°",
            "caution": "ÏΩî ÏÑ∏Ï†ï Ï§ÄÎπÑ",
            "warning": "Ïô∏Î∂Ä Í≥µÍ∏∞ Ï∞®Îã®"
        },
        "asthma": {
            "ok": "Ïú†ÏÇ∞ÏÜå Ïö¥Îèô Í∞ÄÎä•",
            "caution": "Î¨¥Î¶¨Ìïú ÌôúÎèô ÏûêÏ†ú",
            "warning": "Ïã§Ïô∏ ÌôúÎèô Ï†ÑÎ©¥ Ï†úÌïú"
        },
        "atopy": {
            "ok": "ÏïºÏô∏ ÎÇòÎì§Ïù¥ Ï∂îÏ≤ú",
            "caution": "ÌîºÎ∂Ä Ï≤≠Í≤∞ Ïú†ÏßÄ",
            "warning": "Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌîºÎ∂Ä ÏÑ∏Ï†ï"
        }
    }
}

# Action Items templates based on logic.csv
ACTION_ITEMS = {
    "infant": {
        "general": {
            "ok": ["Ïú†Î™®Ï∞® ÏÇ∞Ï±Ö", "15Î∂Ñ ÌôòÍ∏∞", "Í∑ÄÍ∞Ä ÌõÑ ÏÜêÎ∞ú ÏîªÍ∏∞"],
            "caution": ["Ïú†Î™®Ï∞® Ïª§Î≤Ñ ÏÇ¨Ïö©", "Í∑∏Îäò ÏúÑÏ£º ÏÇ∞Ï±Ö", "Î≥µÍ∑Ä ÌõÑ Î≥¥Ïäµ"],
            "warning": ["Ïô∏Ï∂ú Ï†àÎåÄ Í∏àÏßÄ", "Ï∞ΩÎ¨∏ ÌãàÏÉà Î∞ÄÌèê", "Î¨ºÍ±∏Î†à Ï≤≠ÏÜå"]
        },
        "rhinitis": {
            "ok": ["ÏæåÏ†ÅÌïú ÌôòÍ∏∞", "ÏßëÏïà Î®ºÏßÄ ÌÑ∏Í∏∞", "Í∞ÄÎ≤ºÏö¥ Ïô∏Ï∂ú"],
            "caution": ["Í∞ÄÏäµÍ∏∞ Í∞ÄÎèô", "ÎØ∏ÏßÄÍ∑ºÌïú Î¨º ÏÑ≠Ï∑®", "Ïú†Î™®Ï∞® Í∞ÄÎ¶ºÎßâ"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞", "ÏÉÅÎπÑÏïΩ ÌôïÏù∏", "ÏÉÅÌÉú ÏßëÏ§ë Î™®ÎãàÌÑ∞"]
        },
        "asthma": {
            "ok": ["Ïã†ÏÑ†Ìïú Í≥µÍ∏∞ Ïú†ÏûÖ", "Î≥¥Ìò∏ÏûêÏôÄ ÏÇ∞Ï±Ö", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù"],
            "caution": ["Î™© Í∞ÄÏã∏Í∞ú ÏÇ¨Ïö©", "Í∏âÍ≤©Ìïú Ïò®ÎèÑÏ∞® Ï£ºÏùò", "Ìò∏Ìù° ÏàòÏãú Í¥ÄÏ∞∞"],
            "warning": ["Î≥ëÏõê Ïô∏ Ïô∏Ï∂ú Í∏àÏßÄ", "Ï¶ùÏÉÅ ÎåÄÏùë Ï§ÄÎπÑ", "Í≥µÏ≤≠Í∏∞ ÌíÄÍ∞ÄÎèô"]
        },
        "atopy": {
            "ok": ["Ïô∏Ï∂ú Ï†Ñ ÏÑ†ÌÅ¨Î¶º", "ÌôúÎèô ÌõÑ ÏÑ∏Ïïà", "Î©¥ ÏÜåÏû¨ Ïò∑ ÏûÖÍ∏∞"],
            "caution": ["ÏÜêÏàòÍ±¥ ÏßÄÏ∞∏", "ÏàòÏãúÎ°ú Î≥¥ÏäµÏ†ú", "ÏñáÏùÄ Í∏¥ÏÜåÎß§ Ïò∑"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ Ï≤¥Î•ò", "ÏãúÏõêÌïú Ïò®ÎèÑ Ïú†ÏßÄ", "Í≥†Î≥¥Ïäµ ÌÅ¨Î¶º ÏÇ¨Ïö©"]
        }
    },
    "toddler": {
        "general": {
            "ok": ["ÏïºÏô∏ ÎÜÄÏù¥ Í∂åÏû•", "Ï†ÑÎ©¥ ÌôòÍ∏∞ ÏãúÌÇ§Í∏∞", "ÌôúÎèô ÌõÑ ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "caution": ["Ï§ëÍ∞Ñ ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄÌïòÍ∏∞", "Ïû•ÏãúÍ∞Ñ Ï≤¥Î•ò ÏûêÏ†ú"],
            "warning": ["ÏïºÏô∏ ÌôúÎèô Í∏àÏßÄ", "Í≥µÍ∏∞Ï≤≠Ï†ïÍ∏∞ ÏÇ¨Ïö©", "Ïã§ÎÇ¥ Ï†ÅÏ†ï Í∞ÄÏäµ"]
        },
        "rhinitis": {
            "ok": ["Ïà≤ Ï≤¥Ìóò Ï∂îÏ≤ú", "ÌôòÍ∏∞ ÌõÑ Ï≤≠ÏÜå", "Ïô∏Ï∂ú ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÎßàÏä§ÌÅ¨ ÌïÑÏàò Ï∞©Ïö©", "ÏΩî Ï£ºÎ≥Ä Î≥¥Ïäµ", "ÏãùÏóºÏàò ÏÑ∏Ï≤ô"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞", "Í≥µÏ≤≠Í∏∞ Í∞ÄÎèô", "Ï¶ùÏÉÅ Ïãú ÏïΩ Î≥µÏö©"]
        },
        "asthma": {
            "ok": ["Ïú†ÏÇ∞ÏÜå ÎÜÄÏù¥", "Í∑úÏπôÏ†Å ÏïΩ Î≥µÏö©", "Ïã§ÎÇ¥Ïô∏ Í≥µÍ∏∞ Ï†ïÌôî"],
            "caution": ["Ïö¥Îèô Í∞ïÎèÑ ÎÇÆÏ∂îÍ∏∞", "Ï§ëÍ∞Ñ Ìú¥Ïãù Ï∑®ÌïòÍ∏∞", "Ìò∏Ìù° ÏÉÅÌÉú ÌôïÏù∏"],
            "warning": ["Ïô∏Ï∂ú Ï†àÎåÄ Í∏àÏßÄ", "Î≥¥Ìò∏Ïûê Î∞ÄÏ∞© Í¥ÄÏ∞∞", "ÎπÑÏÉÅ Ïãú Î≥ëÏõê Î∞©Î¨∏"]
        },
        "atopy": {
            "ok": ["ÏÑ†ÌÅ¨Î¶º ÎèÑÌè¨", "ÌôúÎèô ÌõÑ ÏÉ§Ïõå", "Î©¥ ÏÜåÏû¨ Ïò∑ Ï∂îÏ≤ú"],
            "caution": ["ÏàòÏãúÎ°ú ÎïÄ Îã¶Í∏∞", "Ïô∏Ï∂ú ÌõÑ Î≥¥Ïäµ", "ÌÜµÍ∏∞ÏÑ± ÏùòÎ•ò"],
            "warning": ["ÎÉâÏ∞úÏßà ÏßÑÏ†ï", "Ïã§Ïô∏ ÌôúÎèô Ï§ëÎã®", "ÏûêÍ∑π ÏóÜÎäî Î°úÏÖò"]
        }
    },
    "elementary_low": {
        "general": {
            "ok": ["Ï∂ïÍµ¨/Îã¨Î¶¨Í∏∞ Ï∂îÏ≤ú", "ÍµêÏã§ Ï†ÑÎ©¥ ÌôòÍ∏∞", "ÏïºÏô∏ ÌïôÏäµ"],
            "caution": ["Ï§ëÍ∞Ñ ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÌôúÎèô ÌõÑ ÏñëÏπò", "ÎåÄÍ∏∞Ïßà ÏàòÏãú Ï≤¥ÌÅ¨"],
            "warning": ["Ïã§ÎÇ¥ ÌôúÎèô Ï†ÑÌôò", "Ï∞ΩÎ¨∏ Î∞ÄÌèê Í¥ÄÎ¶¨", "Ïã§ÎÇ¥ Í≥µÍ∏∞ Ï†ïÌôî"]
        },
        "rhinitis": {
            "ok": ["ÏïºÏô∏ ÏÇ∞Ï±Ö", "ÌôòÍ∏∞ ÌõÑ ÎåÄÏ≤≠ÏÜå", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù"],
            "caution": ["ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "ÏÜê ÏîªÍ∏∞ ÍµêÏú°", "Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞"],
            "warning": ["ÌôòÍ∏∞ Ï†àÎåÄ Í∏àÏßÄ", "ÏãùÏóºÏàò ÏΩî ÏÑ∏Ï≤ô", "Ï¶ùÏÉÅ ÏôÑÌôîÏ†ú Ï§ÄÎπÑ"]
        },
        "asthma": {
            "ok": ["ÌïôÍµê Ï≤¥Ïú° Ï∞∏Ïó¨", "ÍπäÏùÄ Ìò∏Ìù° Ïö¥Îèô", "Ïª®ÎîîÏÖò Ïú†ÏßÄ"],
            "caution": ["Î¨¥Î¶¨Ìïú Îã¨Î¶¨Í∏∞ ÏûêÏ†ú", "Ï§ëÍ∞Ñ Ìú¥Ïãù ÎäòÎ¶¨Í∏∞", "Ìò∏Ìù° Î™®ÎãàÌÑ∞ÎßÅ"],
            "warning": ["ÎÖ∏Ï∂ú Ï†ÑÎ©¥ Ï∞®Îã®", "Î≥¥Ìò∏Ïûê ÏÉÅÏãú Í¥ÄÏ∞∞", "ÎπÑÏÉÅÏïΩ ÏúÑÏπò ÌôïÏù∏"]
        },
        "atopy": {
            "ok": ["ÏÑ†ÌÅ¨Î¶º Î∞îÎ•¥Í∏∞", "ÏïºÏô∏ ÌôúÎèô Ï¶êÍ∏∞Í∏∞", "ÌôúÎèô ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÏÜêÏàòÍ±¥ ÏßÄÏ∞∏", "Ïô∏Ï∂ú ÌõÑ Î≥¥ÏäµÏ†ú", "Î©¥ ÏÜçÏò∑ ÏûÖÌûàÍ∏∞"],
            "warning": ["Ïô∏Ï∂ú Í∏àÏßÄ", "ÎÉâÏ∞úÏßà ÏßÑÏ†ï", "Ïã§ÎÇ¥ ÏäµÎèÑ Ï°∞Ï†à"]
        }
    },
    "elementary_high": {
        "general": {
            "ok": ["Ï†ÑÎ©¥ ÌôòÍ∏∞ Ïã§Ïãú", "ÏïºÏô∏ Ïö¥Îèô Í∂åÏû•", "ÏûêÏ†ÑÍ±∞/ÎèÑÎ≥¥ Îì±Íµê"],
            "caution": ["Ïô∏Ï∂ú ÌõÑ ÏúÑÏÉù Í¥ÄÎ¶¨", "Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞", "ÍµêÏã§ ÌôòÍ∏∞ ÌòëÏ°∞"],
            "warning": ["Ïã§Ïô∏ ÌôúÎèô Ï†ÑÎ©¥ Ï§ëÎã®", "Ï∞ΩÎ¨∏ Î∞ÄÌèê ÌôïÏù∏", "Í≥µÏ≤≠Í∏∞ Í∞ÄÎèô"]
        },
        "rhinitis": {
            "ok": ["ÏÉÅÏæåÌïú ÏïÑÏπ® ÏÇ∞Ï±Ö", "ÍµêÏã§ ÌôòÍ∏∞ Í∂åÏû•", "Í∑úÏπôÏ†Å ÏàòÎ©¥ Í¥ÄÎ¶¨"],
            "caution": ["ÏÜêÏàòÍ±¥/ÎßàÏä§ÌÅ¨ ÏßÄÏ∞∏", "ÏΩßÎ¨º Ï¶ùÏÉÅ Í¥ÄÎ¶¨", "Ïã§ÎÇ¥ ÏäµÎèÑ Ï°∞Ï†à"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞", "Î¨ºÍ±∏Î†à Ï≤≠ÏÜå", "Ï¶ùÏÉÅ Ïãú ÏïΩ Î≥µÏö©"]
        },
        "asthma": {
            "ok": ["Ïö¥ÎèôÏû• ÌôúÎèô Í∂åÏû•", "ÍπäÏùÄ Ìò∏Ìù° Ïó∞Ïäµ", "Í∑úÏπôÏ†Å Ìà¨ÏïΩ Ïú†ÏßÄ"],
            "caution": ["Ï∂©Î∂ÑÌïú Î¨º ÏÑ≠Ï∑®", "Ïª®ÎîîÏÖò Ï≤¥ÌÅ¨", "Î¨¥Î¶¨Ìïú Îã¨Î¶¨Í∏∞ ÏûêÏ†ú"],
            "warning": ["ÏïºÏô∏ ÌïôÏõê Ïù¥Îèô ÏûêÏ†ú", "Î≥¥Ìò∏Ïûê Î∞ÄÏ∞© ÌôïÏù∏", "ÎπÑÏÉÅ Ïó∞ÎùΩÎßù Ï†êÍ≤Ä"]
        },
        "atopy": {
            "ok": ["Î≥¥ÏäµÏ†ú ÎèÑÌè¨ ÌõÑ Ïô∏Ï∂ú", "Í∞ÄÎ≤ºÏö¥ Ïö¥Îèô", "ÎïÄ Îã¶Í∏∞"],
            "caution": ["Î≥¥ÏäµÏ†ú Ìú¥ÎåÄ", "Ïô∏Ï∂ú ÌõÑ ÏÑ∏Ïïà", "Î©¥ ÏÜåÏû¨ ÏùòÎ•ò"],
            "warning": ["Ïã§Ïô∏ ÌôúÎèô Ï§ëÎã®", "ÎÉâÏ∞úÏßà ÏßÑÏ†ï", "Í≥†Î≥¥Ïäµ Í¥ÄÎ¶¨"]
        }
    },
    "teen_adult": {
        "general": {
            "ok": ["Ï°∞ÍπÖ/Îì±ÏÇ∞ Í∂åÏû•", "Ï†ÑÎ©¥ ÌôòÍ∏∞ Ïã§Ïãú", "ÏïºÏô∏ ÌïôÏäµ/ÏóÖÎ¨¥"],
            "caution": ["Ï∂©Î∂ÑÌïú ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÏÜê ÏîªÍ∏∞ ÏÉùÌôúÌôî", "Í∞ÄÎ≤ºÏö¥ ÌôòÍ∏∞"],
            "warning": ["Ïô∏Ï∂ú ÏûêÏ†ú", "Î≥¥Í±¥Ïö© ÎßàÏä§ÌÅ¨ ÌïÑÏ∞©", "Î¨ºÍ±∏Î†à Ï≤≠ÏÜå"]
        },
        "rhinitis": {
            "ok": ["Ïπ®Íµ¨Î•ò ÌñáÎ≥ï ÏÜåÎèÖ", "Ï†ÑÎ©¥ ÌôòÍ∏∞", "Í∞ÄÎ≤ºÏö¥ ÏÇ∞Ï±Ö"],
            "caution": ["ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "Í∑ÄÍ∞Ä ÌõÑ ÏΩî ÏÑ∏Ï≤ô", "Ïã§ÎÇ¥ ÏäµÎèÑ Ïú†ÏßÄ"],
            "warning": ["Ïô∏Ï∂ú Í∏àÏßÄ", "Ï∞ΩÎ¨∏ Î∞ÄÌèê", "ÏïΩÎ¨º Î≥µÏö© Ï†êÍ≤Ä"]
        },
        "asthma": {
            "ok": ["Í∑úÏπôÏ†Å Ïö¥Îèô", "Ïã§ÎÇ¥Ïô∏ ÌôòÍ∏∞", "Ïª®ÎîîÏÖò Í¥ÄÎ¶¨"],
            "caution": ["Ï¶ùÏÉÅ Ïú†Î¨¥ ÌôïÏù∏", "Î¨¥Î¶¨Ìïú Îì±ÏÇ∞ ÏûêÏ†ú", "ÎπÑÏÉÅÏïΩ ÏßÄÏ∞∏"],
            "warning": ["Ïã§ÎÇ¥ ÏïàÏã¨ ÎåÄÍ∏∞", "Í≥µÍ∏∞Ïßà Í¥ÄÎ¶¨", "ÎπÑÏÉÅ Ïãú ÏùòÎ£åÍ∏∞Í¥Ä"]
        },
        "atopy": {
            "ok": ["Ï∂©Î∂ÑÌïú Î≥¥Ïäµ ÌõÑ Ïô∏Ï∂ú", "ÏûêÏô∏ÏÑ† Ï∞®Îã®", "Î©¥ ÏÜåÏû¨ ÏùòÎ•ò"],
            "caution": ["Ïô∏Ï∂ú ÌõÑ Í∞ÄÎ≤ºÏö¥ ÏÉ§Ïõå", "Î≥¥ÏäµÏ†ú ÎèÑÌè¨", "ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "warning": ["ÏïºÏô∏ ÌôúÎèô Ï§ëÎã®", "Ï†ÄÏûêÍ∑π ÏÑ∏Ïïà Î∞è ÏÉ§Ïõå", "Í≥†Î≥¥Ïäµ ÏßÑÏ†ï Í¥ÄÎ¶¨"]
        }
    }
}

def _calculate_decision(pm25_grade: str, o3_grade: str) -> str:
    """
    Calculate decision level: 'ok', 'caution', 'warning'
    
    Logic (1:Ï¢ãÏùå, 2:Î≥¥ÌÜµ, 3:ÎÇòÏÅ®, 4:Îß§Ïö∞ÎÇòÏÅ®):
    ‚Ä¢ OK: PM2.5 <= 2 AND O3 <= 2
    ‚Ä¢ Caution: Either one is 3
    ‚Ä¢ Warning: Either one is 4 OR Both are 3
    """
    p_score = GRADE_MAP.get(pm25_grade, 2)
    o_score = GRADE_MAP.get(o3_grade, 2)
    
    # Check Warning Conditions
    if p_score >= 4 or o_score >= 4:
        return "warning"
    if p_score == 3 and o_score == 3:
        return "warning"
        
    # Check Caution Conditions
    if p_score == 3 or o_score == 3:
        return "caution"
        
    # Default OK
    return "ok"

def _normalize_age_group(age_group: Any) -> str:
    if age_group is None:
        return "elementary_high"
    raw = str(age_group).strip().lower()
    
    # Updated 5 groups based on planning document
    if raw in {"infant", "ÏòÅÏïÑ", "0-2", "0~2"}:
        return "infant"
    if raw in {"toddler", "Ïú†ÏïÑ", "3-6", "3~6"}:
        return "toddler"
    if raw in {"elementary_low", "Ï¥àÎì±Ï†ÄÌïôÎÖÑ", "Ï¥àÎì± Ï†ÄÌïôÎÖÑ", "7-9", "7~9", "1-3", "1~3"}:
        return "elementary_low"
    if raw in {"elementary_high", "Ï¥àÎì±Í≥†ÌïôÎÖÑ", "Ï¥àÎì± Í≥†ÌïôÎÖÑ", "10-12", "10~12", "4-6", "4~6"}:
        return "elementary_high"
    if raw in {"teen", "teen_adult", "Ï≤≠ÏÜåÎÖÑ", "ÏÑ±Ïù∏", "adult", "13-18", "13~18", "13+"}:
        return "teen_adult"
    
    # Fallbacks
    if "ÏòÅÏïÑ" in raw: return "infant"
    if "Ïú†ÏïÑ" in raw: return "toddler"
    if "Ï†ÄÌïôÎÖÑ" in raw: return "elementary_low"
    if "Í≥†ÌïôÎÖÑ" in raw: return "elementary_high"
    if "Ï≤≠ÏÜåÎÖÑ" in raw or "ÏÑ±Ïù∏" in raw: return "teen_adult"
    
    return "elementary_high"

def _get_display_content(age_group: str, condition: str, decision_key: str):
    """
    Returns (decision_text, action_items)
    """
    # Normalize condition
    cond_key = condition if condition in ["general", "rhinitis", "asthma", "atopy"] else "general"
    
    # Get Text
    group_data = DECISION_TEXTS.get(age_group, DECISION_TEXTS["elementary_high"])
    cond_data = group_data.get(cond_key, group_data.get("general", {}))
    d_text = cond_data.get(decision_key, "ÏÉÅÌÉú ÌôïÏù∏ ÌïÑÏöî")
    
    # Get Actions
    group_actions = ACTION_ITEMS.get(age_group, ACTION_ITEMS.get("toddler", {}))
    cond_actions = group_actions.get(cond_key, group_actions.get("general", {}))
    actions = cond_actions.get(decision_key, ["ÏÉÅÌÉúÏóê Îî∞Î•∏ Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§."])
    
    return d_text, actions[:] # Return a copy

try:
    vo_client = voyageai.Client(api_key=VOYAGE_API_KEY)
except Exception as e:
    print(f"Error initializing Voyage AI client: {e}")
    vo_client = None

try:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    openai_client = None

async def get_air_quality_from_mongodb(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch latest air quality data from MongoDB air_quality_data collection.
    This collection is populated by AWS Lambda cron job every hour.
    Returns None if no recent data (> 2 hours old) or not found.
    """
    if db is None:
        return None
    
    try:
        # Query for the station with most recent data
        query = {"stationName": station_name}
        
        # Sort by dataTime descending to get latest entry
        cursor = db[AIR_QUALITY_DATA_COLLECTION].find(query).sort("createdAt", -1).limit(1)
        doc = await cursor.to_list(length=1)
        
        if not doc:
            print(f"‚ö†Ô∏è  No MongoDB data found for station: {station_name}")
            return None
        
        data = doc[0]
        
        # Check data freshness (must be within 2 hours)
        created_at = data.get("createdAt")
        if created_at:
            from datetime import datetime, timedelta
            now = datetime.now(KST_TZ)
            
            # Handle both datetime objects and strings
            if isinstance(created_at, str):
                created_at = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            
            # Make timezone-aware if needed
            if created_at.tzinfo is None:
                created_at = created_at.replace(tzinfo=KST_TZ)
            
            age = now - created_at
            if age > timedelta(hours=2):
                print(f"‚ö†Ô∏è  MongoDB data is stale ({age.total_seconds()/3600:.1f} hours old)")
                return None
        
        # Convert grade strings to Korean text
        grade_map = {"1": "Ï¢ãÏùå", "2": "Î≥¥ÌÜµ", "3": "ÎÇòÏÅ®", "4": "Îß§Ïö∞ÎÇòÏÅ®"}
        
        # Transform to expected format
        result = {
            "stationName": data.get("stationName", station_name),
            "sidoName": data.get("sidoName", ""),
            "pm25_grade": grade_map.get(str(data.get("pm25Grade", "2")), "Î≥¥ÌÜµ"),
            "pm25_value": data.get("pm25Value", 50),
            "pm10_grade": grade_map.get(str(data.get("pm10Grade", "2")), "Î≥¥ÌÜµ"),
            "pm10_value": data.get("pm10Value", 70),
            "o3_grade": grade_map.get(str(data.get("o3Grade", "1")), "Ï¢ãÏùå"),
            "o3_value": data.get("o3Value", 0.05),
            "no2_grade": grade_map.get(str(data.get("no2Grade", "1")), "Ï¢ãÏùå"),
            "no2_value": data.get("no2Value", 0.02),
            "co_grade": grade_map.get(str(data.get("coGrade", "1")), "Ï¢ãÏùå"),
            "co_value": data.get("coValue", 0.5),
            "so2_grade": grade_map.get(str(data.get("so2Grade", "1")), "Ï¢ãÏùå"),
            "so2_value": data.get("so2Value", 0.003),
            # Note: Lambda data doesn't include temp/humidity yet
            # These will be added when weather API is integrated
            "temp": None,
            "humidity": None,
            "dataTime": data.get("dataTime", "")
        }
        
        print(f"‚úÖ Fetched air quality for {station_name} from MongoDB (Lambda data)")
        return result
        
    except Exception as e:
        print(f"‚ùå Error fetching from MongoDB: {e}")
        return None

async def get_air_quality_from_airkorea_api(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Direct fallback to Air Korea OpenAPI.
    This replaces the dependency on EPI-LOG-AIRKOREA service.
    """
    import httpx
    
    # Air Korea OpenAPI endpoint (replace with actual endpoint if different)
    # Note: This is a placeholder - actual Air Korea API integration would require
    # API key and proper endpoint configuration
    
    # For now, we'll try the old EPI-LOG-AIRKOREA service as temporary fallback
    # TODO: Replace with direct Air Korea OpenAPI call
    AIRKOREA_API_URL = "https://epi-log-airkorea.vercel.app/api/stations"
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                AIRKOREA_API_URL,
                params={"stationName": station_name}
            )
            
            if response.status_code == 200:
                data = response.json()
                
                # API returns array, take first item
                if data and len(data) > 0:
                    station = data[0]
                    realtime = station.get("realtime", {})
                    
                    # Convert grade numbers to Korean text
                    grade_map = {1: "Ï¢ãÏùå", 2: "Î≥¥ÌÜµ", 3: "ÎÇòÏÅ®", 4: "Îß§Ïö∞ÎÇòÏÅ®"}
                    
                    # Extract and normalize data
                    result = {
                        "stationName": station.get("stationName", station_name),
                        "pm25_grade": grade_map.get(realtime.get("pm25", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "pm25_value": realtime.get("pm25", {}).get("value") or 50,
                        "pm10_grade": grade_map.get(realtime.get("pm10", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "pm10_value": realtime.get("pm10", {}).get("value") or 70,
                        "o3_grade": grade_map.get(realtime.get("o3", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "o3_value": realtime.get("o3", {}).get("value") or 0.05,
                        "no2_grade": grade_map.get(realtime.get("no2", {}).get("grade"), "Ï¢ãÏùå"),
                        "no2_value": realtime.get("no2", {}).get("value") or 0.02,
                        "co_grade": grade_map.get(realtime.get("co", {}).get("grade"), "Ï¢ãÏùå"),
                        "co_value": realtime.get("co", {}).get("value") or 0.5,
                        "so2_grade": grade_map.get(realtime.get("so2", {}).get("grade"), "Ï¢ãÏùå"),
                        "so2_value": realtime.get("so2", {}).get("value") or 0.003,
                        "temp": None,
                        "humidity": None
                    }
                    
                    print(f"‚úÖ Fetched air quality for {station_name} from Air Korea API (fallback)")
                    return result
        
        print(f"‚ö†Ô∏è  No data from Air Korea API for {station_name}")
        return None
        
    except Exception as e:
        print(f"‚ùå Error fetching from Air Korea API: {e}")
        return None

async def get_air_quality(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch air quality data with priority order:
    1. MongoDB air_quality_data (Lambda cron job data) - PRIORITY
    2. Air Korea OpenAPI (fallback for real-time data)
    3. Mock data (final fallback)
    
    Note: Temperature and humidity are not yet available from Lambda data.
    They will be added when weather API integration is complete.
    """
    # Priority 1: Try MongoDB (Lambda-stored data)
    data = await get_air_quality_from_mongodb(station_name)
    if data:
        # Add default temp/humidity for now (will be replaced with weather API)
        if data.get("temp") is None:
            data["temp"] = 22.0  # Default value
        if data.get("humidity") is None:
            data["humidity"] = 45.0  # Default value
        return data
    
    # Priority 2: Try Air Korea API (temporary fallback)
    data = await get_air_quality_from_airkorea_api(station_name)
    if data:
        # Add default temp/humidity
        if data.get("temp") is None:
            data["temp"] = 22.0
        if data.get("humidity") is None:
            data["humidity"] = 45.0
        return data
    
    # Priority 3: Return mock data (final fallback)
    print(f"‚ö†Ô∏è  Using mock data for {station_name}")
    return {
        "stationName": station_name,
        "pm10_grade": "ÎÇòÏÅ®",
        "pm10_value": 85,
        "pm25_grade": "ÎÇòÏÅ®",
        "pm25_value": 65,
        "co_grade": "Î≥¥ÌÜµ",
        "co_value": 0.7,
        "o3_grade": "Î≥¥ÌÜµ",
        "o3_value": 0.065,
        "no2_grade": "Ï¢ãÏùå",
        "no2_value": 0.025,
        "so2_grade": "Ï¢ãÏùå",
        "so2_value": 0.004,
        "temp": 22.0,
        "humidity": 45.0
    }

CACHE_COLLECTION = "rag_cache"
CACHE_TTL_SECONDS = 60 * 60 * 30  # 30 hours
_cache_ttl_index_ready = False

def _generate_cache_key(air_data: Dict[str, Any], user_profile: Dict[str, Any]) -> str:
    grade_map = {"Ï¢ãÏùå": 1, "Î≥¥ÌÜµ": 2, "ÎÇòÏÅ®": 3, "Îß§Ïö∞ÎÇòÏÅ®": 4}
    
    pm25 = grade_map.get(air_data.get("pm25_grade", ""), 0)
    pm10 = grade_map.get(air_data.get("pm10_grade", ""), 0)
    o3 = grade_map.get(air_data.get("o3_grade", ""), 0) # Added o3 as per user example
    
    age_group = _normalize_age_group(user_profile.get("ageGroup"))
    condition = user_profile.get("condition", "unknown")
    date_key = air_data.get("date") or datetime.now(KST_TZ).strftime("%Y-%m-%d")
    
    # Key format: pm25:3_pm10:2_o3:1_age:adult_cond:asthma_date:2026-01-28
    return f"pm25:{pm25}_pm10:{pm10}_o3:{o3}_age:{age_group}_cond:{condition}_date:{date_key}"

async def _ensure_cache_ttl_index():
    global _cache_ttl_index_ready
    if _cache_ttl_index_ready or db is None:
        return
    try:
        await db[CACHE_COLLECTION].create_index(
            "created_at",
            expireAfterSeconds=CACHE_TTL_SECONDS,
            name="rag_cache_ttl"
        )
        _cache_ttl_index_ready = True
    except Exception as e:
        print(f"‚ö†Ô∏è Cache TTL index creation failed: {e}")

async def get_medical_advice(station_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main orchestration function with correction logic.
    """
    # Step A: Get Air Quality
    air_data = await get_air_quality(station_name)
    if not air_data:
        raise ValueError(f"No air quality data found for station: {station_name}")

    # Extract Weather Info for Correction
    temp = air_data.get("temp")
    humidity = air_data.get("humidity")
    user_condition = user_profile.get("condition", "Í±¥Í∞ïÌï®")
    age_group_raw = user_profile.get("ageGroup")
    age_group = _normalize_age_group(age_group_raw)

    # Apply Correction Logic to get "Sensed" grades
    pm25_raw = air_data.get("pm25_grade", "Î≥¥ÌÜµ")
    o3_raw = air_data.get("o3_grade", "Î≥¥ÌÜµ")
    
    pm25_corrected = _get_corrected_grade(pm25_raw, temp, humidity, user_condition, "pm25")
    o3_corrected = _get_corrected_grade(o3_raw, temp, humidity, user_condition, "o3")

    cache_key = ""
    # [Step A.1] Check Cache
    if db is not None:
        try:
            await _ensure_cache_ttl_index()
            # Simple key extension: add T/H to capture environmental context
            cache_key = _generate_cache_key(air_data, user_profile) + f"_T:{temp}_H:{humidity}"
            cached_entry = await db[CACHE_COLLECTION].find_one({"_id": cache_key})
            
            if cached_entry:
                print(f"‚úÖ Cache Hit! Key: {cache_key}")
                return cached_entry["data"]
        except Exception as e:
            print(f"‚ö†Ô∏è Cache check failed: {e}")

    # Determine main issue for search (using corrected grades)
    main_condition = "Î≥¥ÌÜµ"
    if pm25_corrected in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"Ï¥àÎØ∏ÏÑ∏Î®ºÏßÄ {pm25_corrected}"
    elif air_data.get("pm10_grade") in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"ÎØ∏ÏÑ∏Î®ºÏßÄ {air_data['pm10_grade']}"
    elif o3_corrected in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"Ïò§Ï°¥ {o3_corrected}"
        
    # Step B: Query Construction
    search_query = f"{main_condition} ÏÉÅÌô©ÏóêÏÑú {user_condition} {age_group} ÌñâÎèô ÏöîÎ†π Ï£ºÏùòÏÇ¨Ìï≠"
    print(f"Generated Search Query (Primary): {search_query}")

    # Step C: Vector Search
    relevant_docs = []
    if vo_client and db is not None:
        try:
            # 1. Primary Search
            embed_result = vo_client.embed([search_query], model="voyage-3-large", input_type="query")
            query_vector = embed_result.embeddings[0]
            
            pipeline = [
                {
                    "$vectorSearch": {
                        "index": VECTOR_INDEX,
                        "path": "embedding",
                        "queryVector": query_vector,
                        "numCandidates": 100,
                        "limit": 3
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "text": 1,
                        "category": 1,
                        "risk_level": 1,
                        "source": 1,
                        "score": {"$meta": "vectorSearchScore"}
                    }
                }
            ]
            
            cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
            relevant_docs = await cursor.to_list(length=3)
            
            # 2. Fallback Search (If no docs found)
            if not relevant_docs:
                print("‚ö†Ô∏è Primary search returned no results. Attempting fallback (General) search.")
                fallback_query = f"{main_condition} ÌñâÎèô ÏöîÎ†π"
                embed_result_fb = vo_client.embed([fallback_query], model="voyage-3-large", input_type="query")
                query_vector_fb = embed_result_fb.embeddings[0]
                
                pipeline[0]["$vectorSearch"]["queryVector"] = query_vector_fb
                
                cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
                relevant_docs = await cursor.to_list(length=3)
                
        except Exception as e:
            print(f"Error during vector search: {e}")
            pass

    # Step D: LLM Generation
    if not openai_client:
         return {
            "decision": "Error",
            "reason": "OpenAI Client not initialized",
            "actionItems": [],
            "references": []
        }

    # [Logic Update] Calculate Deterministic Decision & Action Items using CORRECTED grades
    decision_key = _calculate_decision(pm25_corrected, o3_corrected)
    decision_text, action_items = _get_display_content(age_group, user_condition, decision_key)
    
    # O3 Special Handling: Force-Append and Warnings
    is_o3_dominant = GRADE_MAP.get(o3_corrected, 1) >= GRADE_MAP.get(pm25_corrected, 1)
    if is_o3_dominant and GRADE_MAP.get(o3_corrected, 1) >= 3: # 'ÎÇòÏÅ®' Ïù¥ÏÉÅ
        decision_text += " (Ïò§Ï°¥ÏùÄ ÎßàÏä§ÌÅ¨Î°ú Í±∏Îü¨ÏßÄÏßÄ ÏïäÏïÑÏöî!)"
        # Force-Append Action Item
        o3_force_action = "Ïò§ÌõÑ 2~5Ïãú ÏÇ¨Ïù¥ÏóêÎäî Ïã§Ïô∏ ÌôúÎèôÏùÑ Ï†ÑÎ©¥ Í∏àÏßÄÌïòÍ≥† Ïã§ÎÇ¥Ïóê Î®∏Î¨¥Î•¥ÏÑ∏Ïöî."
        if o3_force_action not in action_items:
            action_items.append(o3_force_action)

    # Infant Special Warning
    if age_group == "infant":
        infant_warning = "‚Äª Ï£ºÏùò: ÎßàÏä§ÌÅ¨ Ï∞©Ïö© Í∏àÏßÄ(ÏßàÏãù ÏúÑÌóò)"
        if infant_warning not in action_items:
            action_items.insert(0, infant_warning) # Put at top

    # Logic for dual bad condition text append
    if GRADE_MAP.get(pm25_corrected, 1) >= 3 and GRADE_MAP.get(o3_corrected, 1) >= 3:
        decision_text += " (ÎØ∏ÏÑ∏Î®ºÏßÄÏôÄ Ïò§Ï°¥ Îëò Îã§ ÎÜíÏïÑÏöî!)"

    # Prepare Context
    context_text = "\n".join([f"- [Ï∂úÏ≤ò: {doc.get('source', 'Í∞ÄÏù¥ÎìúÎùºÏù∏')}] {doc.get('text', '')}" for doc in relevant_docs]) if relevant_docs else "Í¥ÄÎ†® ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    
    system_prompt = """
    ÎãπÏã†ÏùÄ ÌôòÍ≤ΩÎ≥¥Í±¥ ÏùòÏÇ¨ÏûÖÎãàÎã§. ÎåÄÍ∏∞Ïßà Îç∞Ïù¥ÌÑ∞(Ïò®ÎèÑ, ÏäµÎèÑ Ìè¨Ìï®)ÏôÄ ÌôòÏûêÏùò Í∏∞Ï†ÄÏßàÌôò Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú ÌåêÎã® Í∑ºÍ±∞(Reason)Î•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.
    
    [Ï§ëÏöî]
    1. 'decision'Í≥º 'actionItems'Îäî Ïù¥ÎØ∏ ÏãúÏä§ÌÖúÏóêÏÑú Í≥ÑÏÇ∞ÎêòÏóàÏäµÎãàÎã§. ÎãπÏã†ÏùÄ Ïù¥ Í≤∞Ï†ïÏù¥ ÎÇ¥Î†§ÏßÑ 'ÏùòÌïôÏ†Å/ÌôòÍ≤ΩÏ†Å Ïù¥Ïú†(reason)'Î•º ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
    2. Î≥¥Ï†ï Î°úÏßÅÏù¥ Ï†ÅÏö©Îêú Í≤ΩÏö∞(Ïòà: ÏäµÎèÑÍ∞Ä ÎÑàÎ¨¥ ÎÜíÍ±∞ÎÇò ÎÇÆÏïÑÏÑú, ÌòπÏùÄ ÌäπÏ†ï ÏßàÌôò Ìä∏Î¶¨Í±∞Î°ú Ïù∏Ìï¥ Îì±Í∏âÏù¥ Í≤©ÏÉÅÎê®) Í∑∏ Ïù¥Ïú†Î•º ÏÑ§Î™ÖÏóê Ìè¨Ìï®ÌïòÏÑ∏Ïöî.
    3. Ï†úÍ≥µÎêú [ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏] ÎÇ¥Ïö©ÏùÑ ÏµúÏö∞ÏÑ†ÏúºÎ°ú Î∞òÏòÅÌïòÏó¨ ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
    4. Î∞òÎìúÏãú JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ïïº Ìï©ÎãàÎã§.
    """
    
    user_prompt = f"""
    [ÏÉÅÌô© Ï†ïÎ≥¥]
    - ÎåÄÍ∏∞Ïßà: Ï¥àÎØ∏ÏÑ∏Î®ºÏßÄ={pm25_raw}(Î≥¥Ï†ïÌõÑ:{pm25_corrected}), Ïò§Ï°¥={o3_raw}(Î≥¥Ï†ïÌõÑ:{o3_corrected})
    - ÌôòÍ≤Ω: Ïò®ÎèÑ={temp}¬∞C, ÏäµÎèÑ={humidity}%
    - ÏÇ¨Ïö©Ïûê: Ïó∞Î†πÎåÄ={age_group}, Í∏∞Ï†ÄÏßàÌôò={user_condition}
    - ÏãúÏä§ÌÖú Í≤∞Ï†ï: {decision_text}
    - ÏãúÏä§ÌÖú ÌñâÎèôÏàòÏπô: {action_items}
    
    [ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏ (Ï∞∏Í≥† Î¨∏Ìóå)]
    {context_text}
    
    ÏúÑ Í≤∞Ï†ïÏù¥ ÎÇ¥Î†§ÏßÑ Î∞∞Í≤ΩÍ≥º Ïù¥Ïú†Î•º Ïò®ÎèÑ, ÏäµÎèÑ, ÏßàÌôò ÌäπÏÑ±ÏùÑ Í≥†Î†§ÌïòÏó¨ ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.
    """
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=1 
        )
        
        content = response.choices[0].message.content
        llm_result = json.loads(content)
        
        # Merge Results
        final_result = {
            "decision": decision_text,
            "reason": llm_result.get("reason", "Ï†ïÎ≥¥Î•º Î∂àÎü¨Ïò§Îäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."),
            "actionItems": action_items,
            "references": list(set([doc.get("source", "Unknown Source") for doc in relevant_docs])),
            # Add real-time air quality values for frontend display
            "pm25_value": air_data.get("pm25_value"),
            "o3_value": air_data.get("o3_value"),
            "pm10_value": air_data.get("pm10_value"),
            "no2_value": air_data.get("no2_value")
        }
        
        # [Step F] Save to Cache
        if db is not None and cache_key:
            try:
                await db[CACHE_COLLECTION].update_one(
                    {"_id": cache_key},
                    {"$set": {"data": final_result, "created_at": datetime.now(KST_TZ)}},
                    upsert=True
                )
                print(f"üíæ Saved to cache: {cache_key}")
            except Exception as e:
                print(f"Error saving to cache: {e}")
                
        return final_result
        
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        # Fallback even if LLM fails, we satisfy the deterministic requirement
        return {
            "decision": decision_text,
            "reason": "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÎ°ú ÏÉÅÏÑ∏ ÏÑ§Î™ÖÏùÑ Î∂àÎü¨Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÌñâÎèô ÏßÄÏπ®ÏùÄ ÏúÑÏôÄ Í∞ôÏù¥ Ï§ÄÏàòÌï¥Ï£ºÏÑ∏Ïöî.",
            "actionItems": action_items,
            "references": [],
            # Add real-time air quality values for frontend display
            "pm25_value": air_data.get("pm25_value"),
            "o3_value": air_data.get("o3_value"),
            "pm10_value": air_data.get("pm10_value"),
            "no2_value": air_data.get("no2_value")
        }

async def ingest_pdf(file_content: bytes, filename: str) -> Dict[str, Any]:
    """
    Ingest PDF content: Extract text -> Embed -> Store in DB.
    """
    import io
    from PyPDF2 import PdfReader

    if not vo_client:
        return {"status": "error", "message": "Voyage AI Client not initialized"}
    
    if db is None:
        return {"status": "error", "message": "Database not initialized"}

    try:
        # 1. Read PDF
        pdf_file = io.BytesIO(file_content)
        reader = PdfReader(pdf_file)
        
        extracted_text = ""
        documents_to_insert = []
        texts_to_embed = []
        
        # 2. Extract Text per Page (Chunking Strategy: 1 Page = 1 Doc for simplicity)
        print(f"üìÑ Processing PDF: {filename} ({len(reader.pages)} pages)")
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 50: # Ignore empty/too short pages
                extracted_text += text + "\n"
                
                texts_to_embed.append(text)
                documents_to_insert.append({
                    "text": text,
                    "category": "pdf_upload",
                    "source": filename,
                    "page": i + 1,
                    "risk_level": "unknown", # Needs manual classification or LLM analysis
                    "created_at": datetime.now()
                })
        
        if not documents_to_insert:
            return {"status": "error", "message": "No extractable text found in PDF."}

        # 3. Embed Data
        print(f"üß† Embedding {len(texts_to_embed)} pages with Voyage AI...")
        result = vo_client.embed(texts_to_embed, model="voyage-3-large", input_type="document")
        embeddings = result.embeddings
        
        for i, doc in enumerate(documents_to_insert):
            doc["embedding"] = embeddings[i]
            
        # 4. Insert into DB
        insert_result = await db[GUIDELINES_COLLECTION].insert_many(documents_to_insert)
        
        return {
            "status": "success",
            "message": f"Successfully ingested {len(insert_result.inserted_ids)} pages from {filename}",
            "inserted_ids": [str(id) for id in insert_result.inserted_ids]
        }
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        return {"status": "error", "message": str(e)}
