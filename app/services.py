import os
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
import voyageai
from openai import OpenAI
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGODB_URI")
VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_NAME = "epilog_db"
GUIDELINES_COLLECTION = "medical_guidelines"
AIR_QUALITY_COLLECTION = "daily_air_quality"

if not MONGO_URI:
    # Fallback to a dummy URI if not set to prevent startup crash, but it will fail on request
    print("WARNING: MONGODB_URI is not set.")
    MONGO_URI = "mongodb://localhost:27017"

# Initialize Clients
try:
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client[DB_NAME]
except Exception as e:
    print(f"Error initializing MongoDB client: {e}")
    mongo_client = None
    db = None

# --- Logic Constants ---
GRADE_MAP = {
    "ì¢‹ìŒ": 0,
    "ë³´í†µ": 1,
    "ë‚˜ì¨": 2,
    "ë§¤ìš°ë‚˜ì¨": 3
}

# Decision Texts
DECISION_TEXTS = {
    "infant": {
        "ok": "ì˜¤ëŠ˜ì€ ë°”ê¹¥ë†€ì´ ê´œì°®ì•„ìš” ðŸ™‚",
        "caution": "ì˜¤ëŠ˜ì€ ì§§ê²Œ ë‹¤ë…€ì™€ìš”!",
        "warning": "ì˜¤ëŠ˜ì€ ì‹¤ë‚´ê°€ ë” íŽ¸í•´ìš” ðŸ "
    },
    "elementary_low": {
        "ok": "ì˜¤ëŠ˜ì€ ë°–ì—ì„œ ë†€ê¸° ì¢‹ì•„ìš”! ë¬¼ì€ ê¼­ ì±™ê¸°ê¸°!",
        "caution": "ì˜¤ëŠ˜ì€ ìž ê¹ë§Œ ë‹¤ë…€ì™€ìš”. ë•€ë‚˜ëŠ” ë†€ì´ëŠ” ì‰¬ê¸°!",
        "warning": "ì˜¤ëŠ˜ì€ ì‹¤ë‚´ ë†€ì´ê°€ ë” ì¢‹ì•„ìš”!"
    },
    "elementary_high": {
        "ok": "ì˜¤ëŠ˜ì€ ì•¼ì™¸í™œë™ ê´œì°®ì•„ìš”. ë¬¼ ìžì£¼ ë§ˆì…”ìš”!",
        "caution": "ì˜¤ëŠ˜ì€ ì•¼ì™¸ í™œë™ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ê°•ë„ëŠ” ë‚®ê²Œ!",
        "warning": "ì˜¤ëŠ˜ì€ ì‹¤ë‚´ í™œë™ì´ ì•ˆì „í•´ìš”."
    },
    "teen": {
        "ok": "ì˜¤ëŠ˜ì€ ì•¼ì™¸ í™œë™ ë¬´ë¦¬ ì—†ì–´ìš”. ìˆ˜ë¶„ ì„­ì·¨ ìžŠì§€ ë§ˆì„¸ìš”.",
        "caution": "ì˜¤ëŠ˜ì€ ì•¼ì™¸ ìš´ë™ ê°•ë„ëŠ” ë‚®ì¶”ê³  ì‹œê°„ì€ ì§§ê²Œ!",
        "warning": "ì˜¤ëŠ˜ì€ ì‹¤ë‚´ í™œë™ì´ ë” ì•ˆì „í•©ë‹ˆë‹¤."
    }
}

# Action Items templates
ACTION_ITEMS = {
    "infant": {
        "ok": [
            "ê°€ê¹Œìš´ ê³µì›ì—ì„œ ê°€ë³ê²Œ ë›°ì–´ë†€ê¸°",
            "ë¬¼ ìžì£¼ ë§ˆì‹œê¸°",
            "ì§‘ì— ì˜¤ë©´ ì†Â·ì–¼êµ´ ì”»ê¸°"
        ],
        "caution": [
            "ì™¸ì¶œì€ 20â€“30ë¶„ ì´ë‚´ë¡œ ì§§ê²Œ",
            "ë›°ëŠ” ë†€ì´ëŠ” ìž ê¹ë§Œ",
            "ì§‘ì—ì„œëŠ” ë¸”ë¡/ì—­í• ë†€ì´ë¡œ ë°”ê¿”ë³´ê¸°"
        ],
        "warning": [
            "ì™¸ì¶œ ëŒ€ì‹  ìž¥ë‚œê° ì •ë¦¬+ì°¾ê¸° ê²Œìž„",
            "ì‹¤ë‚´ì—ì„œ í’ì„ ë°°êµ¬/ìž¥ì• ë¬¼ ì½”ìŠ¤(ê°€ë³ê²Œ)",
            "í™˜ê¸°ëŠ” ì§§ê²Œ(5â€“10ë¶„) í•˜ê³  ë°”ë¡œ ë‹«ê¸°"
        ]
    },
    "elementary_low": {
        "ok": [
            "ê°€ë²¼ìš´ ë‹¬ë¦¬ê¸°/ìžì „ê±°",
            "ë¬¼ ìžì£¼ ë§ˆì‹œê¸°",
            "ê·€ê°€ í›„ ì†ì”»ê¸°/ì„¸ì•ˆ"
        ],
        "caution": [
            "ë•€ ë§Žì´ ë‚˜ëŠ” ë†€ì´ëŠ” ìž ê¹ë§Œ",
            "ì™¸ì¶œì€ 30ë¶„ ì´ë‚´",
            "ì‹¤ë‚´ì—ì„œëŠ” ë§Œë“¤ê¸°/ë³´ë“œê²Œìž„ ì¶”ì²œ"
        ],
        "warning": [
            "ë°– ëŒ€ì‹  ì‹¤ë‚´ ë†€ì´(ë³´ë“œê²Œìž„/ë§Œë“¤ê¸°)",
            "ì°½ë¬¸ í™˜ê¸°ëŠ” ì§§ê²Œ",
            "ê¸°ì¹¨/ìŒ•ìŒ•ì´ë©´ ì‰¬ê¸°"
        ]
    },
    "elementary_high": {
        "ok": [
            "ê°€ë²¼ìš´ ìš´ë™ì´ë‚˜ ì‚°ì±…",
            "ë§ˆìŠ¤í¬/ì†ì”»ê¸°(í•„ìš” ì‹œ)",
            "ê·€ê°€ í›„ ìƒ¤ì›Œ/ì„¸ì•ˆ"
        ],
        "caution": [
            "ì²´ìœ¡/ë›°ê¸° ëŒ€ì‹  ì‚°ì±…Â·ìžì „ê±° ì²œì²œížˆ",
            "ì‹œê°„ì€ ì§§ê²Œ(30â€“60ë¶„)",
            "ì‹¤ë‚´ì—ì„œëŠ” ë…ì„œ/ë³´ë“œê²Œìž„/ë§Œë“¤ê¸°"
        ],
        "warning": [
            "ì•¼ì™¸ í™œë™ ëŒ€ì‹  ì‹¤ë‚´ í™œë™",
            "ì°½ë¬¸ í™˜ê¸°ëŠ” ì§§ê²Œ",
            "í˜¸í¡ê¸° ì¦ìƒ ìžˆìœ¼ë©´ ë¬´ë¦¬í•˜ì§€ ì•Šê¸°"
        ]
    },
    "teen": {
        "ok": [
            "ê°€ë²¼ìš´ ìš´ë™ì´ë‚˜ ì‚°ì±…",
            "ë§ˆìŠ¤í¬/ì†ì”»ê¸°(í•„ìš” ì‹œ)",
            "ê·€ê°€ í›„ ìƒ¤ì›Œ/ì„¸ì•ˆ"
        ],
        "caution": [
            "ê²©í•œ ìš´ë™ì€ í”¼í•˜ê³  ê°•ë„ ë‚®ì¶”ê¸°",
            "ì™¸ì¶œ ì‹œê°„ì€ ì§§ê²Œ(30â€“60ë¶„)",
            "ì‹¤ë‚´ì—ì„œëŠ” ìŠ¤íŠ¸ë ˆì¹­/ê°€ë²¼ìš´ ìš´ë™ ì¶”ì²œ"
        ],
        "warning": [
            "ì•¼ì™¸ í™œë™ ëŒ€ì‹  ì‹¤ë‚´ ìš´ë™",
            "ì°½ë¬¸ í™˜ê¸°ëŠ” ì§§ê²Œ",
            "í˜¸í¡ê¸° ì¦ìƒ ìžˆìœ¼ë©´ ë¬´ë¦¬í•˜ì§€ ì•Šê¸°"
        ]
    }
}

def _calculate_decision(pm25_grade: str, o3_grade: str) -> str:
    """
    Calculate decision level: 'ok', 'caution', 'warning'
    
    Logic:
    â€¢ OK: PM2.5 <= ë³´í†µ AND O3 <= ë³´í†µ
    â€¢ Caution: One of them is ë‚˜ì¨
    â€¢ Warning: One of them is ë§¤ìš°ë‚˜ì¨ OR Both are ë‚˜ì¨
    """
    p_score = GRADE_MAP.get(pm25_grade, 0)
    o_score = GRADE_MAP.get(o3_grade, 0)
    
    # Check Warning Conditions
    # 1. Any 'ë§¤ìš°ë‚˜ì¨' (score 3)
    if p_score >= 3 or o_score >= 3:
        return "warning"
    # 2. Both 'ë‚˜ì¨' (score 2)
    if p_score == 2 and o_score == 2:
        return "warning"
        
    # Check Caution Conditions
    # One is 'ë‚˜ì¨' (score 2) - note: the case where both are bad is handled above
    if p_score == 2 or o_score == 2:
        return "caution"
        
    # Default OK
    return "ok"

def _normalize_age_group(age_group: Any) -> str:
    if age_group is None:
        return "elementary_high"
    raw = str(age_group).strip().lower()
    if raw in {
        "infant",
        "ìœ ì•„",
        "ì˜ìœ ì•„",
        "0-6",
        "0~6",
        "0-5",
        "0~5",
        "0-3",
        "0~3"
    }:
        return "infant"
    if raw in {
        "elementary_low",
        "ì´ˆë“± ì €í•™ë…„",
        "ì´ˆë“±ì €í•™ë…„",
        "1-3",
        "1~3",
        "7-9",
        "7~9"
    }:
        return "elementary_low"
    if raw in {
        "elementary_high",
        "ì´ˆë“± ê³ í•™ë…„",
        "ì´ˆë“±ê³ í•™ë…„",
        "4-6",
        "4~6",
        "10-12",
        "10~12"
    }:
        return "elementary_high"
    if raw in {
        "teen",
        "ì²­ì†Œë…„",
        "ì¤‘ë“±",
        "ê³ ë“±",
        "ì¤‘í•™ìƒ",
        "ê³ ë“±í•™ìƒ",
        "13-15",
        "13~15",
        "16-18",
        "16~18"
    }:
        return "teen"
    if raw in {"child", "children", "ì´ˆë“±", "ì•„ë™"}:
        return "elementary_high"
    if raw in {"adult", "ì„±ì¸"}:
        return "teen"
    if "ìœ ì•„" in raw:
        return "infant"
    if "ì´ˆë“±" in raw or "ì•„ë™" in raw:
        return "elementary_high"
    if "ì €í•™ë…„" in raw:
        return "elementary_low"
    if "ê³ í•™ë…„" in raw:
        return "elementary_high"
    if "ì¤‘ë“±" in raw or "ê³ ë“±" in raw or "ì²­ì†Œë…„" in raw:
        return "teen"
    return "elementary_high"

def _get_display_content(age_group: str, decision_key: str):
    """
    Returns (decision_text, action_items)
    """
    # Normalize age group to key
    group_key = _normalize_age_group(age_group)
    
    # Get Text
    d_text = DECISION_TEXTS.get(group_key, DECISION_TEXTS["elementary_high"]).get(decision_key, "ìƒíƒœ í™•ì¸ í•„ìš”")
    
    # Get Actions
    actions = ACTION_ITEMS.get(group_key, ACTION_ITEMS["elementary_high"]).get(decision_key, [])
    
    return d_text, actions

try:
    vo_client = voyageai.Client(api_key=VOYAGE_API_KEY)
except Exception as e:
    print(f"Error initializing Voyage AI client: {e}")
    vo_client = None

try:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    openai_client = None

async def get_air_quality(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch air quality data for the given station and today's date.
    """
    if db is None:
        raise Exception("Database connection not initialized")
        
    today_str = datetime.now().strftime("%Y-%m-%d")
    
    # Try to find today's data for the station
    # Note: In a real scenario, you might need to query an external API if DB doesn't have it.
    # For this task, we assume it's in the DB or we simulate it if not found (for dev purposes).
    
    try:
        result = await db[AIR_QUALITY_COLLECTION].find_one({
            "stationName": station_name,
            "date": today_str
        })
        
        if result:
            return result
            
        # Mock data if not found (for demonstration purposes as requested structure implies data exists)
        # In production, this should return None or raise specific error
        print(f"No air quality data found for {station_name} on {today_str}. Using mock data.")
        return {
            "stationName": station_name,
            "date": today_str,
            "pm10_grade": "ë‚˜ì¨",
            "pm25_grade": "ë‚˜ì¨",
            "co_grade": "ë³´í†µ",
            "o3_grade": "ë³´í†µ",
            "no2_grade": "ì¢‹ìŒ",
            "so2_grade": "ì¢‹ìŒ",
            "integrated_grade": "ë‚˜ì¨"
        }
        
    except Exception as e:
        print(f"Error fetching air quality: {e}")
        raise e

CACHE_COLLECTION = "rag_cache"

def _generate_cache_key(air_data: Dict[str, Any], user_profile: Dict[str, Any]) -> str:
    grade_map = {"ì¢‹ìŒ": 1, "ë³´í†µ": 2, "ë‚˜ì¨": 3, "ë§¤ìš°ë‚˜ì¨": 4}
    
    pm25 = grade_map.get(air_data.get("pm25_grade", ""), 0)
    pm10 = grade_map.get(air_data.get("pm10_grade", ""), 0)
    o3 = grade_map.get(air_data.get("o3_grade", ""), 0) # Added o3 as per user example
    
    age_group = _normalize_age_group(user_profile.get("ageGroup"))
    condition = user_profile.get("condition", "unknown")
    
    # Key format: pm25:3_pm10:2_o3:1_age:adult_cond:asthma
    return f"pm25:{pm25}_pm10:{pm10}_o3:{o3}_age:{age_group}_cond:{condition}"

async def get_medical_advice(station_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main orchestration function:
    1. Get Air Quality
    2. Check Cache
    3. Construct Query
    4. Vector Search
    5. Generate Advice with LLM
    6. Save to Cache & Return
    """
    # Step A: Get Air Quality
    air_data = await get_air_quality(station_name)
    if not air_data:
        raise ValueError(f"No air quality data found for station: {station_name}")

    cache_key = ""
    # [Step A.1] Check Cache
    if db is not None:
        try:
            cache_key = _generate_cache_key(air_data, user_profile)
            cached_entry = await db[CACHE_COLLECTION].find_one({"_id": cache_key})
            
            if cached_entry:
                print(f"âœ… Cache Hit! Key: {cache_key}")
                return cached_entry["data"]
        except Exception as e:
            print(f"âš ï¸ Cache check failed: {e}")

    # Determine main issue (simplified logic)
    main_condition = "ë³´í†µ"
    if air_data.get("pm25_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ì´ˆë¯¸ì„¸ë¨¼ì§€ {air_data['pm25_grade']}"
    elif air_data.get("pm10_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ë¯¸ì„¸ë¨¼ì§€ {air_data['pm10_grade']}"
    elif air_data.get("so2_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"í™©ì‚¬/ì´ì‚°í™”í™© {air_data['so2_grade']}" # Simplified
        
    # Step B: Query Construction
    user_condition = user_profile.get("condition", "ê±´ê°•í•¨")
    age_group = _normalize_age_group(user_profile.get("ageGroup"))
    
    # Primary Query: Specific
    search_query = f"{main_condition} ìƒí™©ì—ì„œ {user_condition} {age_group} í–‰ë™ ìš”ë ¹ ì£¼ì˜ì‚¬í•­"
    print(f"Generated Search Query (Primary): {search_query}")

    # Step C: Vector Search
    relevant_docs = []
    if vo_client and db is not None:
        try:
            # 1. Primary Search
            embed_result = vo_client.embed([search_query], model="voyage-3-large", input_type="query")
            query_vector = embed_result.embeddings[0]
            
            pipeline = [
                {
                    "$vectorSearch": {
                        "index": "default",
                        "path": "embedding",
                        "queryVector": query_vector,
                        "numCandidates": 100,
                        "limit": 3
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "text": 1,
                        "category": 1,
                        "risk_level": 1,
                        "source": 1,
                        "score": {"$meta": "vectorSearchScore"}
                    }
                }
            ]
            
            cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
            relevant_docs = await cursor.to_list(length=3)
            
            # 2. Fallback Search (If no docs found)
            if not relevant_docs:
                print("âš ï¸ Primary search returned no results. Attempting fallback (General) search.")
                fallback_query = f"{main_condition} í–‰ë™ ìš”ë ¹"
                embed_result_fb = vo_client.embed([fallback_query], model="voyage-3-large", input_type="query")
                query_vector_fb = embed_result_fb.embeddings[0]
                
                pipeline[0]["$vectorSearch"]["queryVector"] = query_vector_fb
                
                cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
                relevant_docs = await cursor.to_list(length=3)
                
        except Exception as e:
            print(f"Error during vector search: {e}")
            pass

    # Step D: LLM Generation
    if not openai_client:
         return {
            "decision": "Error",
            "reason": "OpenAI Client not initialized",
            "actionItems": [],
            "references": []
        }

    # [Logic Update] Calculate Deterministic Decision & Action Items
    pm25_g = air_data.get("pm25_grade", "ë³´í†µ")
    o3_g = air_data.get("o3_grade", "ë³´í†µ")
    
    decision_key = _calculate_decision(pm25_g, o3_g)
    decision_text, action_items = _get_display_content(age_group, decision_key)
    
    # Logic for dual bad condition text append
    # "ë‘˜ ë‹¤ ë†’ì€ ê²½ìš°: ë” ë‚˜ìœ ìª½ì„ ë”°ë¼ê°€ë˜, ë¬¸êµ¬ëŠ” 'ë‘˜ ë‹¤ ë†’ì•„ìš”'ë¡œ 1ì¤„ ì¶”ê°€"
    # -> If reasoning needs this, we can add it to prompt context or just append to decision text if needed.
    # The requirement says "ë¬¸êµ¬ëŠ” 'ë‘˜ ë‹¤ ë†’ì•„ìš”'ë¡œ 1ì¤„ ì¶”ê°€". 
    # Let's append it to decision text if both are >= 'ë‚˜ì¨'.
    p_score = GRADE_MAP.get(pm25_g, 0)
    o_score = GRADE_MAP.get(o3_g, 0)
    if p_score >= 2 and o_score >= 2:
        decision_text += " (ë¯¸ì„¸ë¨¼ì§€ì™€ ì˜¤ì¡´ ë‘˜ ë‹¤ ë†’ì•„ìš”!)"

    # Prepare Context
    context_text = "\n".join([f"- [ì¶œì²˜: {doc.get('source', 'ê°€ì´ë“œë¼ì¸')}] {doc.get('text', '')}" for doc in relevant_docs]) if relevant_docs else "ê´€ë ¨ ì˜í•™ì  ê°€ì´ë“œë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    system_prompt = """
    ë‹¹ì‹ ì€ í™˜ê²½ë³´ê±´ ì˜ì‚¬ìž…ë‹ˆë‹¤. ëŒ€ê¸°ì§ˆ ë°ì´í„°ì™€ í™˜ìžì˜ ê¸°ì €ì§ˆí™˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨ ê·¼ê±°(Reason)ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ì¤‘ìš”]
    1. 'decision'ê³¼ 'actionItems'ëŠ” ì´ë¯¸ ì‹œìŠ¤í…œì—ì„œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì€ ì´ ê²°ì •ì´ ë‚´ë ¤ì§„ 'ì˜í•™ì /í™˜ê²½ì  ì´ìœ (reason)'ë§Œ ìž‘ì„±í•˜ë©´ ë©ë‹ˆë‹¤.
    2. ì œê³µëœ [ì˜í•™ì  ê°€ì´ë“œë¼ì¸] ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
    3. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    
    ì‘ë‹µ í¬ë§·:
    {
        "reason": "íŒë‹¨ ê·¼ê±° (ê°€ì´ë“œë¼ì¸ ë‚´ìš© ì¸ìš© í¬í•¨)"
    }
    """
    
    user_prompt = f"""
    [ìƒí™© ì •ë³´]
    - ëŒ€ê¸°ì§ˆ: PM2.5={pm25_g}, O3={o3_g}
    - ì‚¬ìš©ìž: {age_group}, {user_condition}
    - ì‹œìŠ¤í…œ ê²°ì •: {decision_text}
    - ì‹œìŠ¤í…œ í–‰ë™ìˆ˜ì¹™: {action_items}
    
    [ì˜í•™ì  ê°€ì´ë“œë¼ì¸ (ì°¸ê³  ë¬¸í—Œ)]
    {context_text}
    
    ìœ„ ê²°ì •ì´ ë‚´ë ¤ì§„ ë°°ê²½ê³¼ ì´ìœ ë¥¼ ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=1 
        )
        
        content = response.choices[0].message.content
        llm_result = json.loads(content)
        
        # Merge Results
        final_result = {
            "decision": decision_text,
            "reason": llm_result.get("reason", "ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."),
            "actionItems": action_items,
            "references": list(set([doc.get("source", "Unknown Source") for doc in relevant_docs]))
        }
        
        # [Step F] Save to Cache
        if db is not None and cache_key:
            try:
                await db[CACHE_COLLECTION].update_one(
                    {"_id": cache_key},
                    {"$set": {"data": final_result, "created_at": datetime.now()}},
                    upsert=True
                )
                print(f"ðŸ’¾ Saved to cache: {cache_key}")
            except Exception as e:
                print(f"Error saving to cache: {e}")
                
        return final_result
        
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        # Fallback even if LLM fails, we satisfy the deterministic requirement
        return {
            "decision": decision_text,
            "reason": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ë¡œ ìƒì„¸ ì„¤ëª…ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í–‰ë™ ì§€ì¹¨ì€ ìœ„ì™€ ê°™ì´ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”.",
            "actionItems": action_items,
            "references": []
        }

async def ingest_pdf(file_content: bytes, filename: str) -> Dict[str, Any]:
    """
    Ingest PDF content: Extract text -> Embed -> Store in DB.
    """
    import io
    from PyPDF2 import PdfReader

    if not vo_client:
        return {"status": "error", "message": "Voyage AI Client not initialized"}
    
    if db is None:
        return {"status": "error", "message": "Database not initialized"}

    try:
        # 1. Read PDF
        pdf_file = io.BytesIO(file_content)
        reader = PdfReader(pdf_file)
        
        extracted_text = ""
        documents_to_insert = []
        texts_to_embed = []
        
        # 2. Extract Text per Page (Chunking Strategy: 1 Page = 1 Doc for simplicity)
        print(f"ðŸ“„ Processing PDF: {filename} ({len(reader.pages)} pages)")
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 50: # Ignore empty/too short pages
                extracted_text += text + "\n"
                
                texts_to_embed.append(text)
                documents_to_insert.append({
                    "text": text,
                    "category": "pdf_upload",
                    "source": filename,
                    "page": i + 1,
                    "risk_level": "unknown", # Needs manual classification or LLM analysis
                    "created_at": datetime.now()
                })
        
        if not documents_to_insert:
            return {"status": "error", "message": "No extractable text found in PDF."}

        # 3. Embed Data
        print(f"ðŸ§  Embedding {len(texts_to_embed)} pages with Voyage AI...")
        result = vo_client.embed(texts_to_embed, model="voyage-3-large", input_type="document")
        embeddings = result.embeddings
        
        for i, doc in enumerate(documents_to_insert):
            doc["embedding"] = embeddings[i]
            
        # 4. Insert into DB
        insert_result = await db[GUIDELINES_COLLECTION].insert_many(documents_to_insert)
        
        return {
            "status": "success",
            "message": f"Successfully ingested {len(insert_result.inserted_ids)} pages from {filename}",
            "inserted_ids": [str(id) for id in insert_result.inserted_ids]
        }
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        return {"status": "error", "message": str(e)}
