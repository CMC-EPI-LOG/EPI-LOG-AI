import os
import json
from datetime import datetime
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional, Any
import voyageai
from openai import OpenAI
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGODB_URI")
VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_NAME = "epilog_db"
GUIDELINES_COLLECTION = "medical_guidelines"
AIR_QUALITY_COLLECTION = "daily_air_quality"
VECTOR_INDEX = "vector_index"
KST_TZ = ZoneInfo("Asia/Seoul")

if not MONGO_URI:
    # Fallback to a dummy URI if not set to prevent startup crash, but it will fail on request
    print("WARNING: MONGODB_URI is not set.")
    MONGO_URI = "mongodb://localhost:27017"

# Initialize Clients
try:
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client[DB_NAME]
except Exception as e:
    print(f"Error initializing MongoDB client: {e}")
    mongo_client = None
    db = None

# --- Logic Constants ---
GRADE_MAP = {
    "ì¢‹ìŒ": 1,
    "ë³´í†µ": 2,
    "ë‚˜ì¨": 3,
    "ë§¤ìš°ë‚˜ì¨": 4
}

REVERSE_GRADE_MAP = {v: k for k, v in GRADE_MAP.items()}

# Correction Weights
HUMIDITY_WEIGHTS = {
    "high": 1.2,  # > 70%
    "low": 1.1,   # < 30%
    "normal": 1.0
}

def _get_corrected_grade(
    base_grade: str, 
    temp: Optional[float], 
    humidity: Optional[float], 
    condition: str,
    pollutant_type: str # "pm25" or "o3"
) -> str:
    """
    Apply correction logic based on temperature, humidity and disease condition.
    Returns the corrected grade string.
    """
    score = GRADE_MAP.get(base_grade, 2)
    
    # 1. Humidity Correction (W_h)
    w_h = 1.0
    if humidity is not None:
        if humidity > 70:
            w_h = HUMIDITY_WEIGHTS["high"]
        elif humidity < 30:
            w_h = HUMIDITY_WEIGHTS["low"]
    
    # 2. Temperature & Disease Trigger Logic
    # Asthma + Cold + PM2.5
    if condition == "asthma" and temp is not None and temp < 5 and pollutant_type == "pm25":
        if base_grade == "ë³´í†µ": return "ë‚˜ì¨"
        
    # Rhinitis + Dry + PM2.5
    if condition == "rhinitis" and humidity is not None and humidity < 30 and pollutant_type == "pm25":
        if base_grade == "ë³´í†µ": return "ë‚˜ì¨"
        
    # Atopy + Heat + O3
    if condition == "atopy" and temp is not None and temp > 30 and pollutant_type == "o3":
        if base_grade == "ë³´í†µ": return "ë‚˜ì¨"
        
    # General + High Humidity + PM2.5 Bad
    if humidity is not None and humidity > 80 and pollutant_type == "pm25" and base_grade == "ë‚˜ì¨":
        return "ë§¤ìš°ë‚˜ì¨"

    # Apply multiplicative weight if no specific trigger fired
    # (Simplified: if score * w_h rounds up to next grade)
    final_score = min(4, max(1, round(score * w_h)))
    return REVERSE_GRADE_MAP.get(final_score, base_grade)

# Decision Texts based on 80-segment dataset
DECISION_TEXTS = {
    "infant": {
        "general": {
            "ok": "ìœ ëª¨ì°¨ ì‚°ì±… ê°€ìš”!",
            "caution": "ì§§ì€ ì‚°ì±…ë§Œ ì¶”ì²œí•´ìš”",
            "warning": "ì‹¤ë‚´ê°€ ë” ì•ˆì „í•´ìš”"
        },
        "rhinitis": {
            "ok": "ì½”ê°€ íŽ¸ì•ˆí•œ ë‚ ì´ì—ìš”",
            "caution": "ì½”ì ë§‰ ë³´ìŠµì— ì§‘ì¤‘",
            "warning": "ì½§ë¬¼ ìœ ë°œ ì£¼ì˜ë³´"
        },
        "asthma": {
            "ok": "ìƒì¾Œí•˜ê²Œ ìˆ¨ ì‰¬ì–´ìš”",
            "caution": "ì°¬ ë°”ëžŒ ë…¸ì¶œ ì£¼ì˜",
            "warning": "ìŒ•ìŒ•ê±°ë¦¼ ëª¨ë‹ˆí„°ë§"
        },
        "atopy": {
            "ok": "í”¼ë¶€ ê°€ë ¤ì›€ ê±±ì • ëš",
            "caution": "ë•€ë‚˜ë©´ ë°”ë¡œ ë‹¦ì•„ì£¼ì„¸ìš”",
            "warning": "ì™¸ë¶€ ë¨¼ì§€ ì ‘ì´‰ ì°¨ë‹¨"
        }
    },
    "toddler": {
        "general": {
            "ok": "ë†€ì´í„°ì—ì„œ ë›°ë†€ì•„ìš”",
            "caution": "ë¬¼ í•œ ì»µ ë§ˆì‹œê³  ë‚˜ê°€ê¸°",
            "warning": "ì‹¤ì™¸ ë†€ì´ëŠ” ì§§ê²Œ"
        },
        "rhinitis": {
            "ok": "ì½” ë©´ì—­ë ¥ í‚¤ìš°ëŠ” ë‚ ",
            "caution": "ìž¬ì±„ê¸° ìœ ë„ ë¨¼ì§€ ì¡°ì‹¬",
            "warning": "ìž… ëŒ€ì‹  ì½”ë¡œ ìˆ¨ ì‰¬ê¸°"
        },
        "asthma": {
            "ok": "ê¸°ë„ê°€ ì—´ë¦¬ëŠ” ë‚ ì”¨",
            "caution": "ê°‘ìž‘ìŠ¤ëŸ° ê¸°ì¹¨ ì£¼ì˜",
            "warning": "ê²©ë ¬í•œ ìš´ë™ ê¸ˆì§€"
        },
        "atopy": {
            "ok": "í”¼ë¶€ê°€ ìˆ¨ ì‰¬ëŠ” ë‚ ",
            "caution": "ë•€ê³¼ ë¨¼ì§€ë¥¼ ë©€ë¦¬í•´ìš”",
            "warning": "ë•€ ë‹¦ê³  ë°”ë¡œ ë³´ìŠµ"
        }
    },
    "elementary_low": {
        "general": {
            "ok": "ìš´ë™ìž¥ì—ì„œ ë§ˆìŒê»!",
            "caution": "ì²´ìœ¡ ì „ ìƒíƒœ í™•ì¸",
            "warning": "ì‹¤ì™¸ ì²´ìœ¡ì€ ì‰¬ì–´ê°€ìš”"
        },
        "rhinitis": {
            "ok": "ìˆ² ì²´í—˜ ê°€ê¸° ì¢‹ì€ ë‚ ",
            "caution": "ë§ˆìŠ¤í¬ ì“°ê³  ë“±êµí•˜ê¸°",
            "warning": "ìž¬ì±„ê¸°/ì•ˆêµ¬ ì¦ìƒ ì£¼ì˜"
        },
        "asthma": {
            "ok": "ì»¨ë””ì…˜ ìµœìƒì¸ ë‚ ",
            "caution": "ìš´ë™ ê°•ë„ë¥¼ ì¡°ì ˆí•´ìš”",
            "warning": "ìš´ë™ ê°•ë„ ì¡°ì ˆ í•„ìˆ˜"
        },
        "atopy": {
            "ok": "ìžì™¸ì„  ì°¨ë‹¨ì œ í•„ìˆ˜",
            "caution": "ê¸´ì†Œë§¤ë¡œ í”¼ë¶€ ë³´í˜¸",
            "warning": "ê¸ì§€ ì•Šê²Œ ì‹œì›í•˜ê²Œ"
        }
    },
    "elementary_high": {
        "general": {
            "ok": "ì•¼ì™¸í™œë™ ê´œì°®ì•„ìš”",
            "caution": "ë“±í•˜êµ ì‹œ ìƒíƒœ í™•ì¸",
            "warning": "KF80 ë§ˆìŠ¤í¬ í•„ìˆ˜ (ê³ ë†ë„ëŠ” í ì„±ìž¥ì— ì˜í–¥ì„ ì¤˜ìš”)"
        },
        "rhinitis": {
            "ok": "ë§‘ì€ ê³µê¸°ë¡œ ì½” ì •í™”",
            "caution": "ë§ˆìŠ¤í¬ íœ´ëŒ€ ì¶”ì²œ",
            "warning": "ìž… ëŒ€ì‹  ì½”ë¡œ ìˆ¨ ì‰¬ê¸°"
        },
        "asthma": {
            "ok": "ì•¼ì™¸í™œë™ ë¬´ë¦¬ ì—†ì–´ìš”",
            "caution": "ìš´ë™ ê°•ë„ 50% í•˜í–¥",
            "warning": "ê¸°ë„ ì—¼ì¦ ì˜ˆë°© ì£¼ì˜ (ì‹¤ì™¸ ì´ë™ ì „ë©´ ì œí•œ)"
        },
        "atopy": {
            "ok": "í”¼ë¶€ ìž¥ë²½ ì•ˆì‹¬ ë‚ ",
            "caution": "ë•€ê³¼ ë¨¼ì§€ ì ‘ì´‰ ì£¼ì˜",
            "warning": "ì¦‰ê°ì ì¸ í”¼ë¶€ ì„¸ì • í•„ìš”"
        }
    },
    "teen_adult": {
        "general": {
            "ok": "ì•¼ì™¸ í™œë™ ë¬´ë¦¬ ì—†ì–´ìš”",
            "caution": "ìš´ë™ ê°•ë„ëŠ” ë‚®ì¶”ê³  ì‹œê°„ì€ ì§§ê²Œ",
            "warning": "ì‹¤ë‚´ í™œë™ì´ ë” ì•ˆì „í•©ë‹ˆë‹¤"
        },
        "rhinitis": {
            "ok": "ì½”ê°€ íŽ¸ì•ˆí•œ ë‚ ìž…ë‹ˆë‹¤",
            "caution": "ìƒë¦¬ì‹ì—¼ìˆ˜ ì½” ì„¸ì²™ ê¶Œìž¥",
            "warning": "ì™¸ì¶œ í›„ ì½§ì† ë¯¸ì„¸ë¨¼ì§€ ì„¸ì • (ê³µê¸°ì²­ì •ê¸° ê°€ë™)"
        },
        "asthma": {
            "ok": "ìƒì¾Œí•œ í˜¸í¡ ê°€ëŠ¥",
            "caution": "ì•¼ì™¸ ëŸ¬ë‹ ê°•ë„ ì¡°ì ˆ",
            "warning": "ì•¼ì™¸ í™œë™ ì „ë©´ ê¸ˆì§€"
        },
        "atopy": {
            "ok": "í”¼ë¶€ ê°€ë ¤ì›€ ì•ˆì‹¬",
            "caution": "ìžê·¹ ì„±ë¶„ ì ‘ì´‰ ì£¼ì˜",
            "warning": "ë¨¼ì§€ ì ‘ì´‰ í”¼í•˜ê¸° (ì¦‰ê°ì ì¸ ì„¸ì •ê³¼ ë³´ìŠµ)"
        }
    }
}

# Action Items templates
ACTION_ITEMS = {
    "infant": {
        "general": {
            "ok": ["ìœ ëª¨ì°¨ ì‚°ì±…", "15ë¶„ í™˜ê¸°", "ë³µê·€ í›„ ì†ë°œ ì”»ê¸°"],
            "caution": ["ìœ ëª¨ì°¨ ì»¤ë²„ ì‚¬ìš©", "ê·¸ëŠ˜ ì‚°ì±…", "ë³µê·€ í›„ ë³´ìŠµ"],
            "warning": ["ì°½ë¬¸ ë‹«ê¸°/ë°€í", "ê³µê¸°ì²­ì •ê¸° ê°€ë™", "ìŠµë„ 50% ìœ ì§€/ë¬¼ê±¸ë ˆ ì²­ì†Œ"]
        },
        "rhinitis": {
            "ok": ["ì¾Œì í•œ í™˜ê¸°", "ë¨¼ì§€ í„¸ê¸°", "ê°€ë²¼ìš´ ì™¸ì¶œ"],
            "caution": ["ê°€ìŠµê¸° ê°€ë™", "ë¯¸ì§€ê·¼í•œ ë¬¼ ë§ˆì‹œê¸°", "ì™¸ì¶œ ê°€ë¦¼ë§‰"],
            "warning": ["ì‹¤ì™¸ í™œë™ ìžì œ/ì‹¤ë‚´ ëŒ€ê¸°", "ì‹ì—¼ìˆ˜ ì½” ì„¸ì •", "ìƒë¹„ì•½ í™•ì¸"]
        },
        "asthma": {
            "ok": ["ì‹ ì„ í•œ ê³µê¸° ìœ ì§€", "ë³´í˜¸ìž ì‚°ì±…", "ì¶©ë¶„í•œ íœ´ì‹"],
            "caution": ["ëª© ê°€ì‹¸ê°œ ì‚¬ìš©", "ì˜¨ë„ ë³€í™” ì£¼ì˜", "ìƒíƒœ ê´€ì°°"],
            "warning": ["ê²©ë ¬í•œ ë†€ì´ ê¸ˆì§€", "ìŠµë„ 55% ìœ ì§€", "ë¹„ìƒì•½ í™•ì¸/ëŒ€ì‘ ì¤€ë¹„"]
        },
        "atopy": {
            "ok": ["ì™¸ì¶œ ì „ ì„ í¬ë¦¼", "í™œë™ í›„ ì„¸ì•ˆ", "ë©´ ì†Œìž¬ ì˜·"],
            "caution": ["íœ´ëŒ€ ì†ìˆ˜ê±´ ì§€ì°¸", "ë³´ìŠµì œ ë°”ë¥´ê¸°", "ì–‡ì€ ê¸´ì†Œë§¤"],
            "warning": ["ì™¸ì¶œ ìµœì†Œí™”/ì‹¤ë‚´ ì²´ë¥˜", "ê·€ê°€ ì¦‰ì‹œ ìƒ¤ì›Œ", "ê³ ë³´ìŠµ í¬ë¦¼/ì‹œì›í•œ ì˜¨ë„ ìœ ì§€"]
        }
    },
    "toddler": {
        "general": {
            "ok": ["ì•¼ì™¸ ë†€ì´ ê¶Œìž¥", "ì „ë©´ í™˜ê¸°", "í™œë™ í›„ ìˆ˜ë¶„ ì„­ì·¨"],
            "caution": ["ë¬¼ ìžì£¼ ë§ˆì‹œê¸°", "ë§ˆìŠ¤í¬ íœ´ëŒ€", "ìž¥ì‹œê°„ ì²´ë¥˜ ìžì œ"],
            "warning": ["ì†Œí˜• ë§ˆìŠ¤í¬ ë°€ì°©", "ì‹¤ë‚´ ë†€ì´ ìœ„ì£¼", "ì•¼ì™¸ í™œë™ ê¸ˆì§€/ê³µì²­ê¸° ì‚¬ìš©"]
        },
        "rhinitis": {
            "ok": ["ìˆ² ì²´í—˜ ì¶”ì²œ", "í™˜ê¸° í›„ ì²­ì†Œ", "ì™¸ì¶œ í›„ ì„¸ì•ˆ"],
            "caution": ["ë§ˆìŠ¤í¬ í•„ìˆ˜", "ì½” ì£¼ë³€ ë³´ìŠµ", "ì‹ì—¼ìˆ˜ ì„¸ì²™"],
            "warning": ["ì™¸ì¶œ í›„ ì½” ì„¸ì²™", "ì‹¤ë‚´ ë¨¼ì§€ ì œê±°", "ì ˆëŒ€ ì‹¤ë‚´ ëŒ€ê¸°/ì¦ìƒ ì‹œ ì•½ ë³µìš©"]
        },
        "asthma": {
            "ok": ["ìœ ì‚°ì†Œ ë†€ì´", "ê·œì¹™ì  ì•½ ë³µìš©", "ê¸°ê´€ í™˜ê¸°"],
            "caution": ["ìš´ë™ ê°•ë„ ë‚®ì¶”ê¸°", "ì¤‘ê°„ íœ´ì‹", "í˜¸í¡ ìƒíƒœ í™•ì¸"],
            "warning": ["ì €ê°•ë„ ë†€ì´ ì „í™˜", "í¡ìž…ê¸° ì§€ì°¸", "ì™¸ì¶œ ê¸ˆì§€/ë¹„ìƒ ì‹œ ë³‘ì› ë°©ë¬¸"]
        },
        "atopy": {
            "ok": ["ì„ í¬ë¦¼ ë„í¬", "ì‚°ì±… í›„ ê°€ë²¼ìš´ ìƒ¤ì›Œ", "ë©´ ì†Œìž¬ ì˜·"],
            "caution": ["ìˆ˜ì‹œë¡œ ë•€ ë‹¦ê¸°", "ì™¸ì¶œ í›„ ë³´ìŠµ", "ê¸´ì†Œë§¤ ê²‰ì˜·"],
            "warning": ["íœ´ëŒ€ìš© ë³´ìŠµì œ", "ì™¸ì¶œ í›„ ì¦‰ì‹œ ìƒ¤ì›Œ", "ëƒ‰ì°œì§ˆ/ìžê·¹ ì—†ëŠ” ë¡œì…˜"]
        }
    },
    "elementary_low": {
        "general": {
            "ok": ["ìš´ë™ìž¥ì—ì„œ ë§ˆìŒê»", "êµì‹¤ ì „ë©´ í™˜ê¸°", "ì•¼ì™¸ í•™ìŠµ"],
            "caution": ["ì¤‘ê°„ ìˆ˜ë¶„ ì„­ì·¨", "í™œë™ í›„ ì–‘ì¹˜", "ëŒ€ê¸°ì§ˆ ì²´í¬"],
            "warning": ["KF80 ë§ˆìŠ¤í¬ í•„ìˆ˜", "ì‹¤ë‚´ ì²´ìœ¡ ëŒ€ì²´", "ë°©ê³¼ í›„ ì¦‰ì‹œ ê·€ê°€"]
        },
        "rhinitis": {
            "ok": ["ì•¼ì™¸ ì‚°ì±…", "í™˜ê¸° í›„ ëŒ€ì²­ì†Œ", "ì¶©ë¶„í•œ íœ´ì‹"],
            "caution": ["ë§ˆìŠ¤í¬ íœ´ëŒ€", "ì† ì”»ê¸° êµìœ¡", "ë¬¼ ìžì£¼ ë§ˆì‹œê¸°"],
            "warning": ["ì•ˆêµ¬ ì„¸ì •", "ë§ˆìŠ¤í¬ í•„ì°©", "ì™¸ì¶œ í›„ ë¨¸ë¦¬ì¹´ë½ í„¸ê¸°"]
        },
        "asthma": {
            "ok": ["í•™êµ ì²´ìœ¡ ì°¸ì—¬", "ê¹Šì€ í˜¸í¡ ìš´ë™", "ì»¨ë””ì…˜ ìœ ì§€"],
            "caution": ["ë¬´ë¦¬í•œ ë‹¬ë¦¬ê¸° ìžì œ", "ì¤‘ê°„ íœ´ì‹", "í˜¸í¡ ëª¨ë‹ˆí„°ë§"],
            "warning": ["ìƒë¹„ì•½ íœ´ëŒ€ í™•ì¸", "êµì‚¬ì—ê²Œ ë¯¸ë¦¬ ì•Œë¦¬ê¸°", "ë…¸ì¶œ ì „ë©´ ì°¨ë‹¨"]
        },
        "atopy": {
            "ok": ["ì„ í¬ë¦¼ ë°”ë¥´ê¸°", "ì•¼ì™¸ í™œë™ ì¦ê¸°ê¸°", "í™œë™ í›„ ì„¸ì•ˆ"],
            "caution": ["ì†ìˆ˜ê±´ ì§€ì°¸", "ì™¸ì¶œ í›„ ë³´ìŠµì œ", "ë©´ ì†ì˜· ìž…ížˆê¸°"],
            "warning": ["ë¯¸ìŠ¤íŠ¸ ì‚¬ìš©", "í†µí’ë˜ëŠ” ì˜·", "ëƒ‰ì°œì§ˆ ì§„ì •/ì‹¤ë‚´ ìŠµë„ ì¡°ì ˆ"]
        }
    },
    "elementary_high": {
        "general": {
            "ok": ["ë¹„íƒ€ë¯¼ C ì„­ì·¨", "ì•¼ì™¸ í™œë™ ê¶Œìž¥", "ê·€ê°€ í›„ ì„¸ì•ˆ"],
            "caution": ["í•™ì› ì´ë™ ì‹œ ì²œì²œížˆ", "ë§ˆìŠ¤í¬ ì°©ìš©", "ìˆ˜ë¶„ ì„­ì·¨"],
            "warning": ["KF80 ë§ˆìŠ¤í¬ í•„ìˆ˜", "ê²©ë ¬í•œ ìš´ë™ ìžì œ", "ì‹¤ì™¸ ë…¸ì¶œ ìµœì†Œí™”"]
        },
        "asthma": {
            "ok": ["ì •ìƒì ì¸ ì²´ìœ¡ í™œë™", "ì¶©ë¶„í•œ ìˆ˜ë¶„", "ê·œì¹™ì  ì•½ ë³µìš©"],
            "caution": ["ìš´ë™ ê°•ë„ ì¡°ì ˆ", "ì¤‘ê°„ì¤‘ê°„ íœ´ì‹", "ì¦ìƒ ë°œìƒ ì‹œ ì¤‘ë‹¨"],
            "warning": ["ê²©ë ¬í•œ ìš´ë™ ê¸ˆì§€", "í¡ìž…ê¸° í•„ìˆ˜ ì§€ì°¸", "ì‹¤ì™¸ ì´ë™ ì „ë©´ ì œí•œ"]
        },
        "rhinitis": {
            "ok": ["ì½”ë¡œ ìˆ¨ì‰¬ê¸° ì§‘ì¤‘", "í™˜ê¸° ë° ì²­ì†Œ", "ì½” ì„¸ì²™"],
            "caution": ["í…€ë¸”ëŸ¬ ì§€ì°¸", "ë¨¼ì§€ ë§Žì€ ê³³ í”¼í•˜ê¸°", "ì† ì”»ê¸°"],
            "warning": ["ì‹ì—¼ìˆ˜ ì½” ì„¸ì²™", "ëˆˆ ë¹„ë¹„ì§€ ì•Šê¸°", "ì¸ê³µëˆˆë¬¼ ì‚¬ìš©"]
        },
        "atopy": {
            "ok": ["ë³´ìŠµì œ ë„í¬", "ë©´ ì†Œìž¬ ë‚´ì˜", "ì¶©ë¶„í•œ ìˆ˜ë©´"],
            "caution": ["ë•€ ë‹¦ëŠ” ìˆ˜ê±´ í•„ìˆ˜", "ë³´ìŠµì œ ë§ë°”ë¥´ê¸°", "í†µí’ë˜ëŠ” ì˜·"],
            "warning": ["ë¯¸ì§€ê·¼í•œ ë¬¼ ìƒ¤ì›Œ", "ê¸‰ê²©í•œ ì˜¨ë„ë³€í™” ì£¼ì˜", "ì•½ì‚°ì„± í´ë Œì € ì„¸ì•ˆ"]
        }
    },
    "teen_adult": {
        "general": {
            "ok": ["ì •ìƒì ì¸ í™œë™ ê°€ëŠ¥", "ì¶©ë¶„í•œ ìˆ˜ë©´", "ìˆ˜ë¶„ ì„­ì·¨"],
            "caution": ["ì•¼ì™¸ ìš´ë™ ê°•ë„ í•˜í–¥", "ë§ˆìŠ¤í¬ íœ´ëŒ€", "ìž¥ì‹œê°„ ë…¸ì¶œ ìžì œ"],
            "warning": ["ì•¼ì™¸ í™œë™ ëŒ€ì‹  ì‹¤ë‚´", "ì°½ë¬¸ ë°€í", "í˜¸í¡ê¸° ì¦ìƒ ê´€ì°°"]
        },
        "rhinitis": {
            "ok": ["í™˜ê¸° ë° ì‹¤ë‚´ ì •í™”", "ì¶©ë¶„í•œ íœ´ì‹", "ì½” ì„¸ì²™"],
            "caution": ["ì™¸ì¶œ ì‹œ ë§ˆìŠ¤í¬ í•„ìˆ˜", "ì½” ì£¼ë³€ ë³´ìŠµ", "ë¬¼ ìžì£¼ ë§ˆì‹œê¸°"],
            "warning": ["ì‹ì—¼ìˆ˜ ì½” ì„¸ì²™", "ê·€ê°€ ì¦‰ì‹œ ì„¸ì•ˆ", "ê³µê¸°ì²­ì •ê¸° í’€ê°€ë™"]
        },
        "asthma": {
            "ok": ["ìœ ì‚°ì†Œ ìš´ë™ ê¶Œìž¥", "ì»¨ë””ì…˜ ê´€ë¦¬", "ì •ê¸° ê²€ì§„"],
            "caution": ["ì•¼ì™¸ í™œë™ ì‹œê°„ ë‹¨ì¶•", "í¡ìž…ê¸° ì†Œì§€", "ë¬´ë¦¬í•œ ìš´ë™ ê¸ˆì§€"],
            "warning": ["ì™¸ì¶œ ì „ë©´ ê¸ˆì§€", "ì‹¤ë‚´ ìŠµë„ ì¡°ì ˆ", "ì¦ìƒ ì•…í™” ì‹œ ë‚´ì›"]
        },
        "atopy": {
            "ok": ["ê³ ë³´ìŠµ í¬ë¦¼ ì‚¬ìš©", "ìžê·¹ ì—†ëŠ” ì„¸ì•ˆ", "ì¶©ë¶„í•œ ìˆ˜ë¶„"],
            "caution": ["ë•€ ë¶„ë¹„ ì‹œ ì¦‰ì‹œ ë‹¦ê¸°", "ë³´ìŠµì œ ìˆ˜ì‹œ ë„í¬", "ë©´ ì†Œìž¬ ì˜ë¥˜"],
            "warning": ["ì•½ì‚°ì„± í´ë Œì € ì‚¬ìš©", "ì¦‰ê°ì ì¸ í”¼ë¶€ ì§„ì •", "ë…¸ì¶œ ë¶€ìœ„ ìµœì†Œí™”"]
        }
    }
}


def _get_display_content(age_group: str, condition: str, decision_key: str):
    """
    Returns (decision_text, action_items)
    """
    # Normalize condition
    cond_key = condition if condition in ["general", "rhinitis", "asthma", "atopy"] else "general"
    
    # Get Text
    group_data = DECISION_TEXTS.get(age_group, DECISION_TEXTS["elementary_high"])
    cond_data = group_data.get(cond_key, group_data.get("general", {}))
    d_text = cond_data.get(decision_key, "ìƒíƒœ í™•ì¸ í•„ìš”")
    
    # Get Actions
    group_actions = ACTION_ITEMS.get(age_group, ACTION_ITEMS.get("toddler", {}))
    cond_actions = group_actions.get(cond_key, group_actions.get("general", {}))
    actions = cond_actions.get(decision_key, ["ìƒíƒœì— ë”°ë¥¸ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."])
    
    return d_text, actions[:] # Return a copy

try:
    vo_client = voyageai.Client(api_key=VOYAGE_API_KEY)
except Exception as e:
    print(f"Error initializing Voyage AI client: {e}")
    vo_client = None

try:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    openai_client = None

async def get_air_quality(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch air quality data for the given station and today's date.
    """
    if db is None:
        raise Exception("Database connection not initialized")
        
    today_str = datetime.now(KST_TZ).strftime("%Y-%m-%d")
    
    # Try to find today's data for the station
    # Note: In a real scenario, you might need to query an external API if DB doesn't have it.
    # For this task, we assume it's in the DB or we simulate it if not found (for dev purposes).
    
    try:
        result = await db[AIR_QUALITY_COLLECTION].find_one({
            "stationName": station_name,
            "date": today_str
        })
        
        if result:
            # Inject mock weather data if not present in the real record
            if "temp" not in result: result["temp"] = 22.0
            if "humidity" not in result: result["humidity"] = 45.0
            return result
            
        # Mock data if not found (for demonstration purposes as requested structure implies data exists)
        # In production, this should return None or raise specific error
        print(f"No air quality data found for {station_name} on {today_str}. Using mock data.")
        return {
            "stationName": station_name,
            "date": today_str,
            "pm10_grade": "ë‚˜ì¨",
            "pm25_grade": "ë‚˜ì¨",
            "co_grade": "ë³´í†µ",
            "o3_grade": "ë³´í†µ",
            "no2_grade": "ì¢‹ìŒ",
            "so2_grade": "ì¢‹ìŒ",
            "integrated_grade": "ë‚˜ì¨",
            "temp": 22.0,       # Added temp
            "humidity": 45.0    # Added humidity
        }
    except Exception as e:
        print(f"Error fetching air quality: {e}")
        raise e

CACHE_COLLECTION = "rag_cache"
CACHE_TTL_SECONDS = 60 * 60 * 30  # 30 hours
_cache_ttl_index_ready = False

def _generate_cache_key(air_data: Dict[str, Any], user_profile: Dict[str, Any]) -> str:
    grade_map = {"ì¢‹ìŒ": 1, "ë³´í†µ": 2, "ë‚˜ì¨": 3, "ë§¤ìš°ë‚˜ì¨": 4}
    
    pm25 = grade_map.get(air_data.get("pm25_grade", ""), 0)
    pm10 = grade_map.get(air_data.get("pm10_grade", ""), 0)
    o3 = grade_map.get(air_data.get("o3_grade", ""), 0) # Added o3 as per user example
    
    age_group = _normalize_age_group(user_profile.get("ageGroup"))
    condition = user_profile.get("condition", "unknown")
    date_key = air_data.get("date") or datetime.now(KST_TZ).strftime("%Y-%m-%d")
    
    # Key format: pm25:3_pm10:2_o3:1_age:adult_cond:asthma_date:2026-01-28
    return f"pm25:{pm25}_pm10:{pm10}_o3:{o3}_age:{age_group}_cond:{condition}_date:{date_key}"

async def _ensure_cache_ttl_index():
    global _cache_ttl_index_ready
    if _cache_ttl_index_ready or db is None:
        return
    try:
        await db[CACHE_COLLECTION].create_index(
            "created_at",
            expireAfterSeconds=CACHE_TTL_SECONDS,
            name="rag_cache_ttl"
        )
        _cache_ttl_index_ready = True
    except Exception as e:
        print(f"âš ï¸ Cache TTL index creation failed: {e}")

async def get_medical_advice(station_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main orchestration function with correction logic.
    """
    # Step A: Get Air Quality
    air_data = await get_air_quality(station_name)
    if not air_data:
        raise ValueError(f"No air quality data found for station: {station_name}")

    # Extract Weather Info for Correction
    temp = air_data.get("temp")
    humidity = air_data.get("humidity")
    user_condition = user_profile.get("condition", "ê±´ê°•í•¨")
    age_group_raw = user_profile.get("ageGroup")
    age_group = _normalize_age_group(age_group_raw)

    # Apply Correction Logic to get "Sensed" grades
    pm25_raw = air_data.get("pm25_grade", "ë³´í†µ")
    o3_raw = air_data.get("o3_grade", "ë³´í†µ")
    
    pm25_corrected = _get_corrected_grade(pm25_raw, temp, humidity, user_condition, "pm25")
    o3_corrected = _get_corrected_grade(o3_raw, temp, humidity, user_condition, "o3")

    cache_key = ""
    # [Step A.1] Check Cache
    if db is not None:
        try:
            await _ensure_cache_ttl_index()
            # Simple key extension: add T/H to capture environmental context
            cache_key = _generate_cache_key(air_data, user_profile) + f"_T:{temp}_H:{humidity}"
            cached_entry = await db[CACHE_COLLECTION].find_one({"_id": cache_key})
            
            if cached_entry:
                print(f"âœ… Cache Hit! Key: {cache_key}")
                return cached_entry["data"]
        except Exception as e:
            print(f"âš ï¸ Cache check failed: {e}")

    # Determine main issue for search (using corrected grades)
    main_condition = "ë³´í†µ"
    if pm25_corrected in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ì´ˆë¯¸ì„¸ë¨¼ì§€ {pm25_corrected}"
    elif air_data.get("pm10_grade") in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ë¯¸ì„¸ë¨¼ì§€ {air_data['pm10_grade']}"
    elif o3_corrected in ["ë‚˜ì¨", "ë§¤ìš°ë‚˜ì¨"]:
        main_condition = f"ì˜¤ì¡´ {o3_corrected}"
        
    # Step B: Query Construction
    search_query = f"{main_condition} ìƒí™©ì—ì„œ {user_condition} {age_group} í–‰ë™ ìš”ë ¹ ì£¼ì˜ì‚¬í•­"
    print(f"Generated Search Query (Primary): {search_query}")

    # Step C: Vector Search
    relevant_docs = []
    if vo_client and db is not None:
        try:
            # 1. Primary Search
            embed_result = vo_client.embed([search_query], model="voyage-3-large", input_type="query")
            query_vector = embed_result.embeddings[0]
            
            pipeline = [
                {
                    "$vectorSearch": {
                        "index": VECTOR_INDEX,
                        "path": "embedding",
                        "queryVector": query_vector,
                        "numCandidates": 100,
                        "limit": 3
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "text": 1,
                        "category": 1,
                        "risk_level": 1,
                        "source": 1,
                        "score": {"$meta": "vectorSearchScore"}
                    }
                }
            ]
            
            cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
            relevant_docs = await cursor.to_list(length=3)
            
            # 2. Fallback Search (If no docs found)
            if not relevant_docs:
                print("âš ï¸ Primary search returned no results. Attempting fallback (General) search.")
                fallback_query = f"{main_condition} í–‰ë™ ìš”ë ¹"
                embed_result_fb = vo_client.embed([fallback_query], model="voyage-3-large", input_type="query")
                query_vector_fb = embed_result_fb.embeddings[0]
                
                pipeline[0]["$vectorSearch"]["queryVector"] = query_vector_fb
                
                cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
                relevant_docs = await cursor.to_list(length=3)
                
        except Exception as e:
            print(f"Error during vector search: {e}")
            pass

    # Step D: LLM Generation
    if not openai_client:
         return {
            "decision": "Error",
            "reason": "OpenAI Client not initialized",
            "actionItems": [],
            "references": []
        }

    # [Logic Update] Calculate Deterministic Decision & Action Items using CORRECTED grades
    decision_key = _calculate_decision(pm25_corrected, o3_corrected)
    decision_text, action_items = _get_display_content(age_group, decision_key)
    
    # O3 Special Handling: Force-Append and Warnings
    is_o3_dominant = GRADE_MAP.get(o3_corrected, 1) >= GRADE_MAP.get(pm25_corrected, 1)
    if is_o3_dominant and GRADE_MAP.get(o3_corrected, 1) >= 3: # 'ë‚˜ì¨' ì´ìƒ
        decision_text += " (ì˜¤ì¡´ì€ ë§ˆìŠ¤í¬ë¡œ ê±¸ëŸ¬ì§€ì§€ ì•Šì•„ìš”!)"
        # Force-Append Action Item
        o3_force_action = "ì˜¤í›„ 2~5ì‹œ ì‚¬ì´ì—ëŠ” ì‹¤ì™¸ í™œë™ì„ ì „ë©´ ê¸ˆì§€í•˜ê³  ì‹¤ë‚´ì— ë¨¸ë¬´ë¥´ì„¸ìš”."
        if o3_force_action not in action_items:
            action_items.append(o3_force_action)

    # Infant Special Warning
    if age_group == "infant":
        infant_warning = "â€» ì£¼ì˜: ë§ˆìŠ¤í¬ ì°©ìš© ê¸ˆì§€(ì§ˆì‹ ìœ„í—˜)"
        if infant_warning not in action_items:
            action_items.insert(0, infant_warning) # Put at top

    # Logic for dual bad condition text append
    if GRADE_MAP.get(pm25_corrected, 1) >= 3 and GRADE_MAP.get(o3_corrected, 1) >= 3:
        decision_text += " (ë¯¸ì„¸ë¨¼ì§€ì™€ ì˜¤ì¡´ ë‘˜ ë‹¤ ë†’ì•„ìš”!)"

    # Prepare Context
    context_text = "\n".join([f"- [ì¶œì²˜: {doc.get('source', 'ê°€ì´ë“œë¼ì¸')}] {doc.get('text', '')}" for doc in relevant_docs]) if relevant_docs else "ê´€ë ¨ ì˜í•™ì  ê°€ì´ë“œë¼ì¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    system_prompt = """
    ë‹¹ì‹ ì€ í™˜ê²½ë³´ê±´ ì˜ì‚¬ìž…ë‹ˆë‹¤. ëŒ€ê¸°ì§ˆ ë°ì´í„°(ì˜¨ë„, ìŠµë„ í¬í•¨)ì™€ í™˜ìžì˜ ê¸°ì €ì§ˆí™˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨ ê·¼ê±°(Reason)ë¥¼ ìž‘ì„±í•´ì£¼ì„¸ìš”.
    
    [ì¤‘ìš”]
    1. 'decision'ê³¼ 'actionItems'ëŠ” ì´ë¯¸ ì‹œìŠ¤í…œì—ì„œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¹ì‹ ì€ ì´ ê²°ì •ì´ ë‚´ë ¤ì§„ 'ì˜í•™ì /í™˜ê²½ì  ì´ìœ (reason)'ë¥¼ ìž‘ì„±í•˜ì„¸ìš”.
    2. ë³´ì • ë¡œì§ì´ ì ìš©ëœ ê²½ìš°(ì˜ˆ: ìŠµë„ê°€ ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ì•„ì„œ, í˜¹ì€ íŠ¹ì • ì§ˆí™˜ íŠ¸ë¦¬ê±°ë¡œ ì¸í•´ ë“±ê¸‰ì´ ê²©ìƒë¨) ê·¸ ì´ìœ ë¥¼ ì„¤ëª…ì— í¬í•¨í•˜ì„¸ìš”.
    3. ì œê³µëœ [ì˜í•™ì  ê°€ì´ë“œë¼ì¸] ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ë°˜ì˜í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
    4. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    user_prompt = f"""
    [ìƒí™© ì •ë³´]
    - ëŒ€ê¸°ì§ˆ: ì´ˆë¯¸ì„¸ë¨¼ì§€={pm25_raw}(ë³´ì •í›„:{pm25_corrected}), ì˜¤ì¡´={o3_raw}(ë³´ì •í›„:{o3_corrected})
    - í™˜ê²½: ì˜¨ë„={temp}Â°C, ìŠµë„={humidity}%
    - ì‚¬ìš©ìž: ì—°ë ¹ëŒ€={age_group}, ê¸°ì €ì§ˆí™˜={user_condition}
    - ì‹œìŠ¤í…œ ê²°ì •: {decision_text}
    - ì‹œìŠ¤í…œ í–‰ë™ìˆ˜ì¹™: {action_items}
    
    [ì˜í•™ì  ê°€ì´ë“œë¼ì¸ (ì°¸ê³  ë¬¸í—Œ)]
    {context_text}
    
    ìœ„ ê²°ì •ì´ ë‚´ë ¤ì§„ ë°°ê²½ê³¼ ì´ìœ ë¥¼ ì˜¨ë„, ìŠµë„, ì§ˆí™˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=1 
        )
        
        content = response.choices[0].message.content
        llm_result = json.loads(content)
        
        # Merge Results
        final_result = {
            "decision": decision_text,
            "reason": llm_result.get("reason", "ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."),
            "actionItems": action_items,
            "references": list(set([doc.get("source", "Unknown Source") for doc in relevant_docs]))
        }
        
        # [Step F] Save to Cache
        if db is not None and cache_key:
            try:
                await db[CACHE_COLLECTION].update_one(
                    {"_id": cache_key},
                    {"$set": {"data": final_result, "created_at": datetime.now(KST_TZ)}},
                    upsert=True
                )
                print(f"ðŸ’¾ Saved to cache: {cache_key}")
            except Exception as e:
                print(f"Error saving to cache: {e}")
                
        return final_result
        
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        # Fallback even if LLM fails, we satisfy the deterministic requirement
        return {
            "decision": decision_text,
            "reason": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ë¡œ ìƒì„¸ ì„¤ëª…ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í–‰ë™ ì§€ì¹¨ì€ ìœ„ì™€ ê°™ì´ ì¤€ìˆ˜í•´ì£¼ì„¸ìš”.",
            "actionItems": action_items,
            "references": []
        }

async def ingest_pdf(file_content: bytes, filename: str) -> Dict[str, Any]:
    """
    Ingest PDF content: Extract text -> Embed -> Store in DB.
    """
    import io
    from PyPDF2 import PdfReader

    if not vo_client:
        return {"status": "error", "message": "Voyage AI Client not initialized"}
    
    if db is None:
        return {"status": "error", "message": "Database not initialized"}

    try:
        # 1. Read PDF
        pdf_file = io.BytesIO(file_content)
        reader = PdfReader(pdf_file)
        
        extracted_text = ""
        documents_to_insert = []
        texts_to_embed = []
        
        # 2. Extract Text per Page (Chunking Strategy: 1 Page = 1 Doc for simplicity)
        print(f"ðŸ“„ Processing PDF: {filename} ({len(reader.pages)} pages)")
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 50: # Ignore empty/too short pages
                extracted_text += text + "\n"
                
                texts_to_embed.append(text)
                documents_to_insert.append({
                    "text": text,
                    "category": "pdf_upload",
                    "source": filename,
                    "page": i + 1,
                    "risk_level": "unknown", # Needs manual classification or LLM analysis
                    "created_at": datetime.now()
                })
        
        if not documents_to_insert:
            return {"status": "error", "message": "No extractable text found in PDF."}

        # 3. Embed Data
        print(f"ðŸ§  Embedding {len(texts_to_embed)} pages with Voyage AI...")
        result = vo_client.embed(texts_to_embed, model="voyage-3-large", input_type="document")
        embeddings = result.embeddings
        
        for i, doc in enumerate(documents_to_insert):
            doc["embedding"] = embeddings[i]
            
        # 4. Insert into DB
        insert_result = await db[GUIDELINES_COLLECTION].insert_many(documents_to_insert)
        
        return {
            "status": "success",
            "message": f"Successfully ingested {len(insert_result.inserted_ids)} pages from {filename}",
            "inserted_ids": [str(id) for id in insert_result.inserted_ids]
        }
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        return {"status": "error", "message": str(e)}
