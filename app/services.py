import os
import json
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse
import voyageai
from openai import OpenAI
from motor.motor_asyncio import AsyncIOMotorClient
from bson import ObjectId
from dotenv import load_dotenv

load_dotenv()

# Configuration
MONGO_URI = os.getenv("MONGO_URI") or os.getenv("MONGODB_URI")
VOYAGE_API_KEY = os.getenv("VOYAGE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def _infer_db_name_from_uri(uri: Optional[str]) -> Optional[str]:
    if not uri:
        return None
    try:
        parsed = urlparse(uri)
        # mongodb+srv://host/<db>?...
        db_name = parsed.path.lstrip("/").split("?")[0].strip()
        return db_name or None
    except Exception:
        return None


DB_NAME = os.getenv("MONGO_DB_NAME", "epilog_db")
AIR_QUALITY_DB_NAME = os.getenv("AIR_QUALITY_DB_NAME") or _infer_db_name_from_uri(MONGO_URI) or DB_NAME
GUIDELINES_COLLECTION = "medical_guidelines"
AIR_QUALITY_COLLECTION = "daily_air_quality"
AIR_QUALITY_DATA_COLLECTION = "air_quality_data"  # Lambda cron job collection
VECTOR_INDEX = "vector_index"
KST_TZ = ZoneInfo("Asia/Seoul")

if not MONGO_URI:
    # Fallback to a dummy URI if not set to prevent startup crash, but it will fail on request
    print("WARNING: MONGODB_URI is not set.")
    MONGO_URI = "mongodb://localhost:27017"

# Initialize Clients
try:
    mongo_client = AsyncIOMotorClient(MONGO_URI)
    db = mongo_client[DB_NAME]
    air_quality_db = mongo_client[AIR_QUALITY_DB_NAME]
    print(f"‚úÖ MongoDB connected: main_db={DB_NAME}, air_quality_db={AIR_QUALITY_DB_NAME}")
except Exception as e:
    print(f"Error initializing MongoDB client: {e}")
    mongo_client = None
    db = None
    air_quality_db = None

# --- Logic Constants ---
GRADE_MAP = {
    "Ï¢ãÏùå": 1,
    "Î≥¥ÌÜµ": 2,
    "ÎÇòÏÅ®": 3,
    "Îß§Ïö∞ÎÇòÏÅ®": 4
}

REVERSE_GRADE_MAP = {v: k for k, v in GRADE_MAP.items()}

# Correction Weights
HUMIDITY_WEIGHTS = {
    "high": 1.2,  # > 70%
    "low": 1.1,   # < 30%
    "normal": 1.0
}

def _get_corrected_grade(
    base_grade: str, 
    temp: Optional[float], 
    humidity: Optional[float], 
    condition: str,
    pollutant_type: str # "pm25" or "o3"
) -> str:
    """
    Apply correction logic based on temperature, humidity and disease condition.
    Returns the corrected grade string.
    """
    score = GRADE_MAP.get(base_grade, 2)
    
    # 1. Humidity Correction (W_h)
    w_h = 1.0
    if humidity is not None:
        if humidity > 70:
            w_h = HUMIDITY_WEIGHTS["high"]
        elif humidity < 30:
            w_h = HUMIDITY_WEIGHTS["low"]
    
    # 2. Temperature & Disease Trigger Logic
    # Asthma + Cold + PM2.5
    if condition == "asthma" and temp is not None and temp < 5 and pollutant_type == "pm25":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # Rhinitis + Dry + PM2.5
    if condition == "rhinitis" and humidity is not None and humidity < 30 and pollutant_type == "pm25":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # Atopy + Heat + O3
    if condition == "atopy" and temp is not None and temp > 30 and pollutant_type == "o3":
        if base_grade == "Î≥¥ÌÜµ": return "ÎÇòÏÅ®"
        
    # General + High Humidity + PM2.5 Bad
    if humidity is not None and humidity > 80 and pollutant_type == "pm25" and base_grade == "ÎÇòÏÅ®":
        return "Îß§Ïö∞ÎÇòÏÅ®"

    # Apply multiplicative weight if no specific trigger fired
    # (Simplified: if score * w_h rounds up to next grade)
    final_score = min(4, max(1, round(score * w_h)))
    return REVERSE_GRADE_MAP.get(final_score, base_grade)


def _normalize_station_candidates(station_name: str) -> List[str]:
    cleaned = " ".join((station_name or "").strip().split())
    if not cleaned:
        return []

    seen = set()
    candidates: List[str] = []

    def add(value: str):
        normalized = " ".join(value.strip().split())
        if not normalized or normalized in seen:
            return
        seen.add(normalized)
        candidates.append(normalized)

    add(cleaned)
    add(cleaned.replace(" ", ""))

    tokens = cleaned.split(" ")
    if len(tokens) >= 2:
        add(tokens[-1])
        add(tokens[-2])
        add(f"{tokens[-2]} {tokens[-1]}")
    elif tokens:
        add(tokens[0])

    return candidates


def _parse_datetime_to_kst(value: Any) -> Optional[datetime]:
    if value is None:
        return None

    dt: Optional[datetime] = None

    if isinstance(value, datetime):
        dt = value
    elif isinstance(value, str):
        text = value.strip()
        if not text:
            return None

        # AirKorea Lambda snapshot format: "YYYY-MM-DD HH:MM"
        try:
            parsed = datetime.strptime(text, "%Y-%m-%d %H:%M")
            dt = parsed.replace(tzinfo=KST_TZ)
        except ValueError:
            try:
                dt = datetime.fromisoformat(text.replace("Z", "+00:00"))
            except ValueError:
                return None
    else:
        return None

    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=KST_TZ)

    return dt.astimezone(KST_TZ)

# Decision Texts based on logic.csv (80-segment dataset)
DECISION_TEXTS = {
    "infant": {
        "general": {
            "ok": "Ïú†Î™®Ï∞® ÏÇ∞Ï±Ö Í∞ÄÏöî",
            "caution": "ÏßßÏùÄ ÏÇ∞Ï±ÖÎßå Ï∂îÏ≤ú",
            "warning": "Ïô∏Î∂Ä Í≥µÍ∏∞ Ï†ÑÎ©¥ Ï∞®Îã®"
        },
        "rhinitis": {
            "ok": "ÏΩîÍ∞Ä Ìé∏ÏïàÌïú ÎÇ†",
            "caution": "ÏΩîÏ†êÎßâ Î≥¥Ïäµ ÏßëÏ§ë",
            "warning": "Ìò∏Ìù° Í≥§ÎûÄ Ï£ºÏùò"
        },
        "asthma": {
            "ok": "ÏÉÅÏæåÌïòÍ≤å Ïà® Ïâ¨Ïñ¥Ïöî",
            "caution": "Ï∞¨ Î∞îÎûå ÎÖ∏Ï∂ú Ï£ºÏùò",
            "warning": "Ï†àÎåÄ ÏïàÏ†ï Ïã§ÎÇ¥ ÎåÄÍ∏∞"
        },
        "atopy": {
            "ok": "Í∞ÄÎ†§ÏõÄ Í±±Ï†ï Îöù",
            "caution": "ÎïÄÎÇòÎ©¥ Î∞îÎ°ú Îã¶ÏïÑÏöî",
            "warning": "Î≥¥ÏäµÏ†ú 2Î∞∞ ÎèÑÌè¨"
        }
    },
    "toddler": {
        "general": {
            "ok": "ÎÜÄÏù¥ÌÑ∞ÏóêÏÑú Îõ∞ÎÜÄÏïÑÏöî",
            "caution": "Î¨º Ìïú Ïªµ ÎßàÏãúÍ≥† Ïô∏Ï∂ú",
            "warning": "Ïò§ÎäòÏùÄ ÏßëÏóêÏÑú ÎÜÄÏïÑÏöî"
        },
        "rhinitis": {
            "ok": "ÏΩî Î©¥Ïó≠Î†• ÌÇ§Ïö∞Îäî ÎÇ†",
            "caution": "Ïû¨Ï±ÑÍ∏∞ Î®ºÏßÄ Ï°∞Ïã¨",
            "warning": "ÌôòÍ∏∞ Í∏àÏßÄ Î¨ºÍ±∏Î†àÏßà"
        },
        "asthma": {
            "ok": "Í∏∞ÎèÑÍ∞Ä Ïó¥Î¶¨Îäî ÎÇ†Ïî®",
            "caution": "Í∞ëÏûëÏä§Îü∞ Í∏∞Ïπ® Ï£ºÏùò",
            "warning": "Ïã§Ïô∏ Ïù¥Îèô Ï†ÑÎ©¥ Ï†úÌïú"
        },
        "atopy": {
            "ok": "ÌîºÎ∂ÄÍ∞Ä Ïà® Ïâ¨Îäî ÎÇ†",
            "caution": "ÎïÄÍ≥º Î®ºÏßÄÎ•º Î©ÄÎ¶¨Ìï¥Ïöî",
            "warning": "ÌîºÎ∂Ä ÏßÑÏ†ï Ìå© Ï∂îÏ≤ú"
        }
    },
    "elementary_low": {
        "general": {
            "ok": "Ïö¥ÎèôÏû•ÏóêÏÑú ÎßàÏùåÍªè!",
            "caution": "Ï≤¥Ïú° Ï†Ñ ÏÉÅÌÉú ÌôïÏù∏",
            "warning": "Ïã§Ïô∏ ÏàòÏóÖ Ï∞∏Ïó¨ Ï†úÏô∏"
        },
        "rhinitis": {
            "ok": "Ïà≤ Ï≤¥Ìóò Í∞ÄÍ∏∞ Ï¢ãÏùÄ ÎÇ†",
            "caution": "ÎßàÏä§ÌÅ¨ Ïì∞Í≥† Îì±Íµê",
            "warning": "ÏΩîÎßâÌûò Ï¶ùÏÉÅ Í¥ÄÎ¶¨"
        },
        "asthma": {
            "ok": "Ïª®ÎîîÏÖò ÏµúÏÉÅÏù∏ ÎÇ†",
            "caution": "Ïö¥Îèô Í∞ïÎèÑÎ•º Ï°∞Ï†àÌï¥Ïöî",
            "warning": "Î∞úÏûë ÏúÑÌóò Ïã§Ïô∏ Í∏àÏßÄ"
        },
        "atopy": {
            "ok": "ÏûêÏô∏ÏÑ† Ï∞®Îã®Ï†ú ÌïÑÏàò",
            "caution": "Í∏¥ÏÜåÎß§Î°ú ÌîºÎ∂Ä Î≥¥Ìò∏",
            "warning": "Í∞ÄÎ†§ÏõÄÏ¶ù Ïã¨Ìôî Ï£ºÏùò"
        }
    },
    "elementary_high": {
        "general": {
            "ok": "ÌôòÍ∏∞ÌïòÎ©∞ Í≥µÎ∂ÄÌï¥Ïöî",
            "caution": "ÌïòÍµê ÌõÑ ÏÜêÎ∞ú ÏîªÍ∏∞",
            "warning": "Ïã§Ïô∏ ÎÖ∏Ï∂ú ÏµúÏÜåÌôî"
        },
        "rhinitis": {
            "ok": "ÏΩîÍ∞Ä ÏãúÏõêÌïú Îì±Íµ£Í∏∏",
            "caution": "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄÌïòÍ∏∞",
            "warning": "ÌôòÍ∏∞ ÎåÄÏã† Í≥µÏ≤≠Í∏∞"
        },
        "asthma": {
            "ok": "ÌôúÍ∏∞Ï∞®Í≤å Ïö¥ÎèôÌï¥Ïöî",
            "caution": "Ïö¥Îèô Ï†ÑÌõÑ ÏàòÎ∂Ñ Î≥¥Ï∂©",
            "warning": "Ïã§Ïô∏ Ïù¥Îèô Ï†ÑÎ©¥ Ï†úÌïú"
        },
        "atopy": {
            "ok": "ÏÉÅÏæåÌïú ÏïºÏô∏ ÌôúÎèô",
            "caution": "Î≥¥ÏäµÎßâ Ïú†ÏßÄÌïòÍ∏∞",
            "warning": "ÌîºÎ∂Ä ÏßÑÏ†ï Î∞è Ï∞®Îã®"
        }
    },
    "teen_adult": {
        "general": {
            "ok": "ÏïºÏô∏ Ïö¥Îèô Ï∂îÏ≤ú",
            "caution": "ÏùºÏÉÅ ÌôúÎèô Î¨¥Í¥Ä",
            "warning": "Ïã§Ïô∏ ÌôúÎèô ÏµúÏÜåÌôî"
        },
        "rhinitis": {
            "ok": "ÏÉÅÏæåÌïú Ìò∏Ìù°",
            "caution": "ÏΩî ÏÑ∏Ï†ï Ï§ÄÎπÑ",
            "warning": "Ïô∏Î∂Ä Í≥µÍ∏∞ Ï∞®Îã®"
        },
        "asthma": {
            "ok": "Ïú†ÏÇ∞ÏÜå Ïö¥Îèô Í∞ÄÎä•",
            "caution": "Î¨¥Î¶¨Ìïú ÌôúÎèô ÏûêÏ†ú",
            "warning": "Ïã§Ïô∏ ÌôúÎèô Ï†ÑÎ©¥ Ï†úÌïú"
        },
        "atopy": {
            "ok": "ÏïºÏô∏ ÎÇòÎì§Ïù¥ Ï∂îÏ≤ú",
            "caution": "ÌîºÎ∂Ä Ï≤≠Í≤∞ Ïú†ÏßÄ",
            "warning": "Ï¶âÍ∞ÅÏ†ÅÏù∏ ÌîºÎ∂Ä ÏÑ∏Ï†ï"
        }
    }
}

# Action Items templates based on logic.csv
ACTION_ITEMS = {
    "infant": {
        "general": {
            "ok": ["Ïú†Î™®Ï∞® ÏÇ∞Ï±Ö", "15Î∂Ñ ÌôòÍ∏∞", "Í∑ÄÍ∞Ä ÌõÑ ÏÜêÎ∞ú ÏîªÍ∏∞"],
            "caution": ["Ïú†Î™®Ï∞® Ïª§Î≤Ñ ÏÇ¨Ïö©", "Í∑∏Îäò ÏúÑÏ£º ÏÇ∞Ï±Ö", "Î≥µÍ∑Ä ÌõÑ Î≥¥Ïäµ"],
            "warning": ["Ïô∏Ï∂ú Ï†àÎåÄ Í∏àÏßÄ", "Ï∞ΩÎ¨∏ ÌãàÏÉà Î∞ÄÌèê", "Î¨ºÍ±∏Î†à Ï≤≠ÏÜå"]
        },
        "rhinitis": {
            "ok": ["ÏæåÏ†ÅÌïú ÌôòÍ∏∞", "ÏßëÏïà Î®ºÏßÄ ÌÑ∏Í∏∞", "Í∞ÄÎ≤ºÏö¥ Ïô∏Ï∂ú"],
            "caution": ["Í∞ÄÏäµÍ∏∞ Í∞ÄÎèô", "ÎØ∏ÏßÄÍ∑ºÌïú Î¨º ÏÑ≠Ï∑®", "Ïú†Î™®Ï∞® Í∞ÄÎ¶ºÎßâ"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞", "ÏÉÅÎπÑÏïΩ ÌôïÏù∏", "ÏÉÅÌÉú ÏßëÏ§ë Î™®ÎãàÌÑ∞"]
        },
        "asthma": {
            "ok": ["Ïã†ÏÑ†Ìïú Í≥µÍ∏∞ Ïú†ÏûÖ", "Î≥¥Ìò∏ÏûêÏôÄ ÏÇ∞Ï±Ö", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù"],
            "caution": ["Î™© Í∞ÄÏã∏Í∞ú ÏÇ¨Ïö©", "Í∏âÍ≤©Ìïú Ïò®ÎèÑÏ∞® Ï£ºÏùò", "Ìò∏Ìù° ÏàòÏãú Í¥ÄÏ∞∞"],
            "warning": ["Î≥ëÏõê Ïô∏ Ïô∏Ï∂ú Í∏àÏßÄ", "Ï¶ùÏÉÅ ÎåÄÏùë Ï§ÄÎπÑ", "Í≥µÏ≤≠Í∏∞ ÌíÄÍ∞ÄÎèô"]
        },
        "atopy": {
            "ok": ["Ïô∏Ï∂ú Ï†Ñ ÏÑ†ÌÅ¨Î¶º", "ÌôúÎèô ÌõÑ ÏÑ∏Ïïà", "Î©¥ ÏÜåÏû¨ Ïò∑ ÏûÖÍ∏∞"],
            "caution": ["ÏÜêÏàòÍ±¥ ÏßÄÏ∞∏", "ÏàòÏãúÎ°ú Î≥¥ÏäµÏ†ú", "ÏñáÏùÄ Í∏¥ÏÜåÎß§ Ïò∑"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ Ï≤¥Î•ò", "ÏãúÏõêÌïú Ïò®ÎèÑ Ïú†ÏßÄ", "Í≥†Î≥¥Ïäµ ÌÅ¨Î¶º ÏÇ¨Ïö©"]
        }
    },
    "toddler": {
        "general": {
            "ok": ["ÏïºÏô∏ ÎÜÄÏù¥ Í∂åÏû•", "Ï†ÑÎ©¥ ÌôòÍ∏∞ ÏãúÌÇ§Í∏∞", "ÌôúÎèô ÌõÑ ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "caution": ["Ï§ëÍ∞Ñ ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄÌïòÍ∏∞", "Ïû•ÏãúÍ∞Ñ Ï≤¥Î•ò ÏûêÏ†ú"],
            "warning": ["ÏïºÏô∏ ÌôúÎèô Í∏àÏßÄ", "Í≥µÍ∏∞Ï≤≠Ï†ïÍ∏∞ ÏÇ¨Ïö©", "Ïã§ÎÇ¥ Ï†ÅÏ†ï Í∞ÄÏäµ"]
        },
        "rhinitis": {
            "ok": ["Ïà≤ Ï≤¥Ìóò Ï∂îÏ≤ú", "ÌôòÍ∏∞ ÌõÑ Ï≤≠ÏÜå", "Ïô∏Ï∂ú ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÎßàÏä§ÌÅ¨ ÌïÑÏàò Ï∞©Ïö©", "ÏΩî Ï£ºÎ≥Ä Î≥¥Ïäµ", "ÏãùÏóºÏàò ÏÑ∏Ï≤ô"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞", "Í≥µÏ≤≠Í∏∞ Í∞ÄÎèô", "Ï¶ùÏÉÅ Ïãú ÏïΩ Î≥µÏö©"]
        },
        "asthma": {
            "ok": ["Ïú†ÏÇ∞ÏÜå ÎÜÄÏù¥", "Í∑úÏπôÏ†Å ÏïΩ Î≥µÏö©", "Ïã§ÎÇ¥Ïô∏ Í≥µÍ∏∞ Ï†ïÌôî"],
            "caution": ["Ïö¥Îèô Í∞ïÎèÑ ÎÇÆÏ∂îÍ∏∞", "Ï§ëÍ∞Ñ Ìú¥Ïãù Ï∑®ÌïòÍ∏∞", "Ìò∏Ìù° ÏÉÅÌÉú ÌôïÏù∏"],
            "warning": ["Ïô∏Ï∂ú Ï†àÎåÄ Í∏àÏßÄ", "Î≥¥Ìò∏Ïûê Î∞ÄÏ∞© Í¥ÄÏ∞∞", "ÎπÑÏÉÅ Ïãú Î≥ëÏõê Î∞©Î¨∏"]
        },
        "atopy": {
            "ok": ["ÏÑ†ÌÅ¨Î¶º ÎèÑÌè¨", "ÌôúÎèô ÌõÑ ÏÉ§Ïõå", "Î©¥ ÏÜåÏû¨ Ïò∑ Ï∂îÏ≤ú"],
            "caution": ["ÏàòÏãúÎ°ú ÎïÄ Îã¶Í∏∞", "Ïô∏Ï∂ú ÌõÑ Î≥¥Ïäµ", "ÌÜµÍ∏∞ÏÑ± ÏùòÎ•ò"],
            "warning": ["ÎÉâÏ∞úÏßà ÏßÑÏ†ï", "Ïã§Ïô∏ ÌôúÎèô Ï§ëÎã®", "ÏûêÍ∑π ÏóÜÎäî Î°úÏÖò"]
        }
    },
    "elementary_low": {
        "general": {
            "ok": ["Ï∂ïÍµ¨/Îã¨Î¶¨Í∏∞ Ï∂îÏ≤ú", "ÍµêÏã§ Ï†ÑÎ©¥ ÌôòÍ∏∞", "ÏïºÏô∏ ÌïôÏäµ"],
            "caution": ["Ï§ëÍ∞Ñ ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÌôúÎèô ÌõÑ ÏñëÏπò", "ÎåÄÍ∏∞Ïßà ÏàòÏãú Ï≤¥ÌÅ¨"],
            "warning": ["Ïã§ÎÇ¥ ÌôúÎèô Ï†ÑÌôò", "Ï∞ΩÎ¨∏ Î∞ÄÌèê Í¥ÄÎ¶¨", "Ïã§ÎÇ¥ Í≥µÍ∏∞ Ï†ïÌôî"]
        },
        "rhinitis": {
            "ok": ["ÏïºÏô∏ ÏÇ∞Ï±Ö", "ÌôòÍ∏∞ ÌõÑ ÎåÄÏ≤≠ÏÜå", "Ï∂©Î∂ÑÌïú Ìú¥Ïãù"],
            "caution": ["ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "ÏÜê ÏîªÍ∏∞ ÍµêÏú°", "Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞"],
            "warning": ["ÌôòÍ∏∞ Ï†àÎåÄ Í∏àÏßÄ", "ÏãùÏóºÏàò ÏΩî ÏÑ∏Ï≤ô", "Ï¶ùÏÉÅ ÏôÑÌôîÏ†ú Ï§ÄÎπÑ"]
        },
        "asthma": {
            "ok": ["ÌïôÍµê Ï≤¥Ïú° Ï∞∏Ïó¨", "ÍπäÏùÄ Ìò∏Ìù° Ïö¥Îèô", "Ïª®ÎîîÏÖò Ïú†ÏßÄ"],
            "caution": ["Î¨¥Î¶¨Ìïú Îã¨Î¶¨Í∏∞ ÏûêÏ†ú", "Ï§ëÍ∞Ñ Ìú¥Ïãù ÎäòÎ¶¨Í∏∞", "Ìò∏Ìù° Î™®ÎãàÌÑ∞ÎßÅ"],
            "warning": ["ÎÖ∏Ï∂ú Ï†ÑÎ©¥ Ï∞®Îã®", "Î≥¥Ìò∏Ïûê ÏÉÅÏãú Í¥ÄÏ∞∞", "ÎπÑÏÉÅÏïΩ ÏúÑÏπò ÌôïÏù∏"]
        },
        "atopy": {
            "ok": ["ÏÑ†ÌÅ¨Î¶º Î∞îÎ•¥Í∏∞", "ÏïºÏô∏ ÌôúÎèô Ï¶êÍ∏∞Í∏∞", "ÌôúÎèô ÌõÑ ÏÑ∏Ïïà"],
            "caution": ["ÏÜêÏàòÍ±¥ ÏßÄÏ∞∏", "Ïô∏Ï∂ú ÌõÑ Î≥¥ÏäµÏ†ú", "Î©¥ ÏÜçÏò∑ ÏûÖÌûàÍ∏∞"],
            "warning": ["Ïô∏Ï∂ú Í∏àÏßÄ", "ÎÉâÏ∞úÏßà ÏßÑÏ†ï", "Ïã§ÎÇ¥ ÏäµÎèÑ Ï°∞Ï†à"]
        }
    },
    "elementary_high": {
        "general": {
            "ok": ["Ï†ÑÎ©¥ ÌôòÍ∏∞ Ïã§Ïãú", "ÏïºÏô∏ Ïö¥Îèô Í∂åÏû•", "ÏûêÏ†ÑÍ±∞/ÎèÑÎ≥¥ Îì±Íµê"],
            "caution": ["Ïô∏Ï∂ú ÌõÑ ÏúÑÏÉù Í¥ÄÎ¶¨", "Î¨º ÏûêÏ£º ÎßàÏãúÍ∏∞", "ÍµêÏã§ ÌôòÍ∏∞ ÌòëÏ°∞"],
            "warning": ["Ïã§Ïô∏ ÌôúÎèô Ï†ÑÎ©¥ Ï§ëÎã®", "Ï∞ΩÎ¨∏ Î∞ÄÌèê ÌôïÏù∏", "Í≥µÏ≤≠Í∏∞ Í∞ÄÎèô"]
        },
        "rhinitis": {
            "ok": ["ÏÉÅÏæåÌïú ÏïÑÏπ® ÏÇ∞Ï±Ö", "ÍµêÏã§ ÌôòÍ∏∞ Í∂åÏû•", "Í∑úÏπôÏ†Å ÏàòÎ©¥ Í¥ÄÎ¶¨"],
            "caution": ["ÏÜêÏàòÍ±¥/ÎßàÏä§ÌÅ¨ ÏßÄÏ∞∏", "ÏΩßÎ¨º Ï¶ùÏÉÅ Í¥ÄÎ¶¨", "Ïã§ÎÇ¥ ÏäµÎèÑ Ï°∞Ï†à"],
            "warning": ["Ï†àÎåÄ Ïã§ÎÇ¥ ÎåÄÍ∏∞", "Î¨ºÍ±∏Î†à Ï≤≠ÏÜå", "Ï¶ùÏÉÅ Ïãú ÏïΩ Î≥µÏö©"]
        },
        "asthma": {
            "ok": ["Ïö¥ÎèôÏû• ÌôúÎèô Í∂åÏû•", "ÍπäÏùÄ Ìò∏Ìù° Ïó∞Ïäµ", "Í∑úÏπôÏ†Å Ìà¨ÏïΩ Ïú†ÏßÄ"],
            "caution": ["Ï∂©Î∂ÑÌïú Î¨º ÏÑ≠Ï∑®", "Ïª®ÎîîÏÖò Ï≤¥ÌÅ¨", "Î¨¥Î¶¨Ìïú Îã¨Î¶¨Í∏∞ ÏûêÏ†ú"],
            "warning": ["ÏïºÏô∏ ÌïôÏõê Ïù¥Îèô ÏûêÏ†ú", "Î≥¥Ìò∏Ïûê Î∞ÄÏ∞© ÌôïÏù∏", "ÎπÑÏÉÅ Ïó∞ÎùΩÎßù Ï†êÍ≤Ä"]
        },
        "atopy": {
            "ok": ["Î≥¥ÏäµÏ†ú ÎèÑÌè¨ ÌõÑ Ïô∏Ï∂ú", "Í∞ÄÎ≤ºÏö¥ Ïö¥Îèô", "ÎïÄ Îã¶Í∏∞"],
            "caution": ["Î≥¥ÏäµÏ†ú Ìú¥ÎåÄ", "Ïô∏Ï∂ú ÌõÑ ÏÑ∏Ïïà", "Î©¥ ÏÜåÏû¨ ÏùòÎ•ò"],
            "warning": ["Ïã§Ïô∏ ÌôúÎèô Ï§ëÎã®", "ÎÉâÏ∞úÏßà ÏßÑÏ†ï", "Í≥†Î≥¥Ïäµ Í¥ÄÎ¶¨"]
        }
    },
    "teen_adult": {
        "general": {
            "ok": ["Ï°∞ÍπÖ/Îì±ÏÇ∞ Í∂åÏû•", "Ï†ÑÎ©¥ ÌôòÍ∏∞ Ïã§Ïãú", "ÏïºÏô∏ ÌïôÏäµ/ÏóÖÎ¨¥"],
            "caution": ["Ï∂©Î∂ÑÌïú ÏàòÎ∂Ñ ÏÑ≠Ï∑®", "ÏÜê ÏîªÍ∏∞ ÏÉùÌôúÌôî", "Í∞ÄÎ≤ºÏö¥ ÌôòÍ∏∞"],
            "warning": ["Ïô∏Ï∂ú ÏûêÏ†ú", "Î≥¥Í±¥Ïö© ÎßàÏä§ÌÅ¨ ÌïÑÏ∞©", "Î¨ºÍ±∏Î†à Ï≤≠ÏÜå"]
        },
        "rhinitis": {
            "ok": ["Ïπ®Íµ¨Î•ò ÌñáÎ≥ï ÏÜåÎèÖ", "Ï†ÑÎ©¥ ÌôòÍ∏∞", "Í∞ÄÎ≤ºÏö¥ ÏÇ∞Ï±Ö"],
            "caution": ["ÎßàÏä§ÌÅ¨ Ìú¥ÎåÄ", "Í∑ÄÍ∞Ä ÌõÑ ÏΩî ÏÑ∏Ï≤ô", "Ïã§ÎÇ¥ ÏäµÎèÑ Ïú†ÏßÄ"],
            "warning": ["Ïô∏Ï∂ú Í∏àÏßÄ", "Ï∞ΩÎ¨∏ Î∞ÄÌèê", "ÏïΩÎ¨º Î≥µÏö© Ï†êÍ≤Ä"]
        },
        "asthma": {
            "ok": ["Í∑úÏπôÏ†Å Ïö¥Îèô", "Ïã§ÎÇ¥Ïô∏ ÌôòÍ∏∞", "Ïª®ÎîîÏÖò Í¥ÄÎ¶¨"],
            "caution": ["Ï¶ùÏÉÅ Ïú†Î¨¥ ÌôïÏù∏", "Î¨¥Î¶¨Ìïú Îì±ÏÇ∞ ÏûêÏ†ú", "ÎπÑÏÉÅÏïΩ ÏßÄÏ∞∏"],
            "warning": ["Ïã§ÎÇ¥ ÏïàÏã¨ ÎåÄÍ∏∞", "Í≥µÍ∏∞Ïßà Í¥ÄÎ¶¨", "ÎπÑÏÉÅ Ïãú ÏùòÎ£åÍ∏∞Í¥Ä"]
        },
        "atopy": {
            "ok": ["Ï∂©Î∂ÑÌïú Î≥¥Ïäµ ÌõÑ Ïô∏Ï∂ú", "ÏûêÏô∏ÏÑ† Ï∞®Îã®", "Î©¥ ÏÜåÏû¨ ÏùòÎ•ò"],
            "caution": ["Ïô∏Ï∂ú ÌõÑ Í∞ÄÎ≤ºÏö¥ ÏÉ§Ïõå", "Î≥¥ÏäµÏ†ú ÎèÑÌè¨", "ÏàòÎ∂Ñ ÏÑ≠Ï∑®"],
            "warning": ["ÏïºÏô∏ ÌôúÎèô Ï§ëÎã®", "Ï†ÄÏûêÍ∑π ÏÑ∏Ïïà Î∞è ÏÉ§Ïõå", "Í≥†Î≥¥Ïäµ ÏßÑÏ†ï Í¥ÄÎ¶¨"]
        }
    }
}

def _calculate_decision(pm25_grade: str, o3_grade: str) -> str:
    """
    Calculate decision level: 'ok', 'caution', 'warning'
    
    Logic (1:Ï¢ãÏùå, 2:Î≥¥ÌÜµ, 3:ÎÇòÏÅ®, 4:Îß§Ïö∞ÎÇòÏÅ®):
    ‚Ä¢ OK: PM2.5 <= 2 AND O3 <= 2
    ‚Ä¢ Caution: Either one is 3
    ‚Ä¢ Warning: Either one is 4 OR Both are 3
    """
    p_score = GRADE_MAP.get(pm25_grade, 2)
    o_score = GRADE_MAP.get(o3_grade, 2)
    
    # Check Warning Conditions
    if p_score >= 4 or o_score >= 4:
        return "warning"
    if p_score == 3 and o_score == 3:
        return "warning"
        
    # Check Caution Conditions
    if p_score == 3 or o_score == 3:
        return "caution"
        
    # Default OK
    return "ok"

def _normalize_age_group(age_group: Any) -> str:
    if age_group is None:
        return "elementary_high"
    raw = str(age_group).strip().lower()
    
    # Updated 5 groups based on planning document
    if raw in {"infant", "ÏòÅÏïÑ", "0-2", "0~2"}:
        return "infant"
    if raw in {"toddler", "Ïú†ÏïÑ", "3-6", "3~6"}:
        return "toddler"
    if raw in {"elementary_low", "Ï¥àÎì±Ï†ÄÌïôÎÖÑ", "Ï¥àÎì± Ï†ÄÌïôÎÖÑ", "7-9", "7~9", "1-3", "1~3"}:
        return "elementary_low"
    if raw in {"elementary_high", "Ï¥àÎì±Í≥†ÌïôÎÖÑ", "Ï¥àÎì± Í≥†ÌïôÎÖÑ", "10-12", "10~12", "4-6", "4~6"}:
        return "elementary_high"
    if raw in {"teen", "teen_adult", "Ï≤≠ÏÜåÎÖÑ", "ÏÑ±Ïù∏", "adult", "13-18", "13~18", "13+"}:
        return "teen_adult"
    
    # Fallbacks
    if "ÏòÅÏïÑ" in raw: return "infant"
    if "Ïú†ÏïÑ" in raw: return "toddler"
    if "Ï†ÄÌïôÎÖÑ" in raw: return "elementary_low"
    if "Í≥†ÌïôÎÖÑ" in raw: return "elementary_high"
    if "Ï≤≠ÏÜåÎÖÑ" in raw or "ÏÑ±Ïù∏" in raw: return "teen_adult"
    
    return "elementary_high"

def _get_display_content(age_group: str, condition: str, decision_key: str):
    """
    Returns (decision_text, action_items)
    """
    # Normalize condition
    cond_key = condition if condition in ["general", "rhinitis", "asthma", "atopy"] else "general"
    
    # Get Text
    group_data = DECISION_TEXTS.get(age_group, DECISION_TEXTS["elementary_high"])
    cond_data = group_data.get(cond_key, group_data.get("general", {}))
    d_text = cond_data.get(decision_key, "ÏÉÅÌÉú ÌôïÏù∏ ÌïÑÏöî")
    
    # Get Actions
    group_actions = ACTION_ITEMS.get(age_group, ACTION_ITEMS.get("toddler", {}))
    cond_actions = group_actions.get(cond_key, group_actions.get("general", {}))
    actions = cond_actions.get(decision_key, ["ÏÉÅÌÉúÏóê Îî∞Î•∏ Ï£ºÏùòÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§."])
    
    return d_text, actions[:] # Return a copy

try:
    vo_client = voyageai.Client(api_key=VOYAGE_API_KEY)
except Exception as e:
    print(f"Error initializing Voyage AI client: {e}")
    vo_client = None

try:
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
except Exception as e:
    print(f"Error initializing OpenAI client: {e}")
    openai_client = None

async def get_air_quality_from_mongodb(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch latest air quality data from MongoDB air_quality_data collection.
    This collection is populated by AWS Lambda cron job every hour.
    Returns None if no recent data (> 2 hours old) or not found.
    """
    if air_quality_db is None:
        return None
    
    try:
        station_candidates = _normalize_station_candidates(station_name)
        if not station_candidates:
            return None

        # Latest-first sort strategy:
        # 1) dataTime (AirKorea snapshot time), 2) updatedAt (ingest time), 3) _id (insert order)
        sort_criteria = [("dataTime", -1), ("updatedAt", -1), ("_id", -1)]
        data = None
        matched_candidate = None

        for candidate in station_candidates:
            data = await air_quality_db[AIR_QUALITY_DATA_COLLECTION].find_one(
                {"stationName": candidate},
                sort=sort_criteria
            )
            if data:
                matched_candidate = candidate
                break

        if not data:
            print(f"‚ö†Ô∏è  No MongoDB data found for station candidates: {station_candidates}")
            return None

        # Check data freshness (must be within 2 hours)
        observed_at = (
            _parse_datetime_to_kst(data.get("dataTime"))
            or _parse_datetime_to_kst(data.get("updatedAt"))
            or _parse_datetime_to_kst(data.get("createdAt"))
        )
        if observed_at:
            now = datetime.now(KST_TZ)
            age = now - observed_at
            if age > timedelta(hours=2):
                print(
                    f"‚ö†Ô∏è  MongoDB data is stale ({age.total_seconds()/3600:.1f} hours old), "
                    f"station={data.get('stationName')} observed_at={observed_at.isoformat()}"
                )
                return None

        # Convert grade strings to Korean text
        grade_map = {"1": "Ï¢ãÏùå", "2": "Î≥¥ÌÜµ", "3": "ÎÇòÏÅ®", "4": "Îß§Ïö∞ÎÇòÏÅ®"}

        # Support both camelCase (Lambda docs) and snake_case (legacy docs)
        pm25_grade_value = data.get("pm25Grade", data.get("pm25_grade", "2"))
        pm10_grade_value = data.get("pm10Grade", data.get("pm10_grade", "2"))
        o3_grade_value = data.get("o3Grade", data.get("o3_grade", "1"))
        no2_grade_value = data.get("no2Grade", data.get("no2_grade", "1"))
        co_grade_value = data.get("coGrade", data.get("co_grade", "1"))
        so2_grade_value = data.get("so2Grade", data.get("so2_grade", "1"))

        def _coerce_number(value: Any) -> Optional[float]:
            if value is None:
                return None
            # Decimal128 in Mongo can break JSON serialization; normalize to float when possible.
            try:
                if hasattr(value, "to_decimal"):
                    return float(value.to_decimal())
            except Exception:
                pass
            try:
                return float(value)
            except Exception:
                return None

        temp_raw = data.get("temperature")
        if temp_raw is None:
            temp_raw = data.get("temp")
        humidity_raw = data.get("humidity")

        temp_value = _coerce_number(temp_raw)
        humidity_value = _coerce_number(humidity_raw)

        result = {
            "stationName": data.get("stationName", station_name),
            "sidoName": data.get("sidoName"),
            "pm25_grade": grade_map.get(str(pm25_grade_value), "Î≥¥ÌÜµ"),
            "pm25_value": data.get("pm25Value", data.get("pm25_value", 50)),
            "pm10_grade": grade_map.get(str(pm10_grade_value), "Î≥¥ÌÜµ"),
            "pm10_value": data.get("pm10Value", data.get("pm10_value", 70)),
            "o3_grade": grade_map.get(str(o3_grade_value), "Ï¢ãÏùå"),
            "o3_value": data.get("o3Value", data.get("o3_value", 0.05)),
            "no2_grade": grade_map.get(str(no2_grade_value), "Ï¢ãÏùå"),
            "no2_value": data.get("no2Value", data.get("no2_value", 0.02)),
            "co_grade": grade_map.get(str(co_grade_value), "Ï¢ãÏùå"),
            "co_value": data.get("coValue", data.get("co_value", 0.5)),
            "so2_grade": grade_map.get(str(so2_grade_value), "Ï¢ãÏùå"),
            "so2_value": data.get("so2Value", data.get("so2_value", 0.003)),
            # Lambda now stores weather on the same document.
            # Support both `temperature` (Lambda) and `temp` (legacy/other sources).
            "temp": temp_value,
            "humidity": humidity_value,
            "dataTime": data.get("dataTime"),
        }

        print(
            f"‚úÖ Fetched latest air quality from MongoDB "
            f"(requested={station_name}, matched={matched_candidate}, station={result['stationName']}, dataTime={result.get('dataTime')})"
        )
        return result

    except Exception as e:
        print(f"‚ùå Error fetching from MongoDB: {e}")
        return None

async def get_air_quality_from_airkorea_api(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Direct fallback to Air Korea OpenAPI.
    This replaces the dependency on EPI-LOG-AIRKOREA service.
    """
    import httpx
    
    # Air Korea OpenAPI endpoint (replace with actual endpoint if different)
    # Note: This is a placeholder - actual Air Korea API integration would require
    # API key and proper endpoint configuration
    
    # For now, we'll try the old EPI-LOG-AIRKOREA service as temporary fallback
    # TODO: Replace with direct Air Korea OpenAPI call
    AIRKOREA_API_URL = "https://epi-log-airkorea.vercel.app/api/stations"
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(
                AIRKOREA_API_URL,
                params={"stationName": station_name}
            )
            
            if response.status_code == 200:
                data = response.json()
                
                # API returns array, take first item
                if data and len(data) > 0:
                    station = data[0]
                    realtime = station.get("realtime", {})
                    
                    # Convert grade numbers to Korean text
                    grade_map = {1: "Ï¢ãÏùå", 2: "Î≥¥ÌÜµ", 3: "ÎÇòÏÅ®", 4: "Îß§Ïö∞ÎÇòÏÅ®"}
                    
                    # Extract and normalize data
                    result = {
                        "stationName": station.get("stationName", station_name),
                        "sidoName": station.get("sidoName"),
                        "pm25_grade": grade_map.get(realtime.get("pm25", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "pm25_value": realtime.get("pm25", {}).get("value") or 50,
                        "pm10_grade": grade_map.get(realtime.get("pm10", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "pm10_value": realtime.get("pm10", {}).get("value") or 70,
                        "o3_grade": grade_map.get(realtime.get("o3", {}).get("grade"), "Î≥¥ÌÜµ"),
                        "o3_value": realtime.get("o3", {}).get("value") or 0.05,
                        "no2_grade": grade_map.get(realtime.get("no2", {}).get("grade"), "Ï¢ãÏùå"),
                        "no2_value": realtime.get("no2", {}).get("value") or 0.02,
                        "co_grade": grade_map.get(realtime.get("co", {}).get("grade"), "Ï¢ãÏùå"),
                        "co_value": realtime.get("co", {}).get("value") or 0.5,
                        "so2_grade": grade_map.get(realtime.get("so2", {}).get("grade"), "Ï¢ãÏùå"),
                        "so2_value": realtime.get("so2", {}).get("value") or 0.003,
                        "temp": None,
                        "humidity": None,
                        "dataTime": realtime.get("dataTime") or station.get("dataTime")
                    }
                    
                    print(f"‚úÖ Fetched air quality for {station_name} from Air Korea API (fallback)")
                    return result
        
        print(f"‚ö†Ô∏è  No data from Air Korea API for {station_name}")
        return None
        
    except Exception as e:
        print(f"‚ùå Error fetching from Air Korea API: {e}")
        return None

async def get_air_quality(station_name: str) -> Optional[Dict[str, Any]]:
    """
    Fetch air quality data with priority order:
    1. MongoDB air_quality_data (Lambda cron job data) - PRIORITY
    2. Air Korea OpenAPI (fallback for real-time data)
    3. Mock data (final fallback)
    
    Note: Temperature and humidity are expected to be present in the Lambda-stored MongoDB document.
    If missing, the API returns default placeholders.
    """
    # Priority 1: Try MongoDB (Lambda-stored data)
    data = await get_air_quality_from_mongodb(station_name)
    if data:
        # Add default temp/humidity for now (will be replaced with weather API)
        if data.get("temp") is None:
            data["temp"] = 22.0  # Default value
        if data.get("humidity") is None:
            data["humidity"] = 45.0  # Default value
        return data
    
    # Priority 2: Try Air Korea API (temporary fallback)
    data = await get_air_quality_from_airkorea_api(station_name)
    if data:
        # Add default temp/humidity
        if data.get("temp") is None:
            data["temp"] = 22.0
        if data.get("humidity") is None:
            data["humidity"] = 45.0
        return data
    
    # Priority 3: Return mock data (final fallback)
    print(f"‚ö†Ô∏è  Using mock data for {station_name}")
    return {
        "sidoName": None,
        "stationName": station_name,
        "pm10_grade": "ÎÇòÏÅ®",
        "pm10_value": 85,
        "pm25_grade": "ÎÇòÏÅ®",
        "pm25_value": 65,
        "co_grade": "Î≥¥ÌÜµ",
        "co_value": 0.7,
        "o3_grade": "Î≥¥ÌÜµ",
        "o3_value": 0.065,
        "no2_grade": "Ï¢ãÏùå",
        "no2_value": 0.025,
        "so2_grade": "Ï¢ãÏùå",
        "so2_value": 0.004,
        "temp": 22.0,
        "humidity": 45.0,
        "dataTime": None,
    }

CACHE_COLLECTION = "rag_cache"
CACHE_TTL_SECONDS = 60 * 60 * 30  # 30 hours
_cache_ttl_index_ready = False

def _normalize_cache_token(value: Any) -> str:
    if value is None:
        return "na"
    token = str(value).strip()
    if not token:
        return "na"
    for old, new in [
        (" ", ""),
        (":", "-"),
        ("/", "-"),
        ("\\", "-"),
        ("\n", ""),
        ("\t", "")
    ]:
        token = token.replace(old, new)
    return token or "na"


def _generate_cache_key(air_data: Dict[str, Any], user_profile: Dict[str, Any]) -> str:
    grade_map = {"Ï¢ãÏùå": 1, "Î≥¥ÌÜµ": 2, "ÎÇòÏÅ®": 3, "Îß§Ïö∞ÎÇòÏÅ®": 4}
    
    pm25 = grade_map.get(air_data.get("pm25_grade", ""), 0)
    pm10 = grade_map.get(air_data.get("pm10_grade", ""), 0)
    o3 = grade_map.get(air_data.get("o3_grade", ""), 0) # Added o3 as per user example
    
    age_group = _normalize_age_group(user_profile.get("ageGroup"))
    condition = user_profile.get("condition", "unknown")
    station_key = _normalize_cache_token(
        f"{air_data.get('sidoName', '')}_{air_data.get('stationName', '')}"
    )

    observed_at = (
        _parse_datetime_to_kst(air_data.get("dataTime"))
        or _parse_datetime_to_kst(air_data.get("updatedAt"))
    )
    observed_key = observed_at.strftime("%Y%m%d%H%M") if observed_at else datetime.now(KST_TZ).strftime("%Y%m%d")

    pm25_value = _normalize_cache_token(air_data.get("pm25_value"))
    pm10_value = _normalize_cache_token(air_data.get("pm10_value"))
    o3_value = _normalize_cache_token(air_data.get("o3_value"))
    no2_value = _normalize_cache_token(air_data.get("no2_value"))
    
    # Key format:
    # station:seoul_jongro_pm25:2_pm10:2_o3:1_age:toddler_cond:asthma_obs:202602071300_vals:14_52_0.03_0.009
    return (
        f"station:{station_key}_"
        f"pm25:{pm25}_pm10:{pm10}_o3:{o3}_"
        f"age:{_normalize_cache_token(age_group)}_cond:{_normalize_cache_token(condition)}_"
        f"obs:{observed_key}_vals:{pm25_value}_{pm10_value}_{o3_value}_{no2_value}"
    )

async def _ensure_cache_ttl_index():
    global _cache_ttl_index_ready
    if _cache_ttl_index_ready or db is None:
        return
    try:
        await db[CACHE_COLLECTION].create_index(
            "created_at",
            expireAfterSeconds=CACHE_TTL_SECONDS,
            name="rag_cache_ttl"
        )
        _cache_ttl_index_ready = True
    except Exception as e:
        print(f"‚ö†Ô∏è Cache TTL index creation failed: {e}")

async def get_medical_advice(station_name: str, user_profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    Main orchestration function with correction logic.
    """
    # Step A: Get Air Quality
    air_data = await get_air_quality(station_name)
    if not air_data:
        raise ValueError(f"No air quality data found for station: {station_name}")

    # Extract Weather Info for Correction
    temp = air_data.get("temp")
    humidity = air_data.get("humidity")
    user_condition = user_profile.get("condition", "Í±¥Í∞ïÌï®")
    age_group_raw = user_profile.get("ageGroup")
    age_group = _normalize_age_group(age_group_raw)

    # Apply Correction Logic to get "Sensed" grades
    pm25_raw = air_data.get("pm25_grade", "Î≥¥ÌÜµ")
    o3_raw = air_data.get("o3_grade", "Î≥¥ÌÜµ")
    
    pm25_corrected = _get_corrected_grade(pm25_raw, temp, humidity, user_condition, "pm25")
    o3_corrected = _get_corrected_grade(o3_raw, temp, humidity, user_condition, "o3")

    cache_key = ""
    # [Step A.1] Check Cache
    if db is not None:
        try:
            await _ensure_cache_ttl_index()
            cache_key = _generate_cache_key(air_data, user_profile)
            cached_entry = await db[CACHE_COLLECTION].find_one({"_id": cache_key})
            
            if cached_entry:
                print(f"‚úÖ Cache Hit! Key: {cache_key}")
                return cached_entry["data"]
        except Exception as e:
            print(f"‚ö†Ô∏è Cache check failed: {e}")

    # Determine main issue for search (using corrected grades)
    main_condition = "Î≥¥ÌÜµ"
    if pm25_corrected in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"Ï¥àÎØ∏ÏÑ∏Î®ºÏßÄ {pm25_corrected}"
    elif air_data.get("pm10_grade") in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"ÎØ∏ÏÑ∏Î®ºÏßÄ {air_data['pm10_grade']}"
    elif o3_corrected in ["ÎÇòÏÅ®", "Îß§Ïö∞ÎÇòÏÅ®"]:
        main_condition = f"Ïò§Ï°¥ {o3_corrected}"
        
    # Step B: Query Construction
    search_query = f"{main_condition} ÏÉÅÌô©ÏóêÏÑú {user_condition} {age_group} ÌñâÎèô ÏöîÎ†π Ï£ºÏùòÏÇ¨Ìï≠"
    print(f"Generated Search Query (Primary): {search_query}")

    # Step C: Vector Search
    relevant_docs = []
    if vo_client and db is not None:
        try:
            # 1. Primary Search
            embed_result = vo_client.embed([search_query], model="voyage-3-large", input_type="query")
            query_vector = embed_result.embeddings[0]
            
            pipeline = [
                {
                    "$vectorSearch": {
                        "index": VECTOR_INDEX,
                        "path": "embedding",
                        "queryVector": query_vector,
                        "numCandidates": 100,
                        "limit": 3
                    }
                },
                {
                    "$project": {
                        "_id": 0,
                        "text": 1,
                        "category": 1,
                        "risk_level": 1,
                        "source": 1,
                        "score": {"$meta": "vectorSearchScore"}
                    }
                }
            ]
            
            cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
            relevant_docs = await cursor.to_list(length=3)
            
            # 2. Fallback Search (If no docs found)
            if not relevant_docs:
                print("‚ö†Ô∏è Primary search returned no results. Attempting fallback (General) search.")
                fallback_query = f"{main_condition} ÌñâÎèô ÏöîÎ†π"
                embed_result_fb = vo_client.embed([fallback_query], model="voyage-3-large", input_type="query")
                query_vector_fb = embed_result_fb.embeddings[0]
                
                pipeline[0]["$vectorSearch"]["queryVector"] = query_vector_fb
                
                cursor = db[GUIDELINES_COLLECTION].aggregate(pipeline)
                relevant_docs = await cursor.to_list(length=3)
                
        except Exception as e:
            print(f"Error during vector search: {e}")
            pass

    # Step D: LLM Generation
    if not openai_client:
         return {
            "decision": "Error",
            "three_reason": [
                "**AI ÏãúÏä§ÌÖú**Ïù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.",
                "ÏÑúÎ≤Ñ ÏÑ§Ï†ïÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.",
                "**Í¥ÄÎ¶¨Ïûê**ÏóêÍ≤å Î¨∏ÏùòÌïòÏÑ∏Ïöî."
            ],
            "detail_answer": "OpenAI Client not initialized",
            "actionItems": [],
            "references": []
        }

    # [Logic Update] Calculate Deterministic Decision & Action Items using CORRECTED grades
    decision_key = _calculate_decision(pm25_corrected, o3_corrected)
    decision_text, action_items = _get_display_content(age_group, user_condition, decision_key)
    
    # O3 Special Handling: Force-Append and Warnings
    is_o3_dominant = GRADE_MAP.get(o3_corrected, 1) >= GRADE_MAP.get(pm25_corrected, 1)
    if is_o3_dominant and GRADE_MAP.get(o3_corrected, 1) >= 3: # 'ÎÇòÏÅ®' Ïù¥ÏÉÅ
        decision_text += " (Ïò§Ï°¥ÏùÄ ÎßàÏä§ÌÅ¨Î°ú Í±∏Îü¨ÏßÄÏßÄ ÏïäÏïÑÏöî!)"
        # Force-Append Action Item
        o3_force_action = "Ïò§ÌõÑ 2~5Ïãú ÏÇ¨Ïù¥ÏóêÎäî Ïã§Ïô∏ ÌôúÎèôÏùÑ Ï†ÑÎ©¥ Í∏àÏßÄÌïòÍ≥† Ïã§ÎÇ¥Ïóê Î®∏Î¨¥Î•¥ÏÑ∏Ïöî."
        if o3_force_action not in action_items:
            action_items.append(o3_force_action)

    # Infant Special Warning
    if age_group == "infant":
        infant_warning = "‚Äª Ï£ºÏùò: ÎßàÏä§ÌÅ¨ Ï∞©Ïö© Í∏àÏßÄ(ÏßàÏãù ÏúÑÌóò)"
        if infant_warning not in action_items:
            action_items.insert(0, infant_warning) # Put at top

    # Logic for dual bad condition text append
    if GRADE_MAP.get(pm25_corrected, 1) >= 3 and GRADE_MAP.get(o3_corrected, 1) >= 3:
        decision_text += " (ÎØ∏ÏÑ∏Î®ºÏßÄÏôÄ Ïò§Ï°¥ Îëò Îã§ ÎÜíÏïÑÏöî!)"

    # Prepare Context
    context_text = "\n".join([f"- [Ï∂úÏ≤ò: {doc.get('source', 'Í∞ÄÏù¥ÎìúÎùºÏù∏')}] {doc.get('text', '')}" for doc in relevant_docs]) if relevant_docs else "Í¥ÄÎ†® ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§."
    
    system_prompt = """
    ÎÑàÎäî Î≥µÏû°Ìïú ÎåÄÍ∏∞Ïßà ÎÖºÎ¨∏Í≥º Îç∞Ïù¥ÌÑ∞Î•º ÌïôÎ∂ÄÎ™®Í∞Ä Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å ÏÑ§Î™ÖÌï¥Ï£ºÎäî AI ÎπÑÏÑúÎã§.
    
    [Ïó≠Ìï† Î∞è Ï∂úÎ†• Ï†úÏïΩ]
    1. 'decision'Í≥º 'actionItems'Îäî Ïù¥ÎØ∏ ÏãúÏä§ÌÖúÏóêÏÑú Í≥ÑÏÇ∞ÎêòÏóàÏäµÎãàÎã§. ÎãπÏã†ÏùÄ Ïù¥ Í≤∞Ï†ïÏù¥ ÎÇ¥Î†§ÏßÑ Ïù¥Ïú†Î•º ÌïôÎ∂ÄÎ™®Í∞Ä Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å ÏÑ§Î™ÖÌï¥Ïïº Ìï©ÎãàÎã§.
    2. Î∞òÎìúÏãú Îã§Ïùå Îëê Í∞ÄÏßÄÎ•º JSON ÌòïÏãùÏúºÎ°ú Ï∂úÎ†•ÌïòÏÑ∏Ïöî:
       - "three_reason": Ï†ïÌôïÌûà 3Í∞úÏùò ÏßßÏùÄ ÏöîÏïΩ Î¨∏Ïû• Î∞∞Ïó¥ (Í∞Å Î¨∏Ïû•ÏùÄ Ìïú Ï§ÑÎ°ú, ÌïµÏã¨ ÌÇ§ÏõåÎìúÎäî **double asterisks**Î°ú Í∞êÏã∏Í∏∞)
       - "detail_answer": ÏÉÅÏÑ∏Ìïú ÏùòÌïôÏ†Å/ÌôòÍ≤ΩÏ†Å ÏÑ§Î™Ö (Í∏∞Ï°¥Ïùò Í∏¥ ÏÑ§Î™Ö)
    
    3. **ÌÇ§ÏõåÎìú ÌïòÏù¥ÎùºÏù¥ÌåÖ Í∑úÏπô**:
       - ÏßàÌôòÎ™Ö (Ïòà: **Ï≤úÏãù**, **ÎπÑÏóº**, **ÏïÑÌÜ†Ìîº**)
       - ÎåÄÍ∏∞Ïßà Îì±Í∏â (Ïòà: **Ï¢ãÏùå**, **ÎÇòÏÅ®**, **Îß§Ïö∞ÎÇòÏÅ®**)
       - ÌñâÎèô ÏöîÎ†π (Ïòà: **Ïã§Ïô∏ ÌôúÎèô**, **ÎßàÏä§ÌÅ¨ Ï∞©Ïö©**, **ÌôòÍ∏∞**)
       - Ï§ëÏöîÌïú ÏàòÏπòÎÇò ÏãúÍ∞ÑÎåÄ (Ïòà: **35**, **Ïò§ÌõÑ 2~5Ïãú**)
    
    4. **ÌÜ§Ïï§Îß§ÎÑà**: ÏπúÏ†àÌïòÏßÄÎßå Î™ÖÌôïÌïòÍ≥† Îã®Ìò∏ÌïòÍ≤å. ÌïôÎ∂ÄÎ™®Í∞Ä Ï¶âÏãú ÌñâÎèôÌï† Ïàò ÏûàÎèÑÎ°ù Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÏûëÏÑ±ÌïòÏÑ∏Ïöî.
    
    5. Î≥¥Ï†ï Î°úÏßÅÏù¥ Ï†ÅÏö©Îêú Í≤ΩÏö∞(Ïòà: ÏäµÎèÑ, Ïò®ÎèÑÎ°ú Ïù∏Ìïú Îì±Í∏â Í≤©ÏÉÅ) Í∑∏ Ïù¥Ïú†Î•º three_reasonÏù¥ÎÇò detail_answerÏóê Ìè¨Ìï®ÌïòÏÑ∏Ïöî.
    
    6. Ï†úÍ≥µÎêú [ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏] ÎÇ¥Ïö©ÏùÑ ÏµúÏö∞ÏÑ†ÏúºÎ°ú Î∞òÏòÅÌïòÏó¨ ÏÑ§Î™ÖÌïòÏÑ∏Ïöî.
    """
    
    user_prompt = f"""
    [ÏÉÅÌô© Ï†ïÎ≥¥]
    - ÎåÄÍ∏∞Ïßà: Ï¥àÎØ∏ÏÑ∏Î®ºÏßÄ={pm25_raw}(Î≥¥Ï†ïÌõÑ:{pm25_corrected}), Ïò§Ï°¥={o3_raw}(Î≥¥Ï†ïÌõÑ:{o3_corrected})
    - ÌôòÍ≤Ω: Ïò®ÎèÑ={temp}¬∞C, ÏäµÎèÑ={humidity}%
    - ÏÇ¨Ïö©Ïûê: Ïó∞Î†πÎåÄ={age_group}, Í∏∞Ï†ÄÏßàÌôò={user_condition}
    - ÏãúÏä§ÌÖú Í≤∞Ï†ï: {decision_text}
    - ÏãúÏä§ÌÖú ÌñâÎèôÏàòÏπô: {action_items}
    
    [ÏùòÌïôÏ†Å Í∞ÄÏù¥ÎìúÎùºÏù∏ (Ï∞∏Í≥† Î¨∏Ìóå)]
    {context_text}
    
    ÏúÑ Í≤∞Ï†ïÏù¥ ÎÇ¥Î†§ÏßÑ Î∞∞Í≤ΩÍ≥º Ïù¥Ïú†Î•º ÌïôÎ∂ÄÎ™®Í∞Ä Ïù¥Ìï¥ÌïòÍ∏∞ ÏâΩÍ≤å ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî.
    
    Ï∂úÎ†• ÌòïÏãù (JSON):
    {{
      "three_reason": [
        "Ï≤´ Î≤àÏß∏ ÏöîÏïΩ Î¨∏Ïû• (ÌïµÏã¨ ÌÇ§ÏõåÎìúÎäî **Ïù¥Î†áÍ≤å** Í∞êÏã∏Í∏∞)",
        "Îëê Î≤àÏß∏ ÏöîÏïΩ Î¨∏Ïû•",
        "ÏÑ∏ Î≤àÏß∏ ÏöîÏïΩ Î¨∏Ïû•"
      ],
      "detail_answer": "ÏÉÅÏÑ∏Ìïú ÏùòÌïôÏ†Å ÏÑ§Î™Ö..."
    }}
    """
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=1 
        )
        
        content = response.choices[0].message.content
        llm_result = json.loads(content)
        
        # Merge Results
        final_result = {
            "decision": decision_text,
            "three_reason": llm_result.get("three_reason", [
                "ÎåÄÍ∏∞Ïßà Ï†ïÎ≥¥Î•º Î∂ÑÏÑùÌïòÍ≥† ÏûàÏäµÎãàÎã§.",
                "Ïû†Ïãú ÌõÑ Îã§Ïãú ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.",
                "Î¨∏Ï†úÍ∞Ä ÏßÄÏÜçÎêòÎ©¥ Í¥ÄÎ¶¨ÏûêÏóêÍ≤å Î¨∏ÏùòÌïòÏÑ∏Ïöî."
            ]),
            "detail_answer": llm_result.get("detail_answer", "Ï†ïÎ≥¥Î•º Î∂àÎü¨Ïò§Îäî Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§."),
            "actionItems": action_items,
            "references": list(set([doc.get("source", "Unknown Source") for doc in relevant_docs])),
            # Add real-time air quality values for frontend display
            "pm25_value": air_data.get("pm25_value"),
            "o3_value": air_data.get("o3_value"),
            "pm10_value": air_data.get("pm10_value"),
            "no2_value": air_data.get("no2_value")
        }
        
        # [Step F] Save to Cache
        if db is not None and cache_key:
            try:
                await db[CACHE_COLLECTION].update_one(
                    {"_id": cache_key},
                    {"$set": {"data": final_result, "created_at": datetime.now(KST_TZ)}},
                    upsert=True
                )
                print(f"üíæ Saved to cache: {cache_key}")
            except Exception as e:
                print(f"Error saving to cache: {e}")
                
        return final_result
        
    except Exception as e:
        print(f"Error calling OpenAI: {e}")
        # Fallback even if LLM fails, we satisfy the deterministic requirement
        return {
            "decision": decision_text,
            "three_reason": [
                "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÎ°ú ÏÉÅÏÑ∏ Î∂ÑÏÑùÏùÑ Î∂àÎü¨Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§.",
                "ÌïòÏßÄÎßå **ÌñâÎèô ÏßÄÏπ®**ÏùÄ ÏúÑÏôÄ Í∞ôÏù¥ Ï§ÄÏàòÌï¥Ï£ºÏÑ∏Ïöî.",
                "Î¨∏Ï†úÍ∞Ä ÏßÄÏÜçÎêòÎ©¥ **Í¥ÄÎ¶¨Ïûê**ÏóêÍ≤å Î¨∏ÏùòÌïòÏÑ∏Ïöî."
            ],
            "detail_answer": "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÎ°ú ÏÉÅÏÑ∏ ÏÑ§Î™ÖÏùÑ Î∂àÎü¨Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§. ÌïòÏßÄÎßå ÌñâÎèô ÏßÄÏπ®ÏùÄ ÏúÑÏôÄ Í∞ôÏù¥ Ï§ÄÏàòÌï¥Ï£ºÏÑ∏Ïöî.",
            "actionItems": action_items,
            "references": [],
            # Add real-time air quality values for frontend display
            "pm25_value": air_data.get("pm25_value"),
            "o3_value": air_data.get("o3_value"),
            "pm10_value": air_data.get("pm10_value"),
            "no2_value": air_data.get("no2_value")
        }

async def ingest_pdf(file_content: bytes, filename: str) -> Dict[str, Any]:
    """
    Ingest PDF content: Extract text -> Embed -> Store in DB.
    """
    import io
    from PyPDF2 import PdfReader

    if not vo_client:
        return {"status": "error", "message": "Voyage AI Client not initialized"}
    
    if db is None:
        return {"status": "error", "message": "Database not initialized"}

    try:
        # 1. Read PDF
        pdf_file = io.BytesIO(file_content)
        reader = PdfReader(pdf_file)
        
        extracted_text = ""
        documents_to_insert = []
        texts_to_embed = []
        
        # 2. Extract Text per Page (Chunking Strategy: 1 Page = 1 Doc for simplicity)
        print(f"üìÑ Processing PDF: {filename} ({len(reader.pages)} pages)")
        
        for i, page in enumerate(reader.pages):
            text = page.extract_text()
            if text and len(text.strip()) > 50: # Ignore empty/too short pages
                extracted_text += text + "\n"
                
                texts_to_embed.append(text)
                documents_to_insert.append({
                    "text": text,
                    "category": "pdf_upload",
                    "source": filename,
                    "page": i + 1,
                    "risk_level": "unknown", # Needs manual classification or LLM analysis
                    "created_at": datetime.now()
                })
        
        if not documents_to_insert:
            return {"status": "error", "message": "No extractable text found in PDF."}

        # 3. Embed Data
        print(f"üß† Embedding {len(texts_to_embed)} pages with Voyage AI...")
        result = vo_client.embed(texts_to_embed, model="voyage-3-large", input_type="document")
        embeddings = result.embeddings
        
        for i, doc in enumerate(documents_to_insert):
            doc["embedding"] = embeddings[i]
            
        # 4. Insert into DB
        insert_result = await db[GUIDELINES_COLLECTION].insert_many(documents_to_insert)
        
        return {
            "status": "success",
            "message": f"Successfully ingested {len(insert_result.inserted_ids)} pages from {filename}",
            "inserted_ids": [str(id) for id in insert_result.inserted_ids]
        }
        
    except Exception as e:
        print(f"Error processing PDF: {e}")
        return {"status": "error", "message": str(e)}
